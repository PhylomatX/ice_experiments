{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0061298124,0.020758545,0.010895181,0.0026304566,-0.0063097016,0.022360578,0.0022621928,-0.002465841,-0.015925294,-0.040430967,0.005899011,0.024668591,-0.0039643524,-0.0025099646,-0.01066438,-0.013142101,0.012904511,0.015409385,0.005905799,0.009075924,-0.002343652,0.011268537,-0.0032091571,-0.00739922,-0.014309684,0.039290536,0.02994987,-0.015721645,0.015083548,-0.02219766,0.015748799,-0.004201942,-0.016142517,-0.028673673,-0.01471698,0.028320683,0.010766204,0.0022791633,0.027492514,0.016780617,0.016848499,0.023107288,-0.004442926,0.004636392,0.0061332067,0.016210401,0.015341502,-0.014974935,-0.023351666,0.009673292,0.0011981305,0.01908863,-0.021627445,-0.004222307,-0.022306273,0.00170216,-0.0024539614,0.00468391,0.010012706,-0.02970549,0.018124694,0.009686869,-0.018518414,0.018029658,-0.026637191,-0.004195154,0.00647262,0.0008612624,-0.005943135,0.014730557,0.014038153,0.023066558,-0.00873651,-0.017554479,0.017147183,-0.023501009,-0.008709357,-0.012103494,-0.008023741,0.01716076,0.009381396,-0.010535403,-0.0114450315,0.016658427,-0.004144242,-0.021898976,0.020812852,0.017798858,-0.03434867,0.009510374,0.00932709,0.02640639,0.01712003,-0.00611963,0.0062791547,-0.0058379164,-0.0060313824,0.026270624,0.015124277,0.007725057,-0.010969852,-0.0032006716,-0.05919376,-0.01798893,-0.011125983,-0.0075349854,0.011166712,0.00434789,0.00036571833,0.00041896384,-0.032800946,0.03388707,-0.00686634,-0.033914223,-0.0072430894,-0.0010453943,0.034918886,-0.007249878,-0.0038319812,-0.012429331,0.02120657,0.008410673,0.031307526,-0.025008006,0.024614286,-0.007310972,-0.017350832,-0.0068527637,-0.023405973,-0.022713568,0.024424214,0.024709322,0.018776368,0.018369073,-0.024030494,0.033425465,-0.013277866,0.017106453,-0.02317517,-0.008811181,0.011933788,0.020717816,0.0044734734,0.023473855,0.005773428,0.010698321,0.0139159635,0.027560396,0.0030496325,-0.010005917,-0.012836629,-0.014866322,0.029135277,0.029406808,0.004320737,0.011852329,-0.005498503,-0.000339838,-0.012768745,-0.0032023687,-0.009164171,0.024722898,-0.008512497,-0.00928636,0.022061894,0.013549397,0.0240848,-0.012137435,0.008505709,-0.0009809057,-0.02499443,0.03817726,-5.51017e-05,-0.008790816,0.0007216785,0.013108159,0.023446701,-0.027994847,-0.012028824,-0.007820093,-0.026908722,0.014554062,-0.009381396,0.003061512,-0.030520085,0.02294437,0.004164607,-0.029732645,-0.002990235,-0.0059363465,0.01649551,0.015966022,-0.00044420775,-0.000801865,-0.6681835,-0.021437373,0.007548562,-0.019414466,0.0070665944,0.016088212,0.011200654,-0.0067271804,-0.038041495,0.033208244,-0.021586714,-0.015219313,0.003577421,-0.029542573,-0.01921082,-0.039996516,0.00043657093,-0.025768293,-0.012286778,0.0054238318,-0.011587585,0.0014450541,-0.006445467,0.016794192,-0.0010377576,0.007881187,0.004269825,-0.024940122,0.021817517,-0.007419585,0.0059634997,0.02392188,0.005427226,0.00013979604,0.023107288,-0.03766135,-0.026162012,0.010175624,-0.0025829386,0.032529414,-0.025333842,-0.017432291,0.008838334,0.0047212453,-0.02172248,0.017350832,0.019455196,0.0062282425,0.010413214,0.010304602,-0.0057666395,0.01483917,0.013040276,-0.011153136,0.018056812,-0.023840422,0.024315601,-0.013372902,0.01168941,-0.019278701,0.016386896,-0.0027781015,-0.029406808,-0.0020381797,0.0028307107,0.013943117,-0.006985135,0.0017615574,-0.0053152195,-0.03125322,0.02250992,0.011295689,-0.00023822603,0.012938452,0.016454779,-0.0011743716,0.041897234,0.011682621,0.018722063,0.003950776,0.016739886,-0.015287195,-0.01908863,0.0021383066,0.008369943,0.0012015246,0.0087636635,-0.0020398768,0.0039541703,-0.0060551413,0.027343173,0.02821207,-0.002718704,-0.025116619,-0.011988094,0.017377984,0.0029919322,0.009517162,-0.0007310124,-0.04385226,-0.0043818317,0.004789128,0.031388983,0.02227912,0.02093504,0.007996588,-0.017568056,-0.0030156912,0.016481932,-0.024953699,0.016957112,-0.005406861,-0.0153550785,0.0036419097,-0.0052880663,-0.024315601,0.014499756,0.017880317,0.008729721,-0.004317343,0.008953735,-0.0053491606,0.016237553,-0.009008041,0.000790834,0.0066083856,-0.015422961,-0.013549397,0.008532862,-0.0105897095,0.015029241,0.0063775843,0.01983534,-0.025551068,-0.003085271,0.012504003,0.008247755,-0.007867611,0.01699784,-0.0015248163,-0.027302442,0.015504421,-0.0009961794,-0.020473437,-0.021573137,-0.0131896185,0.006292731,0.009829422,0.008363155,-0.006832399,0.005970288,-0.02317517,-0.0057564573,-0.00029910836,0.0011777658,0.0033194665,-0.0048977407,-0.033751305,0.0067441515,-0.026867993,0.011954152,0.03861171,-0.023541737,-0.01983534,-0.0041849716,-0.024587132,-0.007847246,0.034620203,-0.0041612126,-0.013332172,0.0078268815,-0.016047481,0.007372067,0.01602033,-0.017921045,0.0032566749,-0.009883729,-0.018952863,-0.0054951087,-0.02132876,0.012415756,0.0030649062,-0.0150156645,0.0013525638,0.035353336,0.021002922,0.006265578,0.021152265,-0.028565062,0.02719383,0.00018201063,0.013698739,-0.021396643,0.034538742,0.020324096,0.0138684455,0.03136183,-0.013264289,-0.020758545,0.01751375,0.027614703,0.025768293,0.022482768,-0.01090197,-0.0022672839,-0.011037735,0.0040084766,-0.010175624,0.021383066,0.020419132,0.02719383,0.011282113,-0.006781487,-0.012327507,0.005990653,0.0024980851,-0.008431038,0.02219766,-0.011132771,0.0030224796,0.009204901,-0.004687304,0.044829767,0.009734387,-0.012103494,-0.0121238595,-0.0018379255,0.024451368,0.034402978,-0.0023775934,-0.0035909975,-0.017065724,-0.0128841465,0.012327507,0.0041340594,0.007575715,0.0002986841,-0.0018396225,0.033859916,-0.008220601,0.015368654,0.0136104915,-0.00036105138,-0.014852745,0.022170506,-0.00039562915,0.029542573,0.02644712,0.004188366,0.0028476813,0.007548562,-0.008593956,-0.0030173883,0.021070806,-0.0021705509,-0.020677086,0.011587585,0.009476432,0.005400073,0.030194247,0.029569726,0.014594791,0.017147183,-0.005501897,0.03263803,-0.022835758,-0.028782286,-0.00068519154,-0.01577595,0.0018871404,0.0031243037,0.008668628,0.00735849,-0.023120865,0.011540067,0.0028714403,0.03467451,0.014160342,0.009449279,0.026338506,-0.025673257,-0.017758127,0.009788693,0.0086822035,-0.012551521,-0.0050470824,-0.011438243,0.010487885,-0.020120448,0.032122117,0.0037437337,0.033778455,-0.017323678,-0.008071259,-0.009659715,0.0009885426,0.02574114,-0.017934622,-0.009883729,-0.008546438,-0.0038693168,0.006618568,-0.012965606,-0.013685162,0.0082138125,-0.0018582903,-0.0065744445,-0.03687391,0.018980017,-0.010243507,0.026202742,-0.023405973,0.0017971959,0.00072252704,0.025211653,0.02207547,-0.0052948548,0.020215483,0.028347837,0.020391978,0.00044038935,-0.027424632,-0.008960523,0.003916835,0.0075078323,0.027030911,0.0049791997,0.022129776,-0.010515038,-0.008139142,0.00029274437,0.0021790364,0.01944162,0.009632562,-0.014689827,-0.009551103,-0.0069579817,0.0034789909,0.015966022,-0.010019494,0.0055901445,-0.03853025,-0.03434867,-0.01948235,-0.021871822,0.013861658,0.017174335,0.024315601,0.0015443326,-0.0020381797,0.002167157,0.009435702,0.003529903,0.0030920594,0.018898558,0.020487014,0.022401309,0.0054815323,-0.013983847,0.019251548,0.014499756,-0.003950776,-0.014499756,0.023758963,-0.0038794992,-0.013196407,-0.0017683457,0.01306743,-0.011736928,-0.019685999,0.0029562938,-0.011424667,0.043390654,0.005773428,0.009456067,0.000987694,-0.030628698,-0.025211653,-0.0050131413,-0.013033488,-0.013237136,-0.042440295,-0.01558588,-0.020120448,-0.021776786,-0.018423378,0.03888324,-0.006397949,0.017785281,0.0032719485,-0.024559978,0.005213395,-0.03269233,-0.012877358,-0.003675851,-0.027696162,-0.046133116,-0.016807768,0.005104783,0.026732227,-0.0035197206,-0.013155677,-0.004215519,0.015640186,-0.007310972,-0.007433161,0.016006753,-0.011078465,-0.007107324,0.01931943,-0.032420803,0.00074077054,0.0066321446,0.02699018,0.005600327,0.02675938,-9.774056e-05,-0.04170716,0.0071276887,-0.0072702426,-0.022645686,0.02085358,-0.0045006266,-0.019590963,0.0072838194,-0.010922334,-0.0029528996,-0.011899847,-0.021423796,0.0007759847,0.0075825034,-0.015708068,0.012307143,-0.017500173,-0.0048705875,-0.011431456,0.0123818135,-0.011275325,0.021641022,0.006937617,0.01826046,0.011472185,-0.0011056403,0.0004395408,0.002789981,-0.01602033,0.0050979946,0.010209566,-0.022564227,0.0050470824,-0.0022554044,0.0017377984,-0.014554062,0.014988512,-0.024559978,0.032529414,-0.014526908,0.00015422112,-0.020921463,-0.0051489067,-0.018531991,0.008885852,-0.0028816229,-0.0064251022,0.0010436972,0.010324966,-0.018559145,0.0071005356,-0.007813305,-0.015165007,-0.017486596,0.023365242,0.009191324,0.028184919,-0.009374608,0.012225684,-0.024179837,-0.031769127,-0.022767875,-0.017215066,-0.005186242,-0.0030937563,0.0019465379,0.026569309,0.03573348,-1.99273e-05,0.034185752,0.009971976,-0.015966022,0.017486596,-0.007894764,0.00814593,-0.022401309,0.0015265134,0.013983847,0.002565968,0.02810346,0.0046024504,0.015558726,8.956015e-06,-0.01821973,-0.010087376,-0.01479844,-0.012761957,-0.0019329613,0.008709357,-0.0096529275,-0.0025489973,-0.02215693,0.005559597,-0.018409802,-0.017174335,0.00081247167,0.0043309196,0.0063572195,-0.0035231148,0.008369943,0.0029325348,0.0038319812,-0.025211653,-0.036466613,-0.017337255,-0.010236719,0.004755187,-0.0075825034,0.024016917,0.016386896,0.012626192,-0.01483917,-0.014010999,-0.027804775,-0.0139159635,0.004863799,-0.0108476635,-0.0024794175,-0.021437373,-0.026854416,0.015422961,0.012639768,0.004731428,-0.02549676,0.004154424,0.0039847177,-0.01774455,0.009503585,0.01436399,0.024152683,0.012361449,0.022224812,0.019237971,-0.025415301,-0.000690707,0.0022757691,0.0073177605,0.00054348627,0.028184919,0.02207547,-0.010399638,-0.020731391,-0.0041035125,0.014879899,-0.01672631,-0.017948199,0.03133468,0.011838752,-0.02132876,-0.0065303207,-0.015246466,-0.026664345,-0.008655051,-0.0076639624,0.0012218895,-0.0050504766,-0.003283828,0.004911317,-0.008587168,0.008668628,0.021613868,-0.0023521374,-0.011879481,0.012571885,0.009822634,-0.0069206464,-0.0021281242,0.0121238595,0.022890063,0.024464943,0.020554896,-0.017880317,-0.005260913,-0.0027458572,0.0076911156,-0.014744134,0.028347837,-0.019170089,0.012890934,0.0058616754,-0.003577421,0.005114965,-0.009863364,-0.01546369,-0.00818666,-0.007725057,0.0064251022,0.031741973,-0.0021858246,-0.023080135,-0.0068256105,-0.007697904,-0.024261296,-0.014744134,0.00543062,-0.011553644,-0.037878577,-0.0028612579,-0.016468355,0.011037735,0.023487432,0.0044259555,-0.010582921,0.013590126,0.015165007,-0.027316019,0.0019584175,0.007962647,-0.00084301893,-0.014540485,-0.0005770034,-0.02475005,-0.022442037,0.011580797,0.014146766,-0.054170433,-0.013081006,0.025822598,0.004195154,0.01629186,0.0004299948,-0.008390308,0.013902388,-0.0004357224,0.009456067,0.009870152,0.030085634,0.0065269265,0.008254543,0.0006826459,0.0035604504,0.005753063,-0.030628698,-0.0014823895,0.0042800074,-0.030330013,0.0049418644,-0.007073383,0.008200237,-0.01196094,-0.0011641892,-0.028429296,-0.012761957,-0.012062765,0.048712663,0.022835758,-0.029406808,-0.029026665,-0.0039643524,0.0039202287,0.015762374,-0.02045986,-0.034511592,0.0018345313,-0.017364407,-0.029379655,-0.022795027,-0.013943117,0.03738982,0.010521826,-0.0076435977,-0.025931211,-0.014812016,-0.013956694,-0.013637644,-0.023555314,-0.004694092,-0.009734387,0.02337882,-0.015803104,0.01896644,-0.008044106,0.0016563392,-0.021844668,0.008668628,-0.0057903985,-0.019115783,0.025564644,0.0016427626,-0.033914223,0.018559145,0.0061773304,-0.016617697,-0.0028680463,0.00062027865,-0.02066351,-0.015341502,-0.0044089844,-0.00026537912,0.013230348,0.029461114,-0.0025116617,-0.01392954,-0.012191742,0.02514377,-0.015762374,0.005535838,-0.021708904,0.013298231,-0.0077046924,-0.015422961,-0.02636566,-0.012171377,-0.0039575645,0.0009741175,0.0034161995,0.012341084,-0.0077046924,0.011865905,0.016169671,0.003628333,0.004313949,0.025048735,0.0036894274,0.016278284,0.0153550785,-0.018627027,-0.019998258,0.0019210818,-0.022998676,0.0007556199,-0.010569344,-0.007528197,-0.01125496,-0.021070806,0.008546438,0.019265125,-0.030139942,0.023473855,0.004045812,-0.0007233756,0.018396225,0.012096706,0.0039813234,-0.011275325,-0.017948199,-0.010080588,-0.018789945,-0.016481932,0.04474831,0.015002088,0.017961776,0.006635539,0.22037457,-0.017282948,0.0025031765,0.02821207,0.021342337,0.0011480671,0.012979182,0.027696162,-0.02711237,0.0035163264,0.009958399,-0.013087794,-0.005016535,0.00369961,0.0070937476,0.00323631,-0.037063982,-0.03738982,-0.030601544,0.028537909,0.012225684,0.013997423,0.013535821,-0.026528578,0.05148228,-0.0014111127,-0.01176408,-0.0005799733,-0.001516331,-0.0043750433,-0.006591415,-0.009123442,0.029651185,0.03698252,-0.010603285,-0.017703822,-0.003482385,0.0048739817,0.018898558,-0.0032736456,-0.012490426,0.0069919233,0.004714457,-0.011478974,-0.009822634,-0.003652092,-0.009734387,-0.010141683,0.003036056,0.022306273,-0.016319014,-0.010223142,0.017948199,0.02699018,-0.0014043244,-0.015884563,0.005474744,-0.014649098,0.0048502227,-0.006832399,-0.024369907,0.0073381253,-0.023853999,0.02141022,-0.03263803,0.0021620656,-0.032882404,-0.005084418,0.023609621,-0.019726727,-0.032094963,0.0032566749,-0.014282531,-0.0031684274,-0.027546821,-0.009795481,0.028021999,0.02097577,0.015762374,0.01349509,-0.0069579817,-0.005461167,-0.011675833,-0.018640604,-0.0024403848,-0.007501044,0.015178583,-0.019631691,0.012666921,0.0026440332,0.004164607,0.0017199792,-0.018016081,0.013311807,-0.010657592,0.01865418,0.001344927,0.022251965,0.01349509,-0.02549676,-0.009293148,0.061257396,0.007697904,0.006951194,0.0020042383,0.00588204,-0.0071480535,0.016047481,0.020079717,-0.019115783,0.015300772,-0.050233237,0.0026796716,-0.003187095,-0.012483638,-0.0060924767,-0.005057265,-0.0019516291,0.0022825575,-0.019373737,-0.002791678,-0.024383483,-0.02120657,0.0053016427,0.011349996,-0.01971315,-0.025442455,-0.0019855704,-0.008634686,-0.0028188312,-0.014214648,-0.0111870775,0.0022554044,0.016699156,-0.012361449,-0.025415301,-0.011553644,-0.026338506,0.0062282425,0.009449279,0.0024811146,-0.010453943,0.0038353754,-0.02282218,0.0073449137,-0.01385487,0.01602033,0.011981306,0.004717851,-0.01602033,0.0012643163,0.0034467466,-0.0136104915,-0.0159796,0.021966858,-0.006017806,-0.023704657,-0.015260043,-0.0056139035,-0.0033669844,-0.017106453,-0.027166678,0.020038988,0.013176042,-0.02837499,-0.008471767,-0.17508319,0.011553644,0.019672422,-0.05178096,0.014065306,0.027478937,0.018871404,0.015789527,-0.034647357,-0.011519703,0.021505255,-0.022306273,-0.0019108994,-0.011954152,-0.0048671933,-0.011282113,-0.0138209285,0.014241802,0.014010999,0.014581215,0.052649863,-0.015300772,-0.007921917,0.012022035,-0.017608786,-0.018762792,0.0115264915,0.033262547,0.013345749,-0.051618043,-0.017187912,-0.012904511,0.03187774,0.020269789,-0.011859116,0.0045549325,0.018776368,-0.026732227,-0.022537073,0.0077386335,-0.0051692715,0.01392954,-0.008946947,0.00556978,-0.016264707,0.0040424177,0.03643946,0.0018956258,0.0010971549,-0.011241384,0.014377567,-0.0057225158,0.0072702426,-0.0058854343,0.0019533262,0.009510374,0.017459443,-0.006146783,-0.0057462747,-0.020812852,-0.02031052,-0.012558309,0.014214648,-0.0038693168,0.014649098,0.014201071,-0.014268954,0.020432709,-0.037172593,0.018423378,0.015056394,-0.018097542,-0.004025447,-0.0019261731,9.917246e-05,0.026555732,0.005450985,0.009469644,0.028755134,0.00020354218,-0.010603285,0.040892567,-0.020826427,0.022251965,-0.0060789003,-0.010447156,0.002394564,0.03407714,-0.012694075,-0.028320683,0.0101213185,-0.040539578,0.020772122,-0.002245222,0.017812435,0.0123818135,0.005797187,-0.018844252,0.008743298,-0.008098412,-0.02022906,-0.0024845088,-0.009849787,0.021220148,0.018599873,-0.011485761,-0.01440472,0.014649098,0.03714544,0.0037539161,-0.009157383,-0.0150156645,0.004768763,0.011261748,0.0030309649,0.012741593,-0.010060224,-0.0024403848,-0.018952863,0.023270207,0.046974864,0.009598621,-0.015762374,0.014852745,-0.013148889,-0.01664485,-0.09530738,-0.0043512844,0.017703822,0.02120657,0.0074127964,0.04385226,-0.0001402203,0.012585462,-0.042331684,0.024261296,-0.043906562,-0.018857827,0.0058277342,-0.004069571,-0.0030699973,-0.011546856,0.00089859794,-0.025401725,-0.022387732,0.028592214,0.0034009258,-0.0490385,0.0003156548,-0.010067012,0.01813827,-0.024369907,-0.021491678,0.010358908,-0.001035212,-0.0007933797,0.018016081,-0.038937546,-0.023596045,-0.016074635,-0.010100953,0.02479078,-0.018477684,0.004049206,0.0049791997,0.0007450132,0.022686414,-0.0010046647,-0.022089047,0.00014350838,-0.02093504,-0.01188627,-0.023704657,0.028836593,0.002365714,-0.011125983,-0.027546821,-0.0052201836,-0.009890517,-0.004595662,-0.022455614,0.008057683,0.009374608,0.030031329,0.015558726,0.017282948,-0.013943117,0.01109883,0.005895617,0.023283783,0.022007588,-0.016672004,-0.02369108,-0.0004709366,0.0071955714,-0.005875252,-0.022455614,-0.00082222983,-0.00017352529,-0.0051726657,-0.033425465,-0.0071616303,-0.014106035,-0.0021026682,-0.0030004175,-0.009883729,0.0091505945,-0.008885852,0.006564262,-0.0044123787,-0.02365035,0.019387314,0.014621944,-0.017730974,-0.026772957,-0.017690245,-0.012375025,0.021831093,0.013508667,-0.025673257,0.0055901445,0.017717399,0.007928705,-0.0075349854,0.004588874,0.010202778,-0.008308848,-0.0034959617,-0.045590054,0.021179419,-0.0012074644,-0.025157347,-0.008410673,-0.018803522,0.011702986,-0.010935911,-0.0016257919,0.010141683,-0.012279989,0.0046533626,-0.03133468,0.018450532,-0.01826046,0.0049927766,0.011702986,-0.020812852,0.0073177605,0.0013245621,-0.011472185,0.0124632735,0.010813722,0.023188747,-0.010779781,-0.039453454,-0.028402142,0.009177748,-0.0093135135,0.00873651,0.021586714,-0.008152719,0.0046771215,0.028130611,-0.009788693,-0.008390308,0.013121735,0.006462438,0.03513611,-0.024329178,-0.0059125875,-0.04013228,0.0003328376,-0.007874399,0.032122117,0.00924563,0.0025082675,0.00060330797,-0.017255796,-0.0020076323,0.021912552,0.0071276887,-0.0038930757,-0.033832762,-0.011702986,0.009164171,-0.011234595,-0.016549814,0.020052565,0.0019516291,0.02943396,-0.016305437,0.023392396,-0.00818666,0.004717851,0.0008116231,0.00072507263,0.02105723,0.013712316,-0.025551068,-0.00053500093,-0.007935493,0.005953317,0.03714544,0.031497598,0.015762374,0.010501461,0.0025286325,-0.0072091483,0.04105549,0.031524748,-0.0051217536,-0.026623614,-0.015436538,0.011736928,0.0062316367,0.013644433,0.010990217,0.002615183,0.0047857338,-0.010107742,0.0036113623,-0.0024607498,0.0055663856,0.00094696437,0.002688157,-0.005006353,-0.010650803,-0.006221454,0.015843835,-0.005461167,0.0010326663,-0.014010999,-0.02128803,-0.03956207,-0.0030683004,-0.027343173,-0.025917634,-0.024424214,0.0051658773,-0.0018209548,-0.0026983393,-0.0015977903,0.021233724,-0.010358908,0.025673257,-0.01515143,-0.01471698,-0.032067813,0.018409802,0.009299937,-0.0038930757,0.009714021,-0.0011175198,0.035896398,0.0048773754,0.01912936,-0.029271042,0.033561233,0.0156130325,0.0130606415,0.0015901534,-0.0001606912,-0.011051312,-0.009700445,-0.028972358,-0.0032566749,0.0048773754,-0.0041612126,0.06777414,-0.0017301616,0.009829422,0.031063147,0.00849892,0.026107706,0.02168175,0.006051747,-0.0045277793,-0.017568056,0.013508667,-0.0074263727,-0.020256212,-0.016970688,0.013169253,-0.0019041111,0.0029766585,0.021518832,-0.016929958,-0.018681332,-0.0011370361,-0.0058616754,0.0015333017,0.011716562,0.0102502955,0.0073924316,0.014201071,0.006666086,-0.011675833,-0.03950776,0.002545603,0.024030494,-0.04170716,-0.010725475,-0.0056410567,0.017500173,-0.0074127964,-0.0105897095,0.02010687,-0.009632562,0.00036571833,0.0016597332,-0.016604122,-0.017934622,-0.01813827,0.006771304,0.00017501022,-0.013678375,-0.025211653]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7721817531945423
0.7744211291109637
0.7794232089320886
0.7419849191727467
0.7697445983441444
0.7713613783114579
0.7209108949201515
0.7624758548622544
0.6912513687727937
0.7569783844930356
0.8203804441675748
0.750494372531372
0.765854952001052
0.7510303429197654
0.7682971821127659
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006042189,0.020706378,0.010923463,0.0025899946,-0.0063578766,0.022308577,0.0022166008,-0.0024643985,-0.01584547,-0.04040799,0.0059335656,0.024616828,-0.003941001,-0.0024881598,-0.010658693,-0.013136669,0.012858321,0.015329509,0.0059131985,0.009049705,-0.0023931141,0.011269701,-0.0032077914,-0.0073864064,-0.014311163,0.039348908,0.029898653,-0.01570969,0.015139418,-0.022227108,0.01577758,-0.0041955872,-0.016130608,-0.028649481,-0.014718501,0.02832361,0.010753739,0.0022505457,0.027481776,0.016728038,0.016877396,0.023109676,-0.0044943024,0.004582559,0.0061677853,0.016225653,0.015370242,-0.014949326,-0.02340839,0.009728603,0.0012720844,0.019131336,-0.021629678,-0.0042023766,-0.022267843,0.0016072901,-0.0024219675,0.0046979715,0.009979796,-0.029681405,0.0181673,0.00974897,-0.01857464,0.017990788,-0.026626365,-0.004178615,0.0064902613,0.0008664431,-0.0059471433,0.014704923,0.0140667595,0.023028208,-0.008730624,-0.017542714,0.017135376,-0.023503436,-0.008649156,-0.012097956,-0.007990626,0.017162532,0.009395943,-0.010536492,-0.011466581,0.016673727,-0.0041752206,-0.021928394,0.020787846,0.017732807,-0.034325063,0.009504567,0.009328053,0.02639554,0.0171218,-0.006123657,0.0062831976,-0.005814758,-0.006055767,0.026314072,0.015112261,0.007719066,-0.010950619,-0.0032010025,-0.059254188,-0.018004365,-0.011161077,-0.007488241,0.011188233,0.0042940276,0.0003699992,0.00035875497,-0.03277718,0.033917725,-0.0068500773,-0.033944882,-0.0073049385,-0.0010141033,0.034922495,-0.007209893,-0.0038357717,-0.012478138,0.02122234,0.008411542,0.03131076,-0.024969855,0.024643984,-0.0072438377,-0.01737978,-0.006863655,-0.02340839,-0.022729494,0.02441316,0.024630407,0.018805465,0.018357392,-0.024046555,0.03342892,-0.013292816,0.01710822,-0.023136832,-0.008839248,0.011928231,0.020706378,0.004463752,0.023530593,0.005729896,0.010726583,0.013883457,0.027576823,0.0030906815,-0.009973006,-0.012810798,-0.014827125,0.029165443,0.029355533,0.0042295326,0.011839975,-0.005471915,-0.00037360587,-0.012742909,-0.0031874245,-0.009137962,0.024766186,-0.008520165,-0.009253375,0.02209133,0.013544008,0.024019398,-0.0121319005,0.008445486,-0.0010090116,-0.024983434,0.038181204,-4.879577e-06,-0.008764569,0.0007077508,0.013129881,0.02340839,-0.028024895,-0.011975754,-0.007841268,-0.026938658,0.014609877,-0.00934842,0.0031042595,-0.030523239,0.022919584,0.004222743,-0.029735716,-0.0030431587,-0.0059709046,0.016497213,0.016035562,-0.00042134084,-0.00078242953,-0.6682525,-0.021439588,0.007569709,-0.019402895,0.0070673246,0.016130608,0.011249334,-0.0066837473,-0.038099736,0.033211675,-0.021548212,-0.01519373,0.003601552,-0.02951847,-0.019212803,-0.039973494,0.000437889,-0.02573022,-0.01219979,0.0053972364,-0.0116159385,0.0014248365,-0.006463106,0.016755193,-0.0010769013,0.007861635,0.004290633,-0.024915544,0.02181977,-0.0074203513,0.005916593,0.023937931,0.005325952,0.00013694639,0.023096098,-0.037583772,-0.026151137,0.010129153,-0.0025628386,0.03255993,-0.02533646,-0.017406935,0.008866403,0.004677605,-0.02175188,0.017339045,0.01949794,0.0062187025,0.010407501,0.010285299,-0.005763841,0.014799968,0.013041624,-0.011133921,0.018058678,-0.023842886,0.024318114,-0.013360705,0.011683828,-0.019199226,0.016347855,-0.002759719,-0.029355533,-0.002029904,0.0027936639,0.013944558,-0.00696549,0.001731189,-0.005325952,-0.03125645,0.022525825,0.011290068,-0.00025670818,0.012953367,0.016524369,-0.0010947224,0.04192872,0.01167704,0.018723996,0.0039274227,0.016741617,-0.015288775,-0.019090602,0.0021028854,0.008384386,0.0012831165,0.008730624,-0.002079124,0.003941001,-0.006055767,0.027305264,0.028160674,-0.002746141,-0.025173524,-0.011948599,0.017393358,0.0029837552,0.009545301,-0.0007853997,-0.04382963,-0.004392468,0.0047319164,0.03131076,0.022213532,0.020910047,0.007990626,-0.017583448,-0.0030567367,0.016524369,-0.024983434,0.017013175,-0.0054277866,-0.015302353,0.003628708,-0.00527164,-0.024318114,0.014555565,0.017882165,0.008696679,-0.0043211835,0.009029339,-0.005414209,0.016171342,-0.008968238,0.00080279645,0.006612463,-0.015451711,-0.013584742,0.008486221,-0.0105840145,0.014976482,0.0063510872,0.019810233,-0.025526552,-0.003080498,0.012464561,0.008262184,-0.007882002,0.016945286,-0.0015063041,-0.02733242,0.015424554,-0.0010225896,-0.02043482,-0.021616101,-0.013184193,0.006289987,0.009844016,0.0083368635,-0.0068161325,0.005977694,-0.023204722,-0.0057536573,-0.00027155905,0.0012313506,0.0033367819,-0.0050034756,-0.033700477,0.006785582,-0.026870769,0.012009699,0.038588542,-0.02354417,-0.019823812,-0.004178615,-0.024603251,-0.007929524,0.03462378,-0.0041616424,-0.013360705,0.007841268,-0.015981251,0.0073592504,0.016035562,-0.017963631,0.0032536169,-0.009837227,-0.018927665,-0.0055499882,-0.021344543,0.012383093,0.0030821953,-0.01499006,0.0013688274,0.03535699,0.020991515,0.006289987,0.021181606,-0.028568013,0.027155906,0.0001740736,0.013700155,-0.021412432,0.034488,0.02036693,0.013849512,0.03136507,-0.013258871,-0.02076069,0.017434092,0.027631134,0.025770955,0.022471512,-0.010957408,-0.002308252,-0.010998142,0.0040088906,-0.010156308,0.021412432,0.020394085,0.027223796,0.011303646,-0.006765215,-0.012362726,0.0060387948,0.0025221047,-0.008404753,0.022199953,-0.011120344,0.002975269,0.009185486,-0.0047319164,0.044888712,0.009776126,-0.012104745,-0.012043644,-0.001821143,0.024453893,0.03440653,-0.0023421969,-0.003601552,-0.016972441,-0.0129058445,0.012321992,0.0041276976,0.0076375986,0.00029319891,-0.0018449044,0.033863414,-0.008214662,0.015383821,0.013557586,-0.00034538918,-0.014813547,0.022064174,-0.00037954623,0.02951847,0.026381962,0.0042431103,0.0028954984,0.007454296,-0.0085948445,-0.0030295807,0.021045826,-0.002169078,-0.020719957,0.011643095,0.009477411,0.0054074195,0.030170212,0.029572781,0.0145827215,0.017094642,-0.005502465,0.032668553,-0.02287885,-0.02878526,-0.0006933242,-0.01572327,0.0018737575,0.0030838926,0.008649156,0.0073592504,-0.02315041,0.011514104,0.0027970583,0.034596626,0.014121071,0.009463833,0.026354806,-0.025635175,-0.017800696,0.009728603,0.008642367,-0.012559607,-0.005081549,-0.011446214,0.010509335,-0.020122526,0.032098282,0.0037780653,0.03383626,-0.017271155,-0.008126405,-0.009613191,0.0009827043,0.025757376,-0.017895741,-0.009911906,-0.0085609,-0.0038154048,0.00663283,-0.012946578,-0.013713732,0.008214662,-0.0018228402,-0.0065819127,-0.036904875,0.018981978,-0.010292089,0.026164716,-0.023394812,0.0017956842,0.0006920513,0.025214259,0.022037018,-0.005298796,0.020190416,0.028296454,0.020394085,0.00039312418,-0.02740031,-0.008981816,0.0038765057,0.0073592504,0.026965814,0.0050034756,0.022132063,-0.0104550235,-0.0081331935,0.0002774994,0.0021758669,0.01944363,0.009647136,-0.014704923,-0.0095520895,-0.00696549,0.0034589835,0.015967673,-0.010006951,0.005583933,-0.038507074,-0.03440653,-0.01949794,-0.021833349,0.013829145,0.017189689,0.024372425,0.0014553869,-0.001973895,0.0021911422,0.009443467,0.0035234788,0.0030414613,0.018914089,0.020461975,0.022322154,0.005492282,-0.013971713,0.019239958,0.01446052,-0.0039274227,-0.014487675,0.023761418,-0.003866322,-0.013157036,-0.0018177484,0.013075569,-0.011704195,-0.019620141,0.0028954984,-0.011419059,0.043367982,0.0057876026,0.009463833,0.0010208923,-0.03057755,-0.02520068,-0.004962742,-0.013014467,-0.013272449,-0.04244468,-0.0155196,-0.02009537,-0.021724725,-0.018411703,0.038914412,-0.00636806,0.017719228,0.0032502224,-0.024576094,0.005207145,-0.032668553,-0.012878688,-0.0036898088,-0.027712602,-0.04616504,-0.016768772,0.0050645764,0.026775723,-0.0035438456,-0.013102724,-0.004158248,0.015696114,-0.0073524616,-0.007474663,0.015981251,-0.011079609,-0.0070809023,0.019362161,-0.03236984,0.00079346163,0.0066090687,0.026992971,0.005695951,0.026748568,-8.125556e-05,-0.041711472,0.007175948,-0.007284572,-0.022648025,0.02082858,-0.0044467794,-0.019620141,0.0072981496,-0.010930252,-0.0029667828,-0.011955388,-0.021426009,0.0008329225,0.007596865,-0.015696114,0.012342359,-0.017474825,-0.0048846686,-0.011412269,0.012362726,-0.011228967,0.021602523,0.0069281505,0.018221613,0.011459792,-0.0010802959,0.00044510225,0.0028106363,-0.016035562,0.005108705,0.010237777,-0.02255298,0.0050238427,-0.0022556374,0.0017159138,-0.01452841,0.015057949,-0.024616828,0.032532774,-0.014555565,0.00018510569,-0.020977937,-0.005125677,-0.01850675,0.008907137,-0.0028954984,-0.0064665,0.0009818557,0.010373556,-0.018601796,0.0071080583,-0.007800534,-0.015166573,-0.017447669,0.023367656,0.009165118,0.028214986,-0.009307686,0.012267681,-0.024168756,-0.031745255,-0.022824539,-0.017148955,-0.0051833834,-0.0031059568,0.001953528,0.026544899,0.03568286,5.0333896e-05,0.03421644,0.009979796,-0.016021984,0.017434092,-0.007929524,0.008201083,-0.022430778,0.0015080014,0.013971713,0.0025356826,0.028079206,0.004538431,0.01558749,-1.274922e-05,-0.018194458,-0.010095208,-0.014813547,-0.012783643,-0.0019552251,0.008730624,-0.009640346,-0.0026103614,-0.022145642,0.0055398047,-0.018411703,-0.017203266,0.0008507436,0.0042872387,0.0063646655,-0.0035132952,0.008431909,0.0029464157,0.003859533,-0.025241414,-0.03641607,-0.017352624,-0.010244565,0.0047590723,-0.0075900755,0.023992242,0.01637501,0.012559607,-0.0148407025,-0.013971713,-0.027780492,-0.013958136,0.0048710904,-0.010814839,-0.0024508205,-0.021412432,-0.026911503,0.015370242,0.01260034,0.0047183386,-0.025485817,0.00414467,0.004019074,-0.01777354,0.009565668,0.014284006,0.024168756,0.012383093,0.022240687,0.019226382,-0.025390772,-0.000691627,0.0022267844,0.0072913608,0.0005919139,0.028160674,0.022104908,-0.010393923,-0.020815002,-0.004039441,0.014772813,-0.016741617,-0.017950054,0.031337917,0.011833186,-0.021303808,-0.006541179,-0.015248041,-0.026707834,-0.008662734,-0.007685121,0.0012568092,-0.005027237,-0.003240039,0.004881274,-0.008567688,0.008581266,0.021588946,-0.0023591693,-0.011860342,0.012546028,0.009782915,-0.006911178,-0.002197931,0.0121319005,0.022892429,0.024426738,0.020529864,-0.017895741,-0.0052648513,-0.0027715997,0.0076511763,-0.014718501,0.028350765,-0.019199226,0.012933,0.0058792536,-0.0035370567,0.005159622,-0.009830438,-0.015478866,-0.008167138,-0.0077598,0.0064427387,0.03177241,-0.00225394,-0.023041787,-0.0068161325,-0.007719066,-0.024304535,-0.014786391,0.005444759,-0.011548049,-0.037909646,-0.0028700398,-0.0164429,0.011093187,0.023489859,0.0044060457,-0.010638326,0.013523641,0.01512584,-0.027359575,0.0019993535,0.007983836,-0.0008074639,-0.014596299,-0.0006042189,-0.024752608,-0.022457935,0.011581994,0.0140667595,-0.054176033,-0.013095936,0.025825266,0.00414467,0.016266387,0.00037190862,-0.00835723,0.013971713,-0.00043194863,0.009443467,0.009803282,0.030088743,0.0064834724,0.008316496,0.0007111453,0.0035506347,0.0057095294,-0.030631863,-0.0014630244,0.004219349,-0.030278835,0.0049186135,-0.0070605357,0.008126405,-0.011853552,-0.0012016488,-0.028459389,-0.012708964,-0.012084378,0.04869054,0.022892429,-0.029355533,-0.02905682,-0.0039172396,0.0039240285,0.015831893,-0.020461975,-0.03454231,0.0017821064,-0.01737978,-0.029355533,-0.022797383,-0.01393098,0.03733937,0.010495758,-0.0076375986,-0.025906734,-0.014786391,-0.013869879,-0.013639053,-0.023503436,-0.004728522,-0.0096810805,0.023313345,-0.015764004,0.018954823,-0.008065304,0.0016344461,-0.021806192,0.008642367,-0.0057163183,-0.019131336,0.025580863,0.0016081388,-0.033863414,0.01850675,0.00620173,-0.01657868,-0.0028666453,0.000627556,-0.020597754,-0.015288775,-0.0043788897,-0.00023358323,0.013231715,0.029491313,-0.0025322882,-0.01393098,-0.012186212,0.02520068,-0.01572327,0.005533016,-0.02175188,0.0133471275,-0.0077054882,-0.015465288,-0.026354806,-0.012193002,-0.0039783404,0.0009928878,0.0034335249,0.012321992,-0.0076579656,0.011887498,0.01611703,0.0035913684,0.004331367,0.025092056,0.0036456804,0.016225653,0.015329509,-0.018628951,-0.019959591,0.0018669685,-0.022973897,0.00077903504,-0.010577225,-0.00756292,-0.011242545,-0.02110014,0.0085948445,0.019267116,-0.030170212,0.02347628,0.0040496243,-0.0007383012,0.018384548,0.0120708,0.003964762,-0.011290068,-0.017922897,-0.010068052,-0.01877831,-0.016470056,0.044752933,0.015030794,0.018004365,0.0066226465,0.22050595,-0.017325468,0.0025136184,0.028214986,0.02135812,0.0011015114,0.012980523,0.027685447,-0.027033703,0.0035574236,0.009911906,-0.013143458,-0.0050238427,0.0037067812,0.007148792,0.0032722866,-0.037067812,-0.037447993,-0.030631863,0.028513702,0.012247314,0.013971713,0.013476118,-0.026504165,0.051569063,-0.0012839652,-0.011792452,-0.0005800332,-0.0014961207,-0.004368706,-0.0065954905,-0.009131174,0.02965425,0.036986344,-0.010563647,-0.017705651,-0.0034640753,0.004922008,0.01890051,-0.0032451306,-0.01247135,0.006999435,0.0046674213,-0.011453004,-0.009776126,-0.0036626528,-0.009715025,-0.010183465,0.003024489,0.022308577,-0.016334277,-0.01021741,0.017950054,0.026979392,-0.0014222906,-0.015913362,0.005465126,-0.014609877,0.0048643015,-0.006772004,-0.02434527,0.0073456727,-0.023788573,0.021412432,-0.032614242,0.002126647,-0.032940112,-0.0050000814,0.02361206,-0.019715188,-0.032071125,0.0033147177,-0.014256851,-0.003163663,-0.027508933,-0.009721815,0.02799774,0.020991515,0.01579116,0.013476118,-0.0069587007,-0.0054820986,-0.011690618,-0.018628951,-0.0024694903,-0.007488241,0.015180151,-0.019592986,0.012708964,0.0026867373,0.0041344864,0.0017218542,-0.017950054,0.013374284,-0.010692638,0.018669685,0.0013790108,0.022294998,0.013523641,-0.025499396,-0.009253375,0.061263725,0.0076919105,0.0069145723,0.0019229776,0.0058826483,-0.0070944806,0.016049141,0.020041058,-0.019077023,0.015288775,-0.05021127,0.0026782511,-0.0032094887,-0.012450983,-0.0061100787,-0.0050883377,-0.0019976564,0.0022793987,-0.019389316,-0.0028055445,-0.02434527,-0.02122234,0.005325952,0.0113579575,-0.01970161,-0.025431506,-0.0019569225,-0.0085609,-0.002836095,-0.014202539,-0.011161077,0.0022199953,0.016700882,-0.0124441935,-0.025458662,-0.011548049,-0.026273338,0.0062594363,0.009436677,0.0024355452,-0.010407501,0.003873111,-0.022797383,0.0073456727,-0.013842723,0.016021984,0.011989333,0.0047319164,-0.016035562,0.0013034835,0.0033944882,-0.013625476,-0.016076297,0.021914816,-0.006021822,-0.023693528,-0.015275197,-0.0056212726,-0.0033486625,-0.017067486,-0.027155906,0.020041058,0.013116303,-0.028377922,-0.008486221,-0.17510128,0.011507315,0.019674454,-0.051759157,0.014134649,0.02752251,0.01883262,0.015872627,-0.03456947,-0.01147337,0.021493899,-0.022308577,-0.0019772893,-0.011955388,-0.004843935,-0.011256123,-0.013781622,0.014243272,0.01399887,0.014609877,0.052655302,-0.015329509,-0.00788879,0.011996121,-0.017542714,-0.01876473,0.011554838,0.033265986,0.013340339,-0.05159622,-0.017216844,-0.012844743,0.031881034,0.020244727,-0.011833186,0.004565587,0.018723996,-0.0267893,-0.02255298,0.0077598,-0.0051901727,0.013985291,-0.008947871,0.0054481537,-0.016266387,0.004059808,0.036388915,0.0018754548,0.0011091491,-0.011269701,0.014365474,-0.0057434742,0.007250627,-0.005841914,0.0019942618,0.009518145,0.017461248,-0.006120262,-0.005814758,-0.020787846,-0.020326195,-0.012579974,0.014093915,-0.0038425606,0.014664189,0.014229694,-0.014311163,0.02042124,-0.03720359,0.018452438,0.015030794,-0.018058678,-0.0040428354,-0.0019077024,6.6033404e-05,0.026558476,0.005502465,0.009477411,0.02878526,0.00021151904,-0.01067906,0.040896796,-0.020787846,0.022254264,-0.0060727396,-0.010400712,0.0024100866,0.03402635,-0.012695386,-0.028269298,0.010122363,-0.04051661,0.020733533,-0.00232183,0.017814275,0.0123898825,0.0057808133,-0.018859776,0.008744202,-0.008126405,-0.020203995,-0.0024932516,-0.009844016,0.02122234,0.018601796,-0.011527682,-0.014446942,0.014623456,0.03714928,0.0037644873,-0.009144751,-0.01499006,0.0047590723,0.011228967,0.0030754064,0.012729331,-0.010068052,-0.002481371,-0.0189684,0.023299767,0.04700687,0.009592824,-0.015750425,0.014827125,-0.013089146,-0.016592259,-0.095262915,-0.004317789,0.017746385,0.021249495,0.0073660393,0.04385679,-0.00015582822,0.012552817,-0.0423089,0.02427738,-0.043883942,-0.018859776,0.00577063,-0.004056413,-0.0030686173,-0.011527682,0.0009623374,-0.025363617,-0.022403622,0.028513702,0.0033978827,-0.04901641,0.00031696033,-0.010040896,0.0181673,-0.02439958,-0.021588946,0.010387134,-0.0009971309,-0.00081892026,0.018017944,-0.03894157,-0.023625638,-0.016089873,-0.010074841,0.02480692,-0.018452438,0.0040699914,0.004955953,0.0007807323,0.02268876,-0.00096064014,-0.022104908,0.00013928009,-0.02096436,-0.011907864,-0.02367995,0.028839571,0.0023184353,-0.0110999765,-0.027617557,-0.0051562274,-0.009925484,-0.0045757703,-0.0224172,0.008065304,0.009436677,0.030034432,0.015506023,0.017230421,-0.013971713,0.011106766,0.0059199873,0.023259033,0.021996284,-0.016619414,-0.02367995,-0.00047607697,0.007223471,-0.005903015,-0.022430778,-0.00088256696,-0.00022636993,-0.005200356,-0.03342892,-0.007121636,-0.014093915,-0.0020638488,-0.0029820579,-0.009850805,0.009205852,-0.008927504,0.0065649403,-0.0044671465,-0.023639215,0.019416472,0.014637033,-0.01777354,-0.026816458,-0.017719228,-0.012376304,0.021901239,0.013537219,-0.025703065,0.005611089,0.017732807,0.007929524,-0.007488241,0.0045757703,0.010142731,-0.008330074,-0.0035336623,-0.04562192,0.021127295,-0.0012321993,-0.02513279,-0.0083911745,-0.018751154,0.011710985,-0.0108419955,-0.0016013498,0.01014952,-0.012315203,0.00464366,-0.03131076,0.018425282,-0.018303081,0.0049831085,0.011656673,-0.020801423,0.0074067735,0.0012746303,-0.011500526,0.012457772,0.010794473,0.023204722,-0.010740161,-0.039376065,-0.028459389,0.009171907,-0.009334843,0.008662734,0.021602523,-0.008153561,0.0047522835,0.028079206,-0.009796493,-0.008384386,0.013068779,0.006473289,0.035139743,-0.024318114,-0.005943749,-0.04019074,0.00027537785,-0.0078752125,0.032071125,0.00928732,0.0025458662,0.00057239557,-0.017298313,-0.0020214177,0.02188766,0.0071284254,-0.0038459552,-0.033809103,-0.011751718,0.009158329,-0.011235756,-0.01651079,0.020013902,0.0020112342,0.02951847,-0.016293544,0.023367656,-0.008180717,0.0046911826,0.0007849754,0.0007366039,0.02108656,0.013747677,-0.025567286,-0.0005193567,-0.007922735,0.0059403544,0.037122123,0.03150085,0.015696114,0.01048218,0.002467793,-0.007250627,0.04111404,0.031528007,-0.0051562274,-0.026626365,-0.015397399,0.01167704,0.0061915466,0.013652632,0.011018509,0.002634123,0.004867696,-0.010088419,0.003642286,-0.0024423343,0.005560172,0.00096318603,0.0027155906,-0.0050442093,-0.01061117,-0.006289987,0.015831893,-0.0054006306,0.0010259841,-0.013985291,-0.021317385,-0.039566156,-0.0031110484,-0.027345996,-0.025866,-0.024426738,0.005152833,-0.0018533906,-0.0026952235,-0.0015894691,0.021208763,-0.010400712,0.02573022,-0.015139418,-0.014677767,-0.032043967,0.018411703,0.00928732,-0.0038391661,0.009715025,-0.0011413967,0.03590011,0.0048643015,0.019104179,-0.029246911,0.033537544,0.015641801,0.013048412,0.0015385518,-0.00018956095,-0.011038876,-0.009701448,-0.029002508,-0.0032841673,0.0048778797,-0.0041616424,0.06772683,-0.0017498587,0.009762548,0.0310392,0.008574477,0.026056092,0.021724725,0.0060795285,-0.00450788,-0.017569872,0.013510063,-0.007447507,-0.020244727,-0.016972441,0.013190981,-0.0019077024,0.0030007276,0.021493899,-0.016945286,-0.018656107,-0.0010650207,-0.005835125,0.00150885,0.01173814,0.010203832,0.00742714,0.014161805,0.0066362247,-0.011697406,-0.039511845,0.0025271964,0.024019398,-0.041738626,-0.010699427,-0.0055975113,0.017501982,-0.007454296,-0.010624748,0.02009537,-0.009599613,0.00031632386,0.0016081388,-0.016592259,-0.017950054,-0.018140145,0.006748243,0.00015561607,-0.013639053,-0.025173524]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7690796075996351
0.6868045114496553
0.7469748783049824
0.7691897606693161
0.7578789554528944
0.6723921415417585
0.7591519279659603
0.6954192430156048
0.7841216967498836
0.8046910163347634
0.6156593982507765
0.7094910256453923
0.7712249482743091
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060983095,0.020698939,0.010926704,0.0026162292,-0.006352972,0.02235594,0.002227445,-0.0024838047,-0.015931664,-0.04044714,0.00593193,0.024624132,-0.0039285882,-0.0024872003,-0.010689019,-0.013113403,0.0128893005,0.015415548,0.0059149526,0.00905239,-0.0023836377,0.011306999,-0.0032393024,-0.0074021798,-0.01432899,0.03927909,0.02996185,-0.015727933,0.015157491,-0.022179374,0.015755098,-0.004193437,-0.016135395,-0.028630817,-0.014709285,0.028359177,0.010743347,0.0022597022,0.02748993,0.016760165,0.016895985,0.02310295,-0.0044922405,0.0045669414,0.0062001743,0.016203303,0.015347639,-0.0149265975,-0.02337459,0.00967037,0.001258031,0.019137012,-0.021690425,-0.0042206007,-0.02234236,0.0016272911,-0.002497387,0.0046993657,0.010064248,-0.029690212,0.018131945,0.009758653,-0.018525822,0.018009707,-0.026607102,-0.0041866456,0.0065159556,0.00088707317,-0.0059387214,0.014750032,0.014111678,0.02303504,-0.008692468,-0.01754792,0.017126879,-0.023496827,-0.008665304,-0.012074381,-0.007965832,0.017167624,0.009425895,-0.0105532,-0.0114699835,0.016705837,-0.004203623,-0.021785498,0.020780431,0.01781956,-0.034362413,0.009534551,0.00932403,0.026389789,0.017140461,-0.0061254734,0.0063088303,-0.0058096927,-0.0060371906,0.026240388,0.015075999,0.0077349385,-0.0109742405,-0.0032036498,-0.05916311,-0.01802329,-0.011157597,-0.007497254,0.01117118,0.004305488,0.00036756237,0.0003518582,-0.03284123,0.03392779,-0.006906437,-0.03392779,-0.007225614,-0.0010186486,0.034987185,-0.007232405,-0.0038199322,-0.012427513,0.021215055,0.008420829,0.03134721,-0.024990845,0.024596969,-0.007273151,-0.017317027,-0.006848714,-0.023483245,-0.022749819,0.02440682,0.02467846,0.018811043,0.018390002,-0.024012942,0.03343884,-0.013269596,0.017086131,-0.023184441,-0.008848661,0.011952143,0.020698939,0.0045058224,0.023523992,0.0057621556,0.010743347,0.0139079485,0.027571421,0.0030729233,-0.009975965,-0.012862137,-0.014817942,0.02920126,0.029445736,0.0043156743,0.0118095325,-0.005497307,-0.0003690479,-0.012807808,-0.0031832769,-0.0091203,0.02474637,-0.008529484,-0.00924933,0.022043556,0.013541236,0.024108017,-0.012169455,0.008495529,-0.0010415682,-0.024990845,0.03816537,-1.3807463e-05,-0.008801124,0.0006786746,0.013120194,0.023428917,-0.028006045,-0.011999681,-0.0077892663,-0.026865158,0.014614211,-0.00932403,0.0030508526,-0.030532293,0.022953548,0.004244369,-0.029771702,-0.0029999202,-0.0059387214,0.016502107,0.015985992,-0.00045117643,-0.00074913114,-0.66801614,-0.021445949,0.0075719547,-0.01940865,0.007028675,0.016121812,0.011211925,-0.0066789393,-0.038029548,0.0331672,-0.021541022,-0.015171073,0.0035788522,-0.029472899,-0.019218503,-0.03998535,0.00048003814,-0.025737854,-0.0122102015,0.0053784647,-0.011599012,0.0014702495,-0.006397113,0.01671942,-0.0010568479,0.007925086,0.0042681377,-0.0249501,0.021839825,-0.007449717,0.005942117,0.023972197,0.00538186,0.00016192268,0.02303504,-0.03764925,-0.026158895,0.010193277,-0.0026264156,0.03251526,-0.02535756,-0.0174121,0.008855452,0.0047265296,-0.021717587,0.017384935,0.019462978,0.0062137563,0.010376634,0.010301933,-0.0057757376,0.014831523,0.013011538,-0.011164389,0.018077618,-0.023849959,0.024243837,-0.013412206,0.011748414,-0.019272832,0.016393451,-0.0027588399,-0.029391408,-0.0019965512,0.0028335408,0.013975859,-0.0070083025,0.0017172717,-0.005351301,-0.031292886,0.02255967,0.011286627,-0.00022389047,0.012936837,0.016474944,-0.0011935167,0.041913994,0.011680503,0.018743133,0.0039048195,0.016814493,-0.01529331,-0.01912343,0.0021510462,0.008352919,0.001224076,0.008760378,-0.0020339016,0.003969334,-0.0060236086,0.027381275,0.028196193,-0.0027299782,-0.025153829,-0.011938562,0.017384935,0.0029880358,0.009520968,-0.0007368225,-0.043896962,-0.0044107484,0.004821603,0.03132005,0.022247285,0.020889087,0.008026951,-0.017588666,-0.00306783,0.016474944,-0.02501801,0.016950313,-0.005415815,-0.015374803,0.0036705304,-0.005300368,-0.02433891,0.014505556,0.01788747,0.008746796,-0.0043564206,0.00897769,-0.0053852554,0.01624405,-0.00897769,0.00083571626,0.006634798,-0.015388384,-0.013541236,0.00850232,-0.01055999,0.014953761,0.006390322,0.01980253,-0.02550696,-0.0030457594,0.012495423,0.008271427,-0.00788434,0.016991058,-0.0015322173,-0.027326947,0.015497041,-0.0010407193,-0.020440882,-0.021568187,-0.013188104,0.0062748753,0.009826563,0.008386874,-0.0067977817,0.005982863,-0.023184441,-0.0056908503,-0.00031662994,0.0011858768,0.0033072124,-0.0049506323,-0.033710476,0.006814759,-0.026865158,0.012033636,0.03859999,-0.023537574,-0.019829692,-0.0041866456,-0.024556221,-0.007911504,0.034579724,-0.004132318,-0.013303551,0.007863967,-0.016013157,0.007361434,0.016053902,-0.01795538,0.0032393024,-0.009853727,-0.019014774,-0.005551635,-0.021282965,0.012373185,0.0030797143,-0.015021672,0.0013938508,0.035340317,0.020997742,0.006251107,0.02111998,-0.028549325,0.02720471,0.00015258507,0.013690637,-0.021405203,0.03447107,0.020305062,0.013819666,0.03134721,-0.013222058,-0.020739686,0.017466428,0.027612168,0.025832929,0.022464598,-0.010981032,-0.0023157278,-0.011048942,0.0040270574,-0.010152531,0.021418784,0.0204273,0.027191127,0.011320582,-0.0067977817,-0.012291693,0.005959094,0.002488898,-0.008420829,0.022220122,-0.011116852,0.003013502,0.009188211,-0.0047333203,0.04490203,0.009683953,-0.012115127,-0.012101545,-0.0018267764,0.024433983,0.034389578,-0.0023598692,-0.0036162024,-0.017099714,-0.012882509,0.012352812,0.0041526905,0.0076330733,0.00029455923,-0.0018471495,0.033900626,-0.008169562,0.015415548,0.013554817,-0.0003909064,-0.014817942,0.02213863,-0.00040936942,0.029554391,0.026376208,0.004217205,0.0028895666,0.0074904626,-0.008644931,-0.003008409,0.021011325,-0.0021765125,-0.020698939,0.011626176,0.009473432,0.0054599564,0.030260654,0.029581554,0.0145870475,0.017086131,-0.0054939114,0.03262392,-0.022872057,-0.028766636,-0.00070880965,-0.015714353,0.0018743135,0.0030695277,0.008658513,0.0073546427,-0.023130113,0.011551475,0.0028352386,0.03460689,0.014084514,0.009487014,0.026362626,-0.025615616,-0.017805977,0.009724699,0.008672095,-0.012576914,-0.0050219377,-0.01144961,0.010526035,-0.020114914,0.03208064,0.0037350447,0.03381913,-0.017303444,-0.008067697,-0.0096160425,0.0010432659,0.025792181,-0.017873887,-0.009908055,-0.008597394,-0.003853887,0.006621216,-0.012970792,-0.013683846,0.008223889,-0.0018352652,-0.006587261,-0.036888663,0.018974029,-0.010322305,0.026199643,-0.023361007,0.0018081012,0.00070329197,0.025262484,0.022084301,-0.005300368,0.02029148,0.028359177,0.020386554,0.00039026974,-0.027408438,-0.008964107,0.0039150063,0.007408971,0.026987396,0.0049404455,0.022111466,-0.01048529,-0.008169562,0.00021985832,0.002186699,0.019449398,0.009656789,-0.014709285,-0.009534551,-0.0069471835,0.0034226594,0.015931664,-0.010030293,0.0055787987,-0.038545664,-0.034389578,-0.019490143,-0.02186699,0.013853621,0.017154042,0.024352493,0.0014914713,-0.0020271107,0.0021714193,0.009412313,0.0035618746,0.0029965246,0.018946864,0.020481627,0.02234236,0.0055278665,-0.014003023,0.019286413,0.014559884,-0.003921797,-0.014491974,0.023849959,-0.0038437007,-0.013208477,-0.0017622621,0.013113403,-0.011748414,-0.019707454,0.0029625697,-0.011422446,0.043435175,0.0057655512,0.009412313,0.00094140106,-0.030613786,-0.025180994,-0.004977796,-0.013059075,-0.013201686,-0.042484436,-0.0156056965,-0.020128496,-0.021717587,-0.018457912,0.038925957,-0.0064684185,0.017778814,0.0032528846,-0.02461055,0.005239249,-0.032678246,-0.012855345,-0.0036841123,-0.027720824,-0.046178736,-0.016841657,0.005113616,0.026770085,-0.0035347105,-0.0131677305,-0.0041968324,0.015673606,-0.0073682247,-0.0074768807,0.015985992,-0.011089687,-0.0070965854,0.01932716,-0.03243377,0.00075125333,0.0065804697,0.027014561,0.0056263357,0.02672934,-0.000116401825,-0.041696683,0.007144122,-0.007259569,-0.02262758,0.020861924,-0.0044582854,-0.019612381,0.007273151,-0.010885958,-0.0029676629,-0.011918188,-0.021378038,0.0008191632,0.007646655,-0.015714353,0.012284902,-0.017507173,-0.004909886,-0.011436028,0.012373185,-0.0112526715,0.021636097,0.0069471835,0.018267766,0.01144282,-0.0010721276,0.00046433398,0.0028369364,-0.016026737,0.005066079,0.010179695,-0.022600416,0.0050694747,-0.0022223517,0.0017172717,-0.014614211,0.015035253,-0.024569804,0.03256959,-0.014559884,0.00015449504,-0.02097058,-0.005167944,-0.018525822,0.008882616,-0.0028437274,-0.0064310683,0.0010305329,0.010397007,-0.018566567,0.0070626303,-0.00788434,-0.015184655,-0.017507173,0.023361007,0.009181419,0.028223356,-0.009330821,0.012264529,-0.024162345,-0.031781837,-0.022804147,-0.01720837,-0.005218876,-0.0031374376,0.0019592007,0.026539192,0.035693448,4.204578e-05,0.034199428,0.009874101,-0.01597241,0.017493593,-0.007992996,0.0081967255,-0.02241027,0.0015305195,0.013948695,0.00257039,0.0281147,0.004587314,0.015537786,-4.6077934e-05,-0.018213436,-0.0100506665,-0.014790777,-0.01282139,-0.001930339,0.008719632,-0.009636416,-0.002609438,-0.022179374,0.005588985,-0.018403584,-0.017167624,0.00085269375,0.004366607,0.00633939,-0.003606016,0.008407246,0.0029489878,0.003853887,-0.025208157,-0.036426872,-0.017289862,-0.01021365,0.0047265296,-0.0076262825,0.024026524,0.016420616,0.012617661,-0.014885851,-0.014003023,-0.027788734,-0.013948695,0.004845372,-0.010885958,-0.0024430589,-0.021405203,-0.026892323,0.015361221,0.012597288,0.0047740666,-0.025479797,0.0041391086,0.0040202662,-0.017765231,0.009487014,0.014342573,0.024148762,0.01240714,0.022192957,0.019204922,-0.025411887,-0.0007185717,0.0022580044,0.007341061,0.00053012173,0.028223356,0.022097884,-0.010410588,-0.020807596,-0.0040474306,0.014831523,-0.01678733,-0.017914632,0.03134721,0.01185707,-0.021282965,-0.0065499106,-0.01529331,-0.026702175,-0.008685677,-0.0077009834,0.0011960632,-0.005059288,-0.0032732575,0.0048996997,-0.008617767,0.008631349,0.021649677,-0.0023666602,-0.011836696,0.0125497505,0.009826563,-0.0069268104,-0.0021103004,0.012108336,0.022844892,0.024461148,0.020522373,-0.017941797,-0.005290182,-0.0027130006,0.0076534464,-0.014750032,0.028359177,-0.019232085,0.012916464,0.005857229,-0.0035788522,0.0051102205,-0.009880891,-0.015456295,-0.008217098,-0.007755311,0.006485396,0.031754673,-0.0022155608,-0.023062205,-0.006848714,-0.0077009834,-0.024284583,-0.014763613,0.0054327925,-0.01158543,-0.037948057,-0.0028233544,-0.016447779,0.011042151,0.02351041,0.0043767933,-0.010661855,0.013541236,0.015103163,-0.027367692,0.0019405256,0.007931877,-0.0008161922,-0.01453272,-0.00057341426,-0.024759952,-0.022437433,0.01158543,0.0141252605,-0.05421927,-0.013086239,0.025819346,0.004166273,0.01631196,0.00037711218,-0.008312172,0.013935112,-0.000407035,0.009432686,0.00986731,0.030151999,0.0064684185,0.0082918,0.00068546564,0.003612807,0.0056874547,-0.030695278,-0.0014965646,0.004244369,-0.030342147,0.004933655,-0.007028675,0.008203517,-0.01186386,-0.0011765391,-0.028467832,-0.012787435,-0.012108336,0.04867782,0.02288564,-0.029364243,-0.029038277,-0.0039116107,0.0039387746,0.015809426,-0.020481627,-0.034498233,0.0018250787,-0.017425682,-0.029364243,-0.02283131,-0.013962276,0.037323285,0.010519245,-0.0076398645,-0.025928002,-0.014817942,-0.013833248,-0.013615936,-0.02351041,-0.004682388,-0.00967037,0.023361007,-0.015850171,0.019001191,-0.008026951,0.0016671882,-0.021771915,0.008719632,-0.005724805,-0.01912343,0.025588453,0.0016137091,-0.03381913,0.018539404,0.0062103607,-0.016610762,-0.0028624025,0.0006213756,-0.02064461,-0.0152661465,-0.004390375,-0.00029455923,0.013208477,0.029500064,-0.002529644,-0.013921531,-0.0122102015,0.025153829,-0.015741516,0.0055176797,-0.021744752,0.013283177,-0.007721356,-0.015469877,-0.026349043,-0.012183038,-0.003955752,0.0010152531,0.0034345435,0.012325648,-0.0076738195,0.011884234,0.016162558,0.003639971,0.0043088836,0.025085919,0.0036705304,0.01625763,0.015320475,-0.018593732,-0.02001984,0.0018997797,-0.02296713,0.00075210223,-0.010566781,-0.0075176265,-0.011218716,-0.0211064,0.008549857,0.019245667,-0.030151999,0.023456082,0.00401008,-0.000719845,0.018390002,0.012108336,0.00394217,-0.011232299,-0.017928215,-0.010098203,-0.018824626,-0.016488526,0.04482054,0.015008089,0.017996125,0.0066314023,0.22057137,-0.017289862,0.0024855025,0.028196193,0.021350874,0.0011247578,0.012991165,0.027720824,-0.027096052,0.0035415017,0.009935219,-0.0130726565,-0.0050491015,0.0037214628,0.0070830034,0.003249489,-0.03707881,-0.037404776,-0.03064095,0.02852216,0.012237365,0.013948695,0.013514072,-0.026512027,0.051448546,-0.00131915,-0.011782369,-0.0005483725,-0.0015220308,-0.004373398,-0.006526142,-0.009099928,0.029663047,0.03699732,-0.0105735725,-0.017724486,-0.0034566142,0.004872536,0.0189197,-0.0032613731,-0.012461468,0.0070015113,0.0046857833,-0.011463192,-0.0097722355,-0.003639971,-0.009724699,-0.010138949,0.0030797143,0.02234236,-0.01631196,-0.010254396,0.017928215,0.027028143,-0.0014753427,-0.015863754,0.005500702,-0.014614211,0.00486914,-0.006828341,-0.024379656,0.0073546427,-0.02379563,0.021418784,-0.03262392,0.002145953,-0.032922722,-0.004994774,0.023591902,-0.019721037,-0.03208064,0.00332419,-0.01426108,-0.0031527174,-0.02755784,-0.009758653,0.027951717,0.02097058,0.015727933,0.013466534,-0.0069471835,-0.0054599564,-0.011660131,-0.018688805,-0.002497387,-0.0074904626,0.0151439095,-0.019639544,0.012699152,0.002656975,0.004179855,0.0017469823,-0.017996125,0.013337506,-0.010682228,0.018688805,0.001372629,0.022260867,0.013527653,-0.025547707,-0.009303657,0.0612819,0.007707774,0.0069369967,0.0019863648,0.0059013707,-0.0071712863,0.01604032,0.020074168,-0.019069102,0.0152661465,-0.050226167,0.0026790458,-0.0031849747,-0.012542959,-0.0061051007,-0.00507287,-0.0019490143,0.002320821,-0.019381488,-0.0027656308,-0.02440682,-0.021242218,0.005341114,0.011368118,-0.01973462,-0.02550696,-0.0019269435,-0.008665304,-0.002831843,-0.014206752,-0.011150806,0.0022512134,0.01671942,-0.0123935575,-0.02542547,-0.01151752,-0.026281133,0.006223943,0.009466641,0.0025058754,-0.010383424,0.0038810512,-0.022804147,0.007388598,-0.013840039,0.016053902,0.011979307,0.0047401115,-0.016053902,0.0012614265,0.0034277525,-0.013663474,-0.016013157,0.0219349,-0.0060304,-0.02371414,-0.0152661465,-0.0055923807,-0.0033649358,-0.017126879,-0.027177544,0.020074168,0.013154149,-0.028386341,-0.008543066,-0.17515323,0.011524311,0.019721037,-0.051774513,0.014070933,0.027530676,0.018824626,0.015809426,-0.034634054,-0.011483565,0.021473112,-0.022315195,-0.0019727827,-0.011972517,-0.004797835,-0.01131379,-0.01377892,0.0142474985,0.013948695,0.014627794,0.052616596,-0.015347639,-0.007870758,0.012006471,-0.01754792,-0.01878388,0.011558266,0.033275854,0.013385043,-0.051557202,-0.017235534,-0.012896091,0.031890493,0.020237152,-0.0118706515,0.00456015,0.018702388,-0.026783668,-0.022546088,0.0077485205,-0.0052120853,0.014016605,-0.0089369435,0.005551635,-0.016271213,0.0041017584,0.036454037,0.0018624292,0.001103536,-0.011239089,0.014356154,-0.005718014,0.00726636,-0.0058504385,0.0020016446,0.009548133,0.017507173,-0.006091519,-0.005799506,-0.020807596,-0.020332227,-0.012576914,0.014152424,-0.0039014241,0.01460063,0.014193171,-0.014233916,0.020413717,-0.03721463,0.018471494,0.015048835,-0.01810478,-0.0040508257,-0.0018912909,7.889221e-05,0.026566355,0.0054565608,0.009473432,0.028739473,0.00018823777,-0.010614319,0.04093609,-0.020834759,0.022233704,-0.006081332,-0.010397007,0.0023904287,0.034063607,-0.012801018,-0.028223356,0.010104994,-0.040528633,0.020794014,-0.002280075,0.01781956,0.012373185,0.0058096927,-0.018878954,0.008740005,-0.008101651,-0.02022357,-0.0024838047,-0.009799399,0.021201473,0.018634478,-0.011483565,-0.014383318,0.014654958,0.037133135,0.0038063503,-0.009181419,-0.014967343,0.0047740666,0.011266253,0.0030576435,0.012780644,-0.010064248,-0.0024328723,-0.018946864,0.023252351,0.047047984,0.00959567,-0.015741516,0.014817942,-0.013126985,-0.016624345,-0.095345505,-0.004373398,0.017751649,0.021228638,0.007375016,0.0438698,-0.00015481336,0.012542959,-0.042348616,0.024271,-0.043924127,-0.018865371,0.0058029015,-0.004094967,-0.00306783,-0.0115922205,0.0008590603,-0.025371142,-0.02242385,0.028630817,0.003378518,-0.049030952,0.0002918004,-0.010043875,0.018159108,-0.024379656,-0.021541022,0.010335888,-0.0010245907,-0.0008085523,0.018050453,-0.038980287,-0.023605483,-0.016067484,-0.010104994,0.024773534,-0.018444331,0.0040202662,0.004984587,0.00075719546,0.022722654,-0.0009872402,-0.022070719,0.000156405,-0.020916251,-0.011897815,-0.023700558,0.028848128,0.0023293097,-0.011110061,-0.027612168,-0.005195108,-0.009928429,-0.0046178736,-0.022464598,0.008060906,0.009419104,0.029989015,0.01556495,0.017249117,-0.013935112,0.0111304335,0.0058911843,0.023225188,0.022057138,-0.016665092,-0.02371414,-0.0004991378,0.007232405,-0.0059115575,-0.022451015,-0.0008582114,-0.00019863648,-0.005218876,-0.03343884,-0.00713054,-0.014138842,-0.00207295,-0.0029948268,-0.0098944735,0.009222166,-0.008930152,0.0066042384,-0.0044447035,-0.02357832,0.019435816,0.014641375,-0.017724486,-0.026824413,-0.017697321,-0.012420721,0.021907736,0.013520862,-0.025656363,0.005612754,0.017710904,0.007925086,-0.007497254,0.0046178736,0.010193277,-0.008332546,-0.0035143376,-0.045608293,0.021147145,-0.0012249249,-0.025153829,-0.008454783,-0.018797463,0.011687295,-0.010885958,-0.0016315355,0.010159322,-0.012284902,0.0046552243,-0.03140154,0.01849866,-0.018254183,0.004977796,0.011680503,-0.020821176,0.007327479,0.0012478445,-0.011483565,0.01248184,0.01083163,0.023157278,-0.010716183,-0.039442074,-0.02844067,0.009188211,-0.009303657,0.008712841,0.021608932,-0.008135607,0.004760484,0.028087538,-0.009785817,-0.008346127,0.013120194,0.0064684185,0.035150167,-0.024325328,-0.0059013707,-0.040202666,0.00032151098,-0.007850385,0.032107804,0.009235747,0.0024685252,0.0006069448,-0.017303444,-0.001977876,0.02186699,0.007089794,-0.0039048195,-0.03381913,-0.011768787,0.009161047,-0.011279835,-0.016515689,0.02001984,0.0019727827,0.029500064,-0.016352706,0.02337459,-0.008183144,0.004736716,0.00079030154,0.00073427585,0.021092817,0.013792502,-0.025534125,-0.00058402517,-0.00795225,0.005945512,0.037105974,0.03148303,0.015755098,0.0105124535,0.0024515477,-0.007212032,0.041099075,0.031510197,-0.00517813,-0.026647847,-0.015442712,0.011687295,0.006230734,0.013609145,0.010987823,0.0026145314,0.004811417,-0.010186486,0.003667135,-0.0024023128,0.0055821943,0.0009091439,0.0026824414,-0.0050626835,-0.010621109,-0.006237525,0.015850171,-0.005422606,0.0010322306,-0.014003023,-0.021282965,-0.03960506,-0.00306783,-0.027326947,-0.025887256,-0.024420403,0.005181526,-0.0018522427,-0.002656975,-0.0016400242,0.021242218,-0.010369843,0.025697108,-0.015116745,-0.014722868,-0.03208064,0.018417167,0.009303657,-0.0038572827,0.00966358,-0.0011043849,0.035937924,0.0048861178,0.019137012,-0.029255588,0.033547495,0.015619278,0.013086239,0.0015627767,-0.00017306415,-0.010994614,-0.009690744,-0.028983949,-0.0032613731,0.0047944393,-0.0041391086,0.06758394,-0.0017334004,0.009826563,0.031075573,0.0085362755,0.026077405,0.021717587,0.006040586,-0.004526195,-0.0175615,0.013514072,-0.0074565075,-0.020250734,-0.01693673,0.013201686,-0.0019150594,0.002965965,0.021513859,-0.016977476,-0.01871597,-0.0010950472,-0.005874207,0.0014906224,0.011700877,0.01028156,0.007422553,0.014111678,0.0066755437,-0.011639758,-0.039523564,0.0024668274,0.024080852,-0.04175101,-0.010736556,-0.0056263357,0.01748001,-0.0074021798,-0.0106279,0.02008775,-0.009656789,0.00037053344,0.0015992783,-0.01665151,-0.01795538,-0.018159108,0.006774013,0.00018494838,-0.013704219,-0.025194576]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006017293,0.020675322,0.010921403,0.0026879278,-0.006346496,0.022317946,0.002280666,-0.0024435706,-0.015951086,-0.040400367,0.0059086895,0.024652913,-0.0039402577,-0.0024435706,-0.010643108,-0.013079891,0.01287626,0.015435222,0.005939234,0.009088725,-0.002436783,0.011308302,-0.0032326404,-0.007351075,-0.01434919,0.039314337,0.029947316,-0.015693154,0.01520444,-0.022222918,0.015747456,-0.004184615,-0.016154718,-0.028616928,-0.0147293005,0.02839972,0.010765286,0.0022467275,0.02751732,0.016724883,0.016887788,0.023118893,-0.0044595166,0.0045850887,0.006170016,0.016127566,0.015353769,-0.015000809,-0.02330895,0.009706406,0.0012998438,0.019141303,-0.021666326,-0.0041812207,-0.02235867,0.001678258,-0.002506357,0.004697086,0.009977913,-0.029648658,0.018109573,0.009760708,-0.018530412,0.017960245,-0.026648495,-0.004204978,0.0065161884,0.00088070356,-0.005929053,0.0147293005,0.014091257,0.023064591,-0.00872219,-0.017552983,0.017159296,-0.02348543,-0.008640737,-0.012048161,-0.00801627,0.017132144,0.009400959,-0.010554868,-0.011450844,0.016684158,-0.004157464,-0.021869957,0.020811077,0.017838066,-0.034372892,0.00960459,0.009278781,0.026390564,0.017172871,-0.006125896,0.006298982,-0.005806874,-0.006095351,0.026241234,0.015068686,0.007751549,-0.010948554,-0.003198702,-0.059188712,-0.01800097,-0.01113861,-0.007459678,0.011172548,0.004313581,0.00039708024,0.00033005173,-0.032743845,0.03388418,-0.006889512,-0.033938482,-0.007174595,-0.0010588806,0.03494306,-0.0072085336,-0.0037400206,-0.012448635,0.021245489,0.008382805,0.031277705,-0.024992298,0.02454431,-0.0073171365,-0.017376503,-0.0068318164,-0.023444703,-0.022725208,0.024381405,0.02472079,0.018801918,0.018367507,-0.024028445,0.033449765,-0.013297097,0.017145721,-0.023173194,-0.008783279,0.011953133,0.020716049,0.0044968487,0.023526154,0.005762754,0.010819588,0.013941928,0.027585197,0.0030561602,-0.009943975,-0.012835533,-0.014756451,0.029159943,0.0294043,0.004293218,0.011817379,-0.0055048214,-0.0003788383,-0.0128083825,-0.0031477942,-0.009122664,0.02474794,-0.008504983,-0.00925163,0.022019286,0.013595755,0.024123473,-0.012136401,0.008498196,-0.0010537899,-0.024978722,0.038146853,3.9175084e-06,-0.008817217,0.00072076847,0.01314098,0.023444703,-0.027992459,-0.012061737,-0.00783979,-0.026892852,0.014593547,-0.0093330825,0.0030765233,-0.030517483,0.022888111,0.004184615,-0.029784411,-0.0030035556,-0.0059562037,0.016521253,0.015951086,-0.00041107985,-0.0007572524,-0.6681265,-0.021462696,0.0075886445,-0.019372085,0.007065992,0.016100416,0.011172548,-0.0066994564,-0.03803825,0.03317826,-0.021571299,-0.015177289,0.0035805097,-0.029512903,-0.019168453,-0.039993107,0.0004581695,-0.025698218,-0.012278942,0.0053724614,-0.011566235,0.0014296585,-0.006370253,0.016697733,-0.0010122152,0.007900879,0.004238916,-0.024883695,0.02182923,-0.0074325274,0.005939234,0.023933416,0.0053724614,0.00018719194,0.023010291,-0.03765814,-0.026159782,0.010201908,-0.0026472015,0.032526642,-0.025331682,-0.017390078,0.00881043,0.0047038733,-0.021788506,0.017349351,0.019494263,0.0061971666,0.01039875,0.010269784,-0.0058102678,0.014810753,0.012971288,-0.011158973,0.018109573,-0.023811238,0.024272801,-0.013371762,0.011722351,-0.019290633,0.016399074,-0.0027456232,-0.029431451,-0.0019735226,0.0028253787,0.013982655,-0.006970964,0.0017071055,-0.005386037,-0.031277705,0.02248085,0.011287939,-0.00021147916,0.012971288,0.016466951,-0.0012200885,0.041975114,0.011681626,0.018774768,0.0038927437,0.016711308,-0.015258742,-0.019087002,0.002100792,0.008376017,0.001228573,0.0087764915,-0.0020210366,0.003909713,-0.006007111,0.027327266,0.028236816,-0.0027371384,-0.0251009,-0.0119734965,0.01747153,0.002950951,0.009536713,-0.0007347681,-0.04382137,-0.0043712766,0.004781932,0.031304855,0.022222918,0.020892529,0.008029845,-0.017566558,-0.0030459787,0.016480526,-0.024978722,0.01691494,-0.005426763,-0.01538092,0.0036314174,-0.0052808276,-0.024286376,0.014484944,0.017851641,0.008749341,-0.0042592795,0.008952972,-0.005321554,0.016249744,-0.009007273,0.000771252,0.006618004,-0.015408071,-0.013527879,0.008532134,-0.010582019,0.014973658,0.006431342,0.019820074,-0.025535313,-0.0030273125,0.01246221,0.008253839,-0.007826214,0.01694209,-0.0015221409,-0.027313689,0.015516673,-0.0010470022,-0.020498842,-0.021584874,-0.013134193,0.006282013,0.0098760985,0.00836923,-0.00679109,0.0060138986,-0.02313247,-0.0056846957,-0.00028911344,0.0012133007,0.0032530033,-0.0049108984,-0.033721276,0.0067876964,-0.0268114,0.012034586,0.038635567,-0.02351258,-0.019833649,-0.0041914023,-0.024598612,-0.007880515,0.034617253,-0.0041201315,-0.013303884,0.007900879,-0.016018962,0.007398589,0.01605969,-0.017933093,0.0032122773,-0.009862523,-0.018937673,-0.005565911,-0.021313366,0.012360395,0.0031121587,-0.015014384,0.0014050531,0.03524172,0.021001132,0.0062412866,0.02112331,-0.028562626,0.027177935,0.00012758748,0.01367042,-0.021367667,0.0345901,0.020322362,0.013785811,0.03144061,-0.013222433,-0.020716049,0.01744438,0.027612349,0.025861124,0.022412973,-0.010921403,-0.0023281798,-0.011030006,0.004035285,-0.010106879,0.021408394,0.020390239,0.027177935,0.0113422405,-0.0067978776,-0.012258579,0.005929053,0.002435086,-0.008409956,0.02218219,-0.011097884,0.0029848893,0.009190541,-0.0047174487,0.044853095,0.009760708,-0.012082099,-0.01210925,-0.0018445564,0.024462856,0.034454346,-0.0023756938,-0.0036789314,-0.017077845,-0.012930562,0.012387546,0.0041472823,0.007656521,0.00029929497,-0.0017834671,0.03391133,-0.008219901,0.015421646,0.013561817,-0.0003979287,-0.014824329,0.022141466,-0.00044671525,0.029512903,0.026404139,0.004225341,0.0029170124,0.007534343,-0.008606799,-0.0030595541,0.021028282,-0.0022043043,-0.020661747,0.011654475,0.009394172,0.0054199756,0.030245975,0.029621506,0.014607122,0.01711857,-0.005525185,0.03260809,-0.02283381,-0.028834134,-0.0007110112,-0.015733879,0.0018920703,0.003085008,0.0086882515,0.0073171365,-0.02315962,0.011573022,0.0027982278,0.034698702,0.014091257,0.009482412,0.026363412,-0.025657492,-0.017770188,0.009747132,0.00872219,-0.012564026,-0.0050161076,-0.0114440555,0.010500566,-0.020050855,0.03206508,0.00371287,0.03380273,-0.017362926,-0.0080434205,-0.00960459,0.0009816706,0.025820397,-0.01782449,-0.009889673,-0.008613586,-0.003885956,0.0065942467,-0.012984863,-0.013683995,0.008260626,-0.0018700103,-0.0065840655,-0.036816463,0.018978398,-0.01028336,0.026241234,-0.023417553,0.0018479503,0.0007339197,0.02522308,0.022114314,-0.00522992,0.020308787,0.02834542,0.020376664,0.00044629103,-0.02736799,-0.008925821,0.0038621991,0.007459678,0.027096484,0.0049312613,0.022141466,-0.010493779,-0.00815881,0.00024032687,0.0021805475,0.019439962,0.009719981,-0.01470215,-0.009482412,-0.007018478,0.003378576,0.015978238,-0.010005064,0.0055964557,-0.038581267,-0.034400046,-0.019548565,-0.021897107,0.013880839,0.017172871,0.02436783,0.0015017778,-0.0020057643,0.0021567906,0.009407747,0.0035872974,0.0030900987,0.018937673,0.020485267,0.022412973,0.00553876,-0.014036956,0.019290633,0.014471368,-0.003964015,-0.014566396,0.023784088,-0.00384523,-0.013161343,-0.0017936487,0.013100253,-0.011715564,-0.019697895,0.0029865864,-0.011437268,0.043414105,0.005762754,0.009373808,0.0009774283,-0.030571783,-0.025209503,-0.005002532,-0.013066315,-0.013242795,-0.042463828,-0.015584551,-0.020132307,-0.02180208,-0.018435383,0.038907073,-0.0064720684,0.017770188,0.003220762,-0.024598612,0.005226526,-0.032662395,-0.012842321,-0.0036619622,-0.027748102,-0.046156336,-0.016792761,0.0051145293,0.026729947,-0.0035431774,-0.013147768,-0.0041676457,0.01570673,-0.0073782257,-0.007480041,0.015991813,-0.011023219,-0.007106718,0.019344935,-0.032390885,0.00074919197,0.0065501267,0.02698788,0.0056270002,0.026784249,-7.763428e-05,-0.041757908,0.007154232,-0.007330712,-0.02263018,0.020878954,-0.004493455,-0.019589292,0.007262835,-0.010941766,-0.0029611324,-0.01193277,-0.021435544,0.0008459166,0.0075547057,-0.01570673,0.012272155,-0.017539406,-0.0048667784,-0.011484782,0.01237397,-0.011206486,0.021639176,0.0069302376,0.018340355,0.011437268,-0.0011343937,0.00044120025,0.0028236816,-0.016086841,0.0050839847,0.010181544,-0.022575878,0.0050602276,-0.002236546,0.0016952271,-0.014607122,0.015014384,-0.024612186,0.032580942,-0.0145392455,0.00018072242,-0.02091968,-0.0051688305,-0.018557562,0.0088783065,-0.0028745893,-0.0064483113,0.001048699,0.010378388,-0.018571137,0.007032053,-0.007900879,-0.015177289,-0.01747153,0.02330895,0.009210904,0.028182514,-0.009360233,0.012258579,-0.024204925,-0.03176642,-0.022738783,-0.017200023,-0.005162043,-0.003149491,0.001944675,0.026553467,0.035676133,-5.7960562e-05,0.03423714,0.009916824,-0.01596466,0.017498681,-0.00795518,0.008152023,-0.022372248,0.0015382617,0.013955504,0.0025827184,0.028101062,0.0046393904,0.015543824,-1.0002572e-05,-0.018204601,-0.010079729,-0.014837904,-0.012801595,-0.0019294027,0.008667888,-0.009733557,-0.0025555675,-0.022236492,0.005654151,-0.018435383,-0.017104995,0.0008467651,0.0043475195,0.0062650437,-0.0035194205,0.008396381,0.002995071,0.003913107,-0.02522308,-0.036436353,-0.017335776,-0.010181544,0.0046835104,-0.0076429457,0.02401487,0.016358348,0.012597964,-0.014865055,-0.014036956,-0.027788829,-0.013941928,0.0048158704,-0.010731348,-0.002480903,-0.021435544,-0.0268114,0.01538092,0.012591177,0.0047004796,-0.02542671,0.004133707,0.004035285,-0.017810915,0.0095095625,0.01437634,0.024109896,0.01246221,0.022195768,0.019236332,-0.02539956,-0.0006897996,0.0022043043,0.0073646503,0.0005553184,0.028155364,0.022046437,-0.010358024,-0.020756776,-0.004062436,0.014837904,-0.016847063,-0.017919518,0.03135916,0.011837742,-0.021326942,-0.0065365513,-0.0152858915,-0.026716372,-0.008654313,-0.00769046,0.0012277246,-0.005067015,-0.003285245,0.004836234,-0.008606799,0.008681463,0.0216256,-0.0023587244,-0.011837742,0.012577601,0.009828584,-0.006970964,-0.0020719443,0.012061737,0.022901688,0.024503583,0.020539569,-0.017905943,-0.005267252,-0.0027388355,0.00769046,-0.014756451,0.028291117,-0.019168453,0.012937349,0.005874751,-0.003573722,0.0051077413,-0.009848948,-0.015462372,-0.008185961,-0.007717611,0.0064958255,0.03173927,-0.0022280612,-0.023051016,-0.00683521,-0.007710823,-0.024259226,-0.014797178,0.005403006,-0.011600173,-0.037983947,-0.0028779833,-0.016466951,0.011084308,0.02351258,0.004354307,-0.010663471,0.013575393,0.015136562,-0.027395142,0.001942978,0.007968755,-0.0008450682,-0.0145528205,-0.000503138,-0.024815818,-0.0224537,0.011613748,0.01417271,-0.054192968,-0.013066315,0.025861124,0.0042558857,0.01631762,0.00044841217,-0.008403168,0.013928353,-0.00039623177,0.009448473,0.009835372,0.03005592,0.0064618867,0.008240263,0.00068301195,0.0036042668,0.005769542,-0.030626085,-0.0015246862,0.004262673,-0.030327426,0.0049550184,-0.0070524164,0.0081316605,-0.0117970165,-0.0011335453,-0.028454022,-0.012754081,-0.012034586,0.048599906,0.022874536,-0.0294043,-0.02902419,-0.003885956,0.003964015,0.015842482,-0.02044454,-0.034481496,0.0018700103,-0.017430805,-0.02937715,-0.022860961,-0.0139962295,0.03738663,0.010493779,-0.0075886445,-0.025942575,-0.014810753,-0.0138061745,-0.013697571,-0.023458278,-0.0047072675,-0.0096996175,0.023295373,-0.015828907,0.018964823,-0.008097721,0.0016553494,-0.021761354,0.008681463,-0.005718634,-0.019127728,0.025521738,0.0015900178,-0.03382988,0.018584711,0.006102139,-0.01656198,-0.002859317,0.0006007111,-0.020634597,-0.015326618,-0.0044357595,-0.0003145673,0.013236008,0.029512903,-0.002547083,-0.013941928,-0.012211066,0.025209503,-0.015720304,0.00553876,-0.021761354,0.013269946,-0.0077243983,-0.015489522,-0.026390564,-0.012211066,-0.0039843777,0.0010003367,0.0034498465,0.01228573,-0.007683672,0.011898831,0.016154718,0.0036246297,0.004269461,0.0250466,0.0036314174,0.016222594,0.015367344,-0.018625438,-0.020050855,0.0018717073,-0.022969564,0.0007865243,-0.010595594,-0.007480041,-0.011220062,-0.021041859,0.008525346,0.01920918,-0.030191673,0.023417553,0.0039945594,-0.0006673154,0.018394656,0.01210925,0.0039266823,-0.011274364,-0.017919518,-0.0100661535,-0.018815493,-0.016534828,0.044744495,0.01502796,0.018014546,0.0066383667,0.22035576,-0.017335776,0.0024673275,0.028209666,0.021367667,0.0011700292,0.012984863,0.027707376,-0.027110059,0.0035194205,0.009984701,-0.013154555,-0.0050975597,0.00371287,0.0070456285,0.0032988205,-0.037087973,-0.037413783,-0.030653236,0.028535474,0.012245004,0.0139962295,0.013514304,-0.026499165,0.051450737,-0.0013516,-0.0117970165,-0.0005553184,-0.0015085655,-0.0043882458,-0.006543339,-0.00916339,0.029648658,0.03703367,-0.010629532,-0.017743038,-0.0034651188,0.0048837475,0.018924097,-0.0032954265,-0.012448635,0.0069404193,0.0046631475,-0.011457631,-0.00978107,-0.0036382051,-0.0096996175,-0.010127243,0.0030680387,0.02230437,-0.016344773,-0.010235846,0.017946668,0.027028607,-0.0014627485,-0.01588321,0.005453914,-0.014634273,0.004836234,-0.0068589672,-0.02439498,0.0073782257,-0.023824815,0.021394819,-0.03263524,0.0021330335,-0.03279815,-0.0049923505,0.023566881,-0.019725045,-0.03209223,0.0033395465,-0.014240587,-0.003152885,-0.027625924,-0.009753919,0.027938157,0.020960405,0.015733879,0.013480364,-0.0069132685,-0.0054573077,-0.01166805,-0.01867974,-0.0024384798,-0.007507192,0.015190864,-0.019657169,0.012774444,0.0026845338,0.004194796,0.0017563164,-0.017987395,0.013283522,-0.010622744,0.018734042,0.0013838416,0.022263644,0.013541454,-0.025494587,-0.009244842,0.06125217,0.007683672,0.0069913273,0.0019344934,0.005912083,-0.0071610194,0.016046114,0.02009158,-0.019100577,0.0152723165,-0.05017465,0.0026251415,-0.0031783388,-0.012516512,-0.006125896,-0.0050873784,-0.0019107364,0.0023349675,-0.01938566,-0.0027829555,-0.024408557,-0.021191187,0.005311372,0.011328665,-0.019697895,-0.025494587,-0.0019039488,-0.0086882515,-0.0027727738,-0.014254162,-0.011152185,0.002238243,0.016670583,-0.012333244,-0.025467437,-0.01157981,-0.02628196,0.0062039546,0.009468837,0.0025182352,-0.010439477,0.0039063194,-0.02280666,0.007351075,-0.013840113,0.016086841,0.011946346,0.004741206,-0.016073264,0.0012353607,0.0033887574,-0.013636482,-0.015923936,0.021910684,-0.006007111,-0.023675485,-0.0152723165,-0.005637182,-0.0033191834,-0.017104995,-0.027177935,0.020078005,0.013202069,-0.02839972,-0.008538922,-0.17506826,0.011552659,0.019697895,-0.051749397,0.014091257,0.027530896,0.018883372,0.015842482,-0.034644403,-0.011525508,0.02144912,-0.02235867,-0.0019277057,-0.011919195,-0.004873566,-0.011294726,-0.013738297,0.014254162,0.0139962295,0.014593547,0.052699674,-0.015353769,-0.007873728,0.011946346,-0.017566558,-0.018747617,0.011539084,0.033232562,0.0133649735,-0.051613644,-0.017213598,-0.012937349,0.03190217,0.020213759,-0.011851317,0.0045443624,0.018734042,-0.026743524,-0.022521576,0.007812639,-0.005117923,0.0139962295,-0.008939396,0.00553876,-0.016331198,0.004042073,0.036409203,0.0018224964,0.0011157276,-0.0112540005,0.01434919,-0.0056948774,0.007303561,-0.0058102678,0.0019361903,0.009570652,0.017485106,-0.006105533,-0.005779723,-0.0207975,-0.020363089,-0.012584389,0.014186285,-0.0038486237,0.014634273,0.01419986,-0.014213436,0.020403815,-0.037169423,0.018448958,0.015082261,-0.018068848,-0.004079405,-0.0018666164,5.705907e-05,0.026621345,0.0054640956,0.009462048,0.02875268,0.0002018279,-0.010568443,0.040916234,-0.020878954,0.022263644,-0.006125896,-0.010432689,0.002346846,0.034019932,-0.012781232,-0.028263967,0.010167969,-0.040590424,0.020783925,-0.0022925443,0.017810915,0.012387546,0.0058204494,-0.018924097,0.008783279,-0.008111297,-0.020159459,-0.0024825998,-0.009815008,0.021191187,0.018666165,-0.011471206,-0.01437634,0.014661424,0.037087973,0.0037026883,-0.009170177,-0.014960082,0.004863384,0.011274364,0.0030493725,0.012835533,-0.010039003,-0.0024469644,-0.01900555,0.023227496,0.04705231,0.009557077,-0.015747456,0.01485148,-0.013147768,-0.016643431,-0.09546216,-0.004361095,0.017729463,0.021218339,0.007371438,0.043902818,-0.00018114665,0.01255045,-0.042300925,0.024245651,-0.04387567,-0.018869795,0.0058102678,-0.004096375,-0.003039191,-0.011606961,0.0008824005,-0.025331682,-0.022412973,0.028589776,0.0033650005,-0.04906147,0.00024053898,-0.010106879,0.018163875,-0.02436783,-0.021544147,0.010296935,-0.0010436083,-0.000839129,0.018055271,-0.038961377,-0.023607608,-0.016086841,-0.0101543935,0.024815818,-0.018530412,0.0040149223,0.004968594,0.0007674339,0.022738783,-0.0010419114,-0.02212789,0.0001298147,-0.020892529,-0.011905619,-0.023675485,0.028861284,0.0023638152,-0.011145397,-0.027598772,-0.0051586493,-0.009835372,-0.0045918766,-0.022412973,0.00810451,0.009407747,0.030001618,0.015598126,0.017254325,-0.013941928,0.011131822,0.005901902,0.023227496,0.02210074,-0.016629856,-0.02368906,-0.0005022895,0.007222109,-0.0058917203,-0.022521576,-0.0008900367,-0.00017478318,-0.0052129505,-0.033449765,-0.0071338685,-0.014131984,-0.0021075797,-0.003039191,-0.009896461,0.009170177,-0.008885094,0.006546733,-0.0044357595,-0.023621183,0.019439962,0.014634273,-0.017715888,-0.026757099,-0.017729463,-0.012387546,0.021869957,0.01349394,-0.025657492,0.0056609386,0.017715888,0.007941605,-0.0075411303,0.0046156333,0.0101543935,-0.008348866,-0.003509239,-0.04561332,0.021204762,-0.0012497846,-0.025195928,-0.00845747,-0.018788343,0.0117087765,-0.0108874645,-0.0016061387,0.010140819,-0.01228573,0.0046563596,-0.03135916,0.018489685,-0.01832678,0.0049821693,0.011634111,-0.02077035,0.0073850136,0.0012209369,-0.011471206,0.012455422,0.010819588,0.02315962,-0.010731348,-0.03947724,-0.028426873,0.009238055,-0.009305932,0.008695039,0.021557724,-0.008152023,0.004764963,0.028073912,-0.009835372,-0.00828099,0.013100253,0.0064517055,0.035133116,-0.024327103,-0.0059154774,-0.04012886,0.00034086962,-0.007846577,0.03211938,0.009238055,0.002497872,0.0005896811,-0.017281475,-0.0019837043,0.021937834,0.0071338685,-0.00384523,-0.03385703,-0.011742715,0.009081937,-0.011294726,-0.016575554,0.020010129,0.0019667349,0.029458601,-0.01631762,0.023349674,-0.0081316605,0.004686904,0.0008437955,0.0007233139,0.021109736,0.013785811,-0.025562463,-0.00055574265,-0.007907666,0.005973173,0.037087973,0.031467758,0.015720304,0.010493779,0.002484297,-0.007194958,0.041079137,0.031467758,-0.0051484676,-0.026648495,-0.01538092,0.011769866,0.0062073483,0.013595755,0.0109757045,0.0026234447,0.0047921133,-0.010181544,0.003621236,-0.0024690246,0.0055523356,0.00089682435,0.002633626,-0.0050432584,-0.010670259,-0.0062446804,0.015828907,-0.0053962185,0.0009986399,-0.014009805,-0.021231914,-0.039640147,-0.0030222218,-0.02734084,-0.025861124,-0.024381405,0.0052197385,-0.0018038303,-0.0026438078,-0.0016171687,0.021231914,-0.010385175,0.025671067,-0.015150138,-0.014688575,-0.03209223,0.018408231,0.009346657,-0.0038520177,0.009679255,-0.0011819076,0.03600194,0.004934655,0.01918203,-0.029295698,0.03355837,0.015611702,0.013086678,0.0015569278,-0.00017838913,-0.011009643,-0.009665679,-0.028942736,-0.0032241556,0.004829446,-0.0041472823,0.06765976,-0.0017461348,0.009808221,0.031060498,0.008566072,0.026091903,0.021761354,0.006034262,-0.0045239995,-0.017566558,0.013480364,-0.007371438,-0.02026806,-0.017023543,0.013208857,-0.001942978,0.0028949524,0.021530572,-0.016928514,-0.01867974,-0.0011674837,-0.005874751,0.0014602032,0.0117087765,0.010262997,0.0074664657,0.01417271,0.0066892747,-0.01166805,-0.039585844,0.0024741152,0.024028445,-0.041730758,-0.010765286,-0.0055964557,0.017525831,-0.0074325274,-0.010643108,0.020118732,-0.009577439,0.0004172312,0.0015671094,-0.016670583,-0.017987395,-0.018109573,0.0067639393,0.00021487301,-0.013731509,-0.025155202]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060848463,0.020726508,0.010920126,0.0026077912,-0.006342909,0.022329213,0.0022563506,-0.0024804578,-0.01595914,-0.040420767,0.005932046,0.024638195,-0.0039558294,-0.0024974355,-0.01064169,-0.0131340325,0.012889552,0.01541585,0.0059184637,0.009072941,-0.0023497287,0.01130722,-0.0032410636,-0.007402324,-0.014288523,0.039279856,0.0299896,-0.01572824,0.015144205,-0.022220556,0.015768988,-0.004203705,-0.016176455,-0.028658539,-0.014709573,0.028332567,0.010784304,0.0022835152,0.02750405,0.016774075,0.016882733,0.023103401,-0.004485537,0.0045975903,0.006183318,0.016217202,0.015347938,-0.014913307,-0.02334788,0.009697724,0.0012572067,0.019110221,-0.0216501,-0.004220683,-0.022274883,0.0016519408,-0.0024482,0.0046824794,0.010030489,-0.029745119,0.0181323,0.009758844,-0.018526183,0.018050805,-0.026634784,-0.0041867276,0.0065398514,0.000882846,-0.005972793,0.014682408,0.01408479,0.023062654,-0.0087062195,-0.01753468,0.017127212,-0.023510868,-0.008672264,-0.012128946,-0.00797957,0.017181542,0.009426079,-0.010553406,-0.011456625,0.016665416,-0.0041799364,-0.021853834,0.02079442,0.017819908,-0.03433592,0.009527946,0.009324213,0.026403887,0.017127212,-0.006118802,0.0062749977,-0.0058301794,-0.0060373084,0.0262409,0.015076294,0.007707925,-0.010994828,-0.0031969212,-0.059272923,-0.017982895,-0.011151024,-0.0075109825,0.011151024,0.0043123635,0.00037075288,0.00035929287,-0.032841872,0.033901285,-0.0068658255,-0.033874124,-0.0072597107,-0.0009957484,0.034933537,-0.0072461283,-0.0038200067,-0.012441338,0.021201886,0.008454949,0.031347826,-0.024991333,0.02461103,-0.007286875,-0.017371694,-0.006869221,-0.023429375,-0.022695934,0.024380133,0.024678942,0.01879783,0.018376779,-0.024026994,0.033412326,-0.013324183,0.017140795,-0.023171313,-0.008821669,0.0119387945,0.020726508,0.0044957236,0.023551615,0.005755477,0.01070281,0.013894638,0.02757196,0.0030543075,-0.010003325,-0.012821641,-0.014872559,0.029174665,0.029364817,0.0043021766,0.011836927,-0.0054736454,-0.00038093957,-0.012794476,-0.003198619,-0.009195181,0.024760434,-0.008536442,-0.009276674,0.022030404,0.013555082,0.024081323,-0.012162901,0.008502486,-0.0009982951,-0.024991333,0.038193278,-5.085384e-05,-0.008821669,0.000685479,0.0131000765,0.02340221,-0.028006593,-0.012006706,-0.007789418,-0.026879266,0.01457375,-0.009344585,0.0030661921,-0.030560054,0.022940414,0.004268221,-0.029745119,-0.0030390276,-0.005955815,0.016488846,0.016013468,-0.00043887633,-0.00076018134,-0.6680292,-0.021432785,0.0076264315,-0.019368283,0.0070356037,0.016122127,0.011232518,-0.006685861,-0.038057454,0.033222176,-0.021514278,-0.015225698,0.003561944,-0.029554969,-0.019232461,-0.039986134,0.00044778967,-0.025738357,-0.01227156,0.005466854,-0.011592448,0.0014448115,-0.0064379848,0.016760493,-0.001022064,0.007877703,0.0042716167,-0.024964169,0.021853834,-0.0074091153,0.0059388373,0.023945501,0.005381965,0.0001690353,0.023035489,-0.037649985,-0.02618657,0.010166312,-0.0025670445,0.032543063,-0.025344472,-0.017371694,0.008808087,0.004709644,-0.021772342,0.017344529,0.019449776,0.006251229,0.010410792,0.010302134,-0.0057656635,0.014858978,0.013032165,-0.01112386,0.01807797,-0.023864007,0.024285056,-0.013385304,0.011721479,-0.019259626,0.016353024,-0.0027758717,-0.029419146,-0.00196603,0.002859063,0.013962549,-0.006988066,0.0017045719,-0.0053276364,-0.03126633,0.022505783,0.01130722,-0.00022856375,0.012971045,0.016516011,-0.0011587354,0.041941978,0.01166715,0.018757083,0.003942247,0.01681482,-0.015307192,-0.019096639,0.0021290171,0.0083530815,0.0011952377,0.008780923,-0.0020509192,0.00393206,-0.0060542864,0.027368227,0.02816958,-0.0027385205,-0.025140738,-0.011932003,0.017371694,0.0029728143,0.009561902,-0.0007258013,-0.043843493,-0.004376879,0.0047911373,0.031347826,0.022274883,0.020943824,0.008027107,-0.017575426,-0.0030424232,0.016475264,-0.024991333,0.016977808,-0.0054091294,-0.015388685,0.0036468331,-0.0053174496,-0.02431222,0.014533004,0.017860655,0.008719802,-0.004308968,0.009018611,-0.0053649875,0.01620362,-0.008991447,0.00081153924,0.006638323,-0.015402268,-0.013568664,0.008529651,-0.010546614,0.0150083825,0.0064210068,0.019802915,-0.025548205,-0.0030610987,0.012482084,0.008278379,-0.007884494,0.01699139,-0.0015330961,-0.02732748,0.015510925,-0.001045833,-0.020427698,-0.021609355,-0.013201944,0.0063055577,0.009860711,0.00834629,-0.006791123,0.005955815,-0.023198476,-0.0057249167,-0.0002928672,0.0011994821,0.003307277,-0.004933751,-0.033711135,0.0067809364,-0.026879266,0.012027079,0.038627908,-0.023551615,-0.019843662,-0.0041765408,-0.024597447,-0.007911659,0.0345804,-0.0041493764,-0.013310601,0.007850538,-0.016040633,0.007381951,0.016040633,-0.01795573,0.0032563435,-0.009833546,-0.018947234,-0.0055245785,-0.021296963,0.012373426,0.00308317,-0.015021965,0.0013658646,0.03536817,0.020984571,0.0062953713,0.021147558,-0.028549882,0.027191658,0.00016372974,0.0136841135,-0.02140562,0.034498908,0.020291876,0.013853892,0.031347826,-0.0132630635,-0.020767255,0.017466769,0.027626289,0.025847014,0.022478618,-0.010967664,-0.0022869108,-0.011015202,0.004040718,-0.010105192,0.02140562,0.020414116,0.027245985,0.011300429,-0.0067877276,-0.01233268,0.005972793,0.0024753644,-0.008427784,0.022234138,-0.0111306505,0.0030407254,0.009188389,-0.004702853,0.044848576,0.009718098,-0.012081408,-0.012074617,-0.001820021,0.024461625,0.034417413,-0.002300493,-0.0035958998,-0.01704572,-0.0128759695,0.01233268,0.0041323984,0.0075924755,0.00033276505,-0.0018098344,0.033874124,-0.0082240505,0.01541585,0.013609411,-0.00037818067,-0.014845395,0.022139061,-0.00039218736,0.029554969,0.026390305,0.0042071007,0.002889623,0.0075245644,-0.008631517,-0.0030152588,0.021052482,-0.002154484,-0.02067218,0.01161282,0.009487199,0.0054464806,0.030206917,0.029582134,0.0145873325,0.017100047,-0.0054940186,0.03265172,-0.022845339,-0.028821526,-0.00070924795,-0.01572824,0.0018658611,0.0031137299,0.008685847,0.0073547866,-0.023116983,0.011538118,0.002859063,0.034661893,0.014166283,0.009500781,0.026403887,-0.025616117,-0.017792743,0.009752053,0.008658682,-0.01257716,-0.0050458047,-0.011429461,0.010560197,-0.020101724,0.032081265,0.0037351178,0.03379263,-0.017303782,-0.0080610635,-0.009616231,0.0010288552,0.025751939,-0.017914983,-0.00991504,-0.00858398,-0.0038335891,0.006590785,-0.012923507,-0.013636575,0.008230842,-0.001820021,-0.0065466426,-0.036835052,0.018974397,-0.01028176,0.026172988,-0.023375046,0.0017962521,0.0007334413,0.025195068,0.022071151,-0.005303867,0.020223964,0.028305402,0.020414116,0.00040534517,-0.027408972,-0.008964282,0.00393206,0.0073615774,0.027015088,0.0049812887,0.022125479,-0.010539823,-0.008156139,0.0002780116,0.0021816483,0.01947694,0.009670559,-0.014695991,-0.00949399,-0.0069609014,0.003468566,0.015945558,-0.01003728,0.0056026764,-0.038519252,-0.034363084,-0.019490523,-0.021853834,0.013833518,0.017154377,0.024352968,0.001455847,-0.0020356392,0.0021850439,0.00943287,0.0035517574,0.0030577031,0.01892007,0.020468445,0.022342796,0.005490623,-0.014003296,0.019259626,0.014533004,-0.003959225,-0.014478675,0.023850424,-0.0038675447,-0.013235899,-0.0017673898,0.013079703,-0.011735061,-0.019694258,0.002957534,-0.0113887135,0.04340886,0.005762268,0.009453244,0.00096349063,-0.030587219,-0.025167903,-0.0049812887,-0.013025374,-0.013249481,-0.042485267,-0.015551672,-0.020156054,-0.021731595,-0.018431108,0.038926717,-0.0064311936,0.01772483,0.0032682281,-0.024624612,0.0052087917,-0.032678884,-0.012869178,-0.0036841843,-0.027694201,-0.046152472,-0.016841985,0.005089947,0.026770608,-0.0035110107,-0.013140824,-0.0042240787,0.015687495,-0.0073479954,-0.0074770264,0.015999887,-0.011083113,-0.007103515,0.019313954,-0.032434404,0.00074872136,0.006621345,0.02697434,0.0056264456,0.026743444,-5.2498566e-05,-0.041670334,0.007178217,-0.0072597107,-0.022682352,0.020848747,-0.0044583725,-0.019612763,0.0073208306,-0.010926917,-0.0029524409,-0.011904839,-0.021378456,0.0007771592,0.007585685,-0.015714658,0.012285142,-0.017521098,-0.004886213,-0.011436252,0.012353053,-0.011239309,0.021622937,0.00695411,0.018240957,0.011443042,-0.001068753,0.0004617964,0.0028183162,-0.016040633,0.005103529,0.010173103,-0.022560112,0.005089947,-0.0022359772,0.0016926875,-0.014614497,0.015049129,-0.024624612,0.032543063,-0.0145873325,0.00012606023,-0.020943824,-0.005168045,-0.018512601,0.008882789,-0.002865854,-0.0064379848,0.0010059351,0.010376836,-0.018594095,0.0070831417,-0.0078369565,-0.015184951,-0.017507516,0.023361463,0.0091612255,0.028196743,-0.00931063,0.012244395,-0.024176398,-0.03180962,-0.022804592,-0.017208707,-0.005229165,-0.0031137299,0.0019507501,0.02653971,0.03566698,-7.938452e-06,0.034200095,0.009928622,-0.015999887,0.017453186,-0.00792524,0.0081900945,-0.022397125,0.0015500739,0.013976132,0.0025500667,0.02811525,0.0046145683,0.015578836,-1.6115653e-05,-0.01820021,-0.010050862,-0.014818231,-0.012774102,-0.0019320745,0.008692638,-0.009656977,-0.00257044,-0.022179808,0.0055890945,-0.018417526,-0.017140795,0.0008518615,0.0043734834,0.0063530956,-0.003561944,0.00840741,0.0029388585,0.003857358,-0.02522223,-0.036427584,-0.017344529,-0.010213849,0.004753786,-0.0076264315,0.024040576,0.016407354,0.012617907,-0.014845395,-0.013989714,-0.027802859,-0.013976132,0.004831884,-0.010845424,-0.0024380132,-0.021419203,-0.026865683,0.015388685,0.012611115,0.004706248,-0.025493877,0.0041799364,0.0039863894,-0.01771125,0.0095143635,0.014342852,0.024149233,0.012387008,0.02219339,0.019232461,-0.025425965,-0.000685479,0.0022444662,0.0073208306,0.0005314054,0.028196743,0.022084733,-0.010437956,-0.02079442,-0.0040780697,0.014858978,-0.016733328,-0.017914983,0.031347826,0.01185051,-0.021310546,-0.0065364563,-0.015252863,-0.026716279,-0.008638308,-0.00768076,0.0011875976,-0.005035618,-0.0033089747,0.0049065864,-0.008590771,0.00858398,0.021595772,-0.0024057555,-0.01185051,0.012563578,0.009833546,-0.0068828035,-0.0021018526,0.012149319,0.022858921,0.024448043,0.020509193,-0.017914983,-0.005303867,-0.0027368227,0.0076468047,-0.0147503195,0.02835973,-0.019205296,0.012909926,0.0059048817,-0.0035483618,0.0051171114,-0.009881084,-0.015456596,-0.008203677,-0.0077554625,0.0064210068,0.03175529,-0.0022054173,-0.023089819,-0.0068182875,-0.0076943426,-0.024271475,-0.014804648,0.005477041,-0.0115177445,-0.03789447,-0.002850574,-0.016475264,0.011049157,0.023538033,0.0043700878,-0.010655273,0.013575455,0.015144205,-0.027354645,0.0019558435,0.00791845,-0.0007992303,-0.014505839,-0.0005738499,-0.024746852,-0.022451453,0.011619612,0.014111954,-0.05419316,-0.0131000765,0.025819851,0.0041595628,0.016298696,0.00040789184,-0.008380246,0.013921803,-0.00043166077,0.009453244,0.00985392,0.030071093,0.0064787315,0.008305544,0.0007041546,0.0035891086,0.0057249167,-0.030668713,-0.0015008382,0.0042478475,-0.030315574,0.004933751,-0.007062768,0.008183303,-0.01191163,-0.0011858998,-0.028468389,-0.012767311,-0.01209499,0.048705935,0.022899667,-0.029364817,-0.029093172,-0.0039558294,0.0039694114,0.015796153,-0.020482028,-0.034471743,0.0017996477,-0.017426021,-0.029391982,-0.022818174,-0.013935384,0.03732401,0.010553406,-0.007606058,-0.025914926,-0.014831813,-0.013847101,-0.013622993,-0.023565197,-0.004675688,-0.009718098,0.023388628,-0.0158369,0.018947234,-0.008088227,0.0016459984,-0.021813087,0.0087062195,-0.0057656635,-0.019137384,0.025588952,0.0016171362,-0.033819795,0.018580513,0.006203691,-0.016583923,-0.002859063,0.0006396389,-0.020631433,-0.015280027,-0.0044210213,-0.00025233268,0.013256272,0.029500639,-0.0025687423,-0.013901429,-0.012190066,0.025167903,-0.01572824,0.0055313697,-0.021731595,0.013337766,-0.007701134,-0.015483761,-0.026376722,-0.012169693,-0.003999972,0.0010110284,0.003439704,0.012305515,-0.007701134,0.011884466,0.016149292,0.0036264597,0.004319154,0.025072826,0.0036536243,0.016217202,0.015307192,-0.01862126,-0.019993067,0.0019150968,-0.022967579,0.00077121693,-0.010566988,-0.0075041912,-0.011232518,-0.021106811,0.008529651,0.019246044,-0.030179752,0.023470122,0.004037323,-0.00072240573,0.01844469,0.012122155,0.003949038,-0.011252891,-0.017928565,-0.010145938,-0.018770665,-0.016448101,0.04479425,0.014967635,0.017982895,0.0066315318,0.22046702,-0.017330946,0.0024753644,0.028251072,0.021324126,0.0011842021,0.012971045,0.027762111,-0.027123746,0.0035211975,0.00991504,-0.01305933,-0.004994871,0.0037181398,0.0070831417,0.0032970903,-0.03707953,-0.03743267,-0.030587219,0.028577046,0.012251186,0.013962549,0.013527918,-0.026526127,0.051503878,-0.0013701092,-0.011762225,-0.0005522032,-0.0015381894,-0.004366692,-0.006543247,-0.009140852,0.02969079,0.037052367,-0.010573779,-0.01771125,-0.0034566817,0.0049065864,0.018906487,-0.0032784147,-0.01251604,0.006994857,0.004679084,-0.011476998,-0.00979959,-0.0036468331,-0.009724888,-0.010145938,0.0030644943,0.02231563,-0.016312279,-0.010241014,0.017928565,0.027001506,-0.0014439626,-0.015864063,0.005514392,-0.014655244,0.004811511,-0.0068284743,-0.024339385,0.0073208306,-0.023796096,0.021419203,-0.032678884,0.002154484,-0.032896202,-0.005025431,0.02363311,-0.019721422,-0.0320541,0.0033157659,-0.014261358,-0.0031663612,-0.027544796,-0.009758844,0.027979428,0.020970989,0.01578257,0.013439633,-0.0069812746,-0.005477041,-0.01167394,-0.018702753,-0.0025008311,-0.0075041912,0.015157787,-0.019639928,0.012692609,0.0026672136,0.0041561676,0.0017385276,-0.01801006,0.013330975,-0.010716393,0.018675588,0.0013870869,0.022261301,0.013507544,-0.02550746,-0.009337794,0.061174437,0.007701134,0.006926946,0.0020084747,0.005877717,-0.0071442616,0.016067797,0.020088142,-0.019096639,0.015320774,-0.050227147,0.002687587,-0.0031850368,-0.012468502,-0.006108615,-0.0050967378,-0.0019354702,0.0022648396,-0.019395448,-0.0027588939,-0.02436655,-0.02128338,0.0052936804,0.01136834,-0.019721422,-0.025521042,-0.001976217,-0.008617936,-0.0027928494,-0.0142341945,-0.011198563,0.0022359772,0.016678998,-0.012434547,-0.025425965,-0.0115517005,-0.02629523,0.006261416,0.009466826,0.0024753644,-0.010370045,0.0039015003,-0.022831757,0.0073547866,-0.013833518,0.016054215,0.011979542,0.004733413,-0.01602705,0.0012928601,0.0034430996,-0.013622993,-0.016013468,0.021935329,-0.006020331,-0.02370102,-0.015252863,-0.0055653253,-0.0033734904,-0.017100047,-0.027123746,0.02007456,0.013106868,-0.02835973,-0.008495695,-0.17515665,0.011538118,0.019694258,-0.051775523,0.014111954,0.027476884,0.018838575,0.015809735,-0.034634728,-0.011490581,0.021514278,-0.022302048,-0.0019728213,-0.011959168,-0.0048522577,-0.011286846,-0.0138131445,0.0142341945,0.013976132,0.0145873325,0.05261762,-0.015334357,-0.007870912,0.011972751,-0.01760259,-0.018757083,0.011538118,0.03324934,0.013378513,-0.051585373,-0.017208707,-0.012889552,0.031891115,0.020251129,-0.011823346,0.0045806128,0.018729918,-0.026757026,-0.02254653,0.007775836,-0.0052155824,0.013962549,-0.0089507,0.0055789077,-0.01625795,0.004040718,0.036427584,0.0018862345,0.0010602641,-0.0112596825,0.014370017,-0.0057045436,0.0072461283,-0.0058607394,0.0019422612,0.009561902,0.017480351,-0.00613578,-0.0057928283,-0.020835167,-0.02031904,-0.01257716,0.014152701,-0.0038777313,0.01462808,0.01420703,-0.014247776,0.020386951,-0.03724252,0.018471856,0.015049129,-0.018105134,-0.0040610917,-0.0019150968,0.0001051563,0.026526127,0.005449876,0.009480408,0.028740034,0.00022007484,-0.010600943,0.04088256,-0.020848747,0.022234138,-0.0060814507,-0.01039721,0.002392173,0.034064274,-0.01275373,-0.028278237,0.010111982,-0.040556587,0.020767255,-0.0022716308,0.017792743,0.0123598445,0.005786037,-0.01886574,0.0087401755,-0.008128975,-0.020210383,-0.0024736666,-0.009819964,0.021174721,0.018580513,-0.011531327,-0.014424345,0.014655244,0.037106697,0.0037622822,-0.009154434,-0.014981218,0.004784346,0.011300429,0.0030237476,0.01275373,-0.010057653,-0.002439711,-0.018960817,0.023293553,0.047021735,0.009595857,-0.015768988,0.014831813,-0.0131340325,-0.016638251,-0.09534737,-0.0043497146,0.01771125,0.021201886,0.0074159065,0.043870658,-0.000183042,0.012536413,-0.042322278,0.02424431,-0.04395215,-0.018892905,0.0058132014,-0.004081465,-0.0030848677,-0.011558492,0.0009176505,-0.025358055,-0.022410706,0.028631376,0.0034023528,-0.04903191,0.00029541386,-0.010030489,0.018159464,-0.024407297,-0.021514278,0.010315716,-0.0010466819,-0.00079753256,0.018050805,-0.03895388,-0.023605945,-0.016067797,-0.010125564,0.024801182,-0.018458273,0.004040718,0.004964311,0.00076485024,0.022695934,-0.0009957484,-0.022111896,0.00013974858,-0.02091666,-0.011904839,-0.023673855,0.02884869,0.0023123776,-0.011157815,-0.027599124,-0.005229165,-0.009908249,-0.0046043815,-0.0224922,0.008013525,0.009392124,0.03004393,0.015565255,0.017276617,-0.013948967,0.011096695,0.0058912993,0.023293553,0.021989657,-0.016665416,-0.023714602,-0.00050381647,0.007218964,-0.0059184637,-0.022465035,-0.00084761705,-0.00021837706,-0.0051952093,-0.03343949,-0.0071646348,-0.014098371,-0.0020560126,-0.0029863964,-0.009921831,0.0091612255,-0.008903163,0.0065738074,-0.0044651637,-0.023605945,0.01940903,0.014641661,-0.017751995,-0.026797771,-0.01765692,-0.012373426,0.021853834,0.0135211265,-0.025684027,0.005612863,0.01771125,0.00792524,-0.0075313556,0.004577217,0.010186685,-0.0083191255,-0.0035211975,-0.04563635,0.021147558,-0.0012241,-0.025140738,-0.008427784,-0.018784247,0.011721479,-0.010906544,-0.0016358118,0.010152729,-0.012251186,0.004631546,-0.031402152,0.01844469,-0.018281704,0.0049677067,0.011707896,-0.020821584,0.007334413,0.0013081401,-0.011497372,0.0124549195,0.010831841,0.023198476,-0.010757139,-0.039442845,-0.028441224,0.009168017,-0.009331003,0.008699429,0.021568608,-0.008149348,0.004746995,0.02811525,-0.00979959,-0.008380246,0.0131340325,0.0064617535,0.035150852,-0.024325803,-0.0059048817,-0.04020345,0.00030581278,-0.007877703,0.03210843,0.00924951,0.0024889465,0.0005827633,-0.017276617,-0.0020016835,0.021881,0.007110306,-0.0038675447,-0.033874124,-0.011694314,0.009147643,-0.011273265,-0.016556758,0.019979484,0.0019643323,0.029473474,-0.01632586,0.023388628,-0.0081900945,0.0047435993,0.00083361036,0.00074872136,0.021066064,0.013765607,-0.025602534,-0.0005229165,-0.007952405,0.0059761885,0.03713386,0.03151081,0.01578257,0.010512658,0.0024889465,-0.0072053815,0.041099876,0.03151081,-0.00514088,-0.026607621,-0.015456596,0.011687523,0.006220669,0.013643366,0.010994828,0.0025823247,0.004842071,-0.010152729,0.0036128776,-0.0024431066,0.0055653253,0.00094396615,0.0026858891,-0.0050593866,-0.010668854,-0.006244438,0.015864063,-0.005449876,0.0010161218,-0.014016878,-0.021337708,-0.03960583,-0.00308317,-0.027341062,-0.025860596,-0.02443446,0.0051544625,-0.00187435,-0.002696076,-0.0015746917,0.021256216,-0.010356463,0.025738357,-0.015157787,-0.014723155,-0.032081265,0.018417526,0.009283465,-0.0038777313,0.009670559,-0.0010823352,0.03591146,0.004872631,0.019205296,-0.02925616,0.03354815,0.015646748,0.01305933,0.0015551673,-0.00018919645,-0.011049157,-0.0096773505,-0.029011678,-0.0032733213,0.004842071,-0.004166354,0.06774825,-0.001741923,0.009806382,0.03102185,0.008550024,0.026091496,0.021731595,0.006095033,-0.004516097,-0.017589008,0.013514335,-0.0074362797,-0.020251129,-0.016964225,0.013174779,-0.0018913278,0.0029796052,0.021514278,-0.016950643,-0.018716335,-0.0011188375,-0.005884508,0.0015424339,0.01172827,0.010268178,0.0074159065,0.014152701,0.006668883,-0.011680732,-0.039497174,0.0024889465,0.024026994,-0.041751824,-0.010729975,-0.0056298412,0.017480351,-0.007402324,-0.010648482,0.020101724,-0.009656977,0.00036990398,0.0016052517,-0.016611088,-0.017928565,-0.018173046,0.006794519,0.00021052481,-0.013704486,-0.02520865]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006048875,0.020692447,0.010875754,0.0026069225,-0.0063374015,0.022362506,0.0022895432,-0.0024762368,-0.015940245,-0.040380143,0.0059368587,0.024629986,-0.0039307503,-0.002469448,-0.010638145,-0.013116078,0.012878468,0.015397135,0.0059606195,0.0090834955,-0.0023710094,0.011296663,-0.0032043422,-0.007338758,-0.014324496,0.03932108,0.029925298,-0.01566869,0.015152737,-0.022199573,0.015763734,-0.0042090933,-0.016130332,-0.028648991,-0.014691094,0.02835028,0.010753555,0.0022674794,0.027440574,0.016782064,0.016863529,0.023109281,-0.0045078034,0.0046130307,0.0061676796,0.016198222,0.015383558,-0.014949071,-0.023380836,0.009680915,0.0012788516,0.019117432,-0.021656465,-0.0042328546,-0.022294618,0.0016276293,-0.0025050896,0.004701286,0.010013569,-0.029680898,0.01811268,0.00971486,-0.01852001,0.018017637,-0.026598755,-0.00419891,0.0064901505,0.00093431695,-0.0059741973,0.014731827,0.014066519,0.02305497,-0.008689742,-0.017542416,0.017094351,-0.02353019,-0.008676164,-0.012063805,-0.007970123,0.017175818,0.009450094,-0.010515945,-0.01143923,0.016632708,-0.004219277,-0.02186013,0.020814646,0.017827548,-0.034351636,0.009565504,0.009348261,0.026435822,0.017135084,-0.0060896077,0.0063238237,-0.005790898,-0.0060556633,0.026232157,0.01507127,0.0076849903,-0.010957221,-0.003194159,-0.059198864,-0.017990481,-0.01114052,-0.0074881134,0.011167675,0.0043007433,0.000375721,0.00039884556,-0.032803774,0.033917148,-0.006894088,-0.03388999,-0.0071894033,-0.001009843,0.03497621,-0.007257292,-0.003801762,-0.012423615,0.021235555,0.00845892,0.03128307,-0.024996584,0.02460283,-0.007291236,-0.017365905,-0.006826199,-0.023489457,-0.022701949,0.024385586,0.024684297,0.018805144,0.018397812,-0.024005411,0.033455506,-0.013292589,0.017121507,-0.023122858,-0.008805152,0.011934817,0.020706024,0.0044942256,0.023489457,0.005753559,0.010760344,0.013930742,0.027603507,0.0030719594,-0.009959258,-0.012803791,-0.014786138,0.0291921,0.029355032,0.0043177153,0.011799039,-0.005485399,-0.00038038834,-0.012790213,-0.0031585174,-0.00917854,0.024752185,-0.008553964,-0.009260005,0.022009486,0.013543776,0.024114033,-0.012124904,0.008533598,-0.0010327554,-0.025010163,0.038153395,1.360424e-05,-0.008811941,0.00071283034,0.013143234,0.02340799,-0.028051572,-0.012009494,-0.007834345,-0.026911043,0.014568894,-0.009348261,0.0030736567,-0.030549873,0.022946348,0.004202305,-0.029653743,-0.0030448039,-0.005926675,0.01649693,0.015980978,-0.0004149691,-0.00076120096,-0.66824114,-0.021412065,0.0075695794,-0.019375408,0.007012893,0.016116755,0.011181253,-0.006683633,-0.038099084,0.033238262,-0.021534266,-0.015274936,0.0035675461,-0.029517965,-0.019212475,-0.040054277,0.0004480648,-0.025716204,-0.012247104,0.005444666,-0.0115682185,0.0013976566,-0.0063951067,0.016768485,-0.0010412415,0.007881867,0.004266799,-0.024915118,0.021792242,-0.007393069,0.005940253,0.02389679,0.005417511,0.00015062785,0.023068547,-0.0376646,-0.026177846,0.010149347,-0.0026052252,0.032559376,-0.02532245,-0.01739306,0.0088323075,0.004725047,-0.021737931,0.017311594,0.019470451,0.0062152017,0.010407323,0.010298701,-0.005804476,0.014867605,0.013000668,-0.011167675,0.018058369,-0.023788167,0.024276964,-0.013319745,0.011683629,-0.019280365,0.016361155,-0.0027647635,-0.029355032,-0.0019501002,0.0028428354,0.013985053,-0.0069925264,0.001736251,-0.005356411,-0.031310223,0.022525439,0.011276297,-0.00017799543,0.012966723,0.01644262,-0.0012211463,0.041955158,0.0116768405,0.018696522,0.0039341445,0.016754908,-0.015329247,-0.019131009,0.0021215188,0.00840461,0.0012236921,0.008744053,-0.0020468414,0.003944328,-0.005980986,0.027359108,0.02818735,-0.0027206358,-0.025118783,-0.011968761,0.01739306,0.0029616405,0.009538349,-0.0007438045,-0.043828882,-0.004402576,0.00479633,0.03133738,0.022253884,0.020923268,0.007976911,-0.017569572,-0.003060079,0.016483353,-0.024955852,0.01697215,-0.0054650325,-0.015356403,0.003676168,-0.0052681556,-0.02430412,0.014473851,0.017909015,0.008689742,-0.0042464323,0.008988451,-0.005373383,0.016225377,-0.008988451,0.0007930238,0.006629322,-0.015437868,-0.013550566,0.008594697,-0.010577044,0.014949071,0.0064018955,0.019796317,-0.025512537,-0.0030634734,0.012471137,0.008268832,-0.007868289,0.016958574,-0.0015478602,-0.027331952,0.015519335,-0.0010395442,-0.020448048,-0.021602154,-0.013190756,0.006279696,0.0098574255,0.008336721,-0.006771888,0.006004747,-0.02323148,-0.005729798,-0.00031249973,0.0011575007,0.003273928,-0.004921924,-0.033672746,0.0068024383,-0.026843155,0.012016283,0.038615037,-0.023570923,-0.019877784,-0.004219277,-0.024629986,-0.007827556,0.034568876,-0.004134416,-0.013319745,0.007861501,-0.015994556,0.00738628,0.016048865,-0.01793617,0.003226406,-0.0098574255,-0.01894092,-0.0055159493,-0.02126271,0.0123964595,0.003038015,-0.014962649,0.0013662581,0.035356384,0.020991156,0.006259329,0.021086201,-0.02854037,0.027277641,0.00013110987,0.013686343,-0.021412065,0.034514565,0.020285115,0.013849275,0.031364534,-0.013251856,-0.020746758,0.01746095,0.027630663,0.025811248,0.022443973,-0.010977588,-0.0022691765,-0.011045476,0.004039372,-0.010149347,0.021398488,0.020393737,0.027182598,0.011337397,-0.006771888,-0.012294627,0.0059606195,0.0024830257,-0.008424976,0.022213152,-0.011133731,0.0029531543,0.009226061,-0.004728441,0.044833634,0.00971486,-0.01209775,-0.012111327,-0.0018262034,0.024453476,0.03437879,-0.0023150013,-0.0036014905,-0.017094351,-0.012898835,0.012321781,0.004144599,0.007637468,0.00033435138,-0.0018245062,0.03388999,-0.008180577,0.015410713,0.013550566,-0.00036320405,-0.014854027,0.022145264,-0.00036702276,0.029517965,0.026462978,0.0041717547,0.0029005406,0.007549213,-0.008601487,-0.0029803098,0.021004735,-0.0021724354,-0.020665292,0.011602162,0.009484038,0.0054480606,0.030169696,0.029599432,0.014568894,0.017135084,-0.005498977,0.032668,-0.022851305,-0.028784769,-0.0007208921,-0.015723001,0.0018890004,0.0031211786,0.008710108,0.00738628,-0.023082126,0.011534274,0.002807194,0.034650344,0.014134408,0.009477249,0.026354356,-0.025594003,-0.017773237,0.009735226,0.008628641,-0.012539025,-0.005081462,-0.011425653,0.010549889,-0.020122182,0.032070577,0.0037033234,0.03383568,-0.017325172,-0.008092321,-0.009599449,0.0009911737,0.025756937,-0.01786828,-0.009898159,-0.008594697,-0.0038730449,0.0066123502,-0.0129192015,-0.013611665,0.008241677,-0.0018262034,-0.006561434,-0.03682278,0.01894092,-0.010312279,0.026232157,-0.023326525,0.0017803786,0.0007264081,0.025159517,0.022090953,-0.0052817333,0.020298693,0.028323125,0.020448048,0.00037805468,-0.027386263,-0.008927352,0.003913778,0.0074202246,0.027019665,0.0049524736,0.022131685,-0.010495578,-0.008173788,0.00026264405,0.0021860132,0.019511186,0.009626605,-0.014650361,-0.009511193,-0.0070264707,0.0034657132,0.015980978,-0.010061091,0.0055804434,-0.038533572,-0.034405947,-0.019524762,-0.021941597,0.0138560645,0.017148662,0.024317699,0.0014256607,-0.0020315666,0.00214528,0.00941615,0.0035573628,0.0031177842,0.018913765,0.020475203,0.022362506,0.0054786103,-0.014039364,0.019280365,0.014514584,-0.003957906,-0.014514584,0.023761012,-0.0038424951,-0.013183967,-0.001799048,0.013075345,-0.011710784,-0.019714851,0.0029531543,-0.011432441,0.04336724,0.0057875034,0.009402572,0.0009784445,-0.030577028,-0.025173094,-0.004962657,-0.013034612,-0.013204333,-0.04252542,-0.015600801,-0.020108605,-0.021751508,-0.018452123,0.03891375,-0.006469784,0.017746082,0.0032671392,-0.02460283,0.005159534,-0.032668,-0.012844524,-0.00372369,-0.027712129,-0.046082783,-0.016877107,0.005101829,0.026775265,-0.003581124,-0.013136445,-0.004164966,0.015695846,-0.007291236,-0.007460958,0.015980978,-0.011045476,-0.0071215145,0.01936183,-0.032396443,0.00074507744,0.006595378,0.027019665,0.005624571,0.026775265,-7.945088e-05,-0.04171076,0.0071690367,-0.0073048137,-0.022661217,0.020868957,-0.004463676,-0.019565497,0.00727087,-0.010896121,-0.002910724,-0.011934817,-0.02143922,0.00074719894,0.0075695794,-0.015709423,0.012314993,-0.01751526,-0.004938896,-0.011432441,0.012342148,-0.011181253,0.021642888,0.0069314265,0.018234879,0.011486752,-0.0010751858,0.00046673417,0.0028037995,-0.01602171,0.005057701,0.010203658,-0.022552595,0.005040729,-0.0022301406,0.0016997609,-0.014568894,0.015016959,-0.024562098,0.032505065,-0.014555317,0.00019199746,-0.020950424,-0.005159534,-0.018560745,0.008893407,-0.0028309547,-0.0064324453,0.0009818389,0.01036659,-0.018601477,0.007101148,-0.0078411335,-0.015152737,-0.01751526,0.023326525,0.009205695,0.028160194,-0.009321106,0.012267471,-0.024236232,-0.03177187,-0.02275626,-0.017230129,-0.005247789,-0.0031211786,0.0019602834,0.026530867,0.03568225,-5.60081e-05,0.034188703,0.00994568,-0.015994556,0.017501682,-0.007902234,0.00816021,-0.022389662,0.0015419199,0.01399863,0.0026306834,0.028105883,0.0045960587,0.015627956,-4.924576e-05,-0.018180568,-0.0100203585,-0.014799716,-0.012797002,-0.0019144587,0.008649008,-0.009646971,-0.0025763726,-0.022185996,0.0055974154,-0.018438546,-0.017175818,0.0008053286,0.0043109264,0.006262724,-0.0035064465,0.008418187,0.0029820069,0.0038934115,-0.02520025,-0.036442604,-0.017325172,-0.010210446,0.004718258,-0.007603524,0.023991833,0.016374731,0.012620492,-0.014840449,-0.014025786,-0.027834328,-0.013957897,0.004847246,-0.010780711,-0.002452476,-0.02138491,-0.026816,0.015397135,0.012593336,0.004731836,-0.025512537,0.004141205,0.0039782724,-0.017759658,0.009463672,0.014324496,0.024100455,0.012382882,0.022185996,0.019226054,-0.025431072,-0.00070052553,0.0022488101,0.0073183915,0.0005647483,0.02818735,0.022063797,-0.0103801675,-0.020760335,-0.004073316,0.014813294,-0.016782064,-0.017895436,0.031364534,0.01185335,-0.021317022,-0.006524095,-0.015261359,-0.026720954,-0.008649008,-0.0076849903,0.0012211463,-0.005057701,-0.003314661,0.004837063,-0.008676164,0.008628641,0.02162931,-0.0023998623,-0.011846562,0.012579759,0.009809904,-0.0069178487,-0.0021232162,0.012118116,0.022851305,0.024467053,0.020543091,-0.01793617,-0.005247789,-0.002752883,0.0076782014,-0.01477256,0.02835028,-0.019144587,0.012912413,0.0058893366,-0.0035675461,0.005098434,-0.009891369,-0.015478602,-0.008234887,-0.007732512,0.0064494177,0.03177187,-0.0022114713,-0.02305497,-0.006809227,-0.0077053565,-0.024263388,-0.014826871,0.005437877,-0.011527485,-0.037854686,-0.0028462298,-0.016483353,0.01107942,0.023516612,0.0043686316,-0.0106042,0.0135845095,0.015152737,-0.027304797,0.0019704667,0.007963333,-0.0008235736,-0.014528162,-0.00057365873,-0.024765763,-0.022457551,0.011602162,0.0141072525,-0.054229416,-0.01304819,0.025838403,0.004188727,0.016334,0.00045527794,-0.008370665,0.013944319,-0.00043915442,0.009490827,0.0098574255,0.03008823,0.0064663896,0.008323143,0.0006860992,0.003574335,0.005753559,-0.030658495,-0.0015300395,0.0042430377,-0.030305473,0.0048845853,-0.007012893,0.008194155,-0.011887295,-0.0011770187,-0.028458903,-0.012769846,-0.0120909605,0.04871686,0.022892037,-0.029355032,-0.029056324,-0.0039409334,0.0039816666,0.015750157,-0.020475203,-0.03448741,0.0018194147,-0.01739306,-0.029382188,-0.02282415,-0.013957897,0.037311576,0.010515945,-0.007637468,-0.02590629,-0.014840449,-0.013815331,-0.013652398,-0.023584502,-0.004718258,-0.00971486,0.023394413,-0.015818045,0.018913765,-0.00803801,0.0016649681,-0.021832975,0.008682952,-0.005767137,-0.019158164,0.025553271,0.0016055655,-0.03383568,0.01853359,0.006194835,-0.016591975,-0.0028615047,0.0005948739,-0.020638136,-0.015302092,-0.0044161538,-0.00029043594,0.0132247005,0.02949081,-0.0025950419,-0.013957897,-0.01221316,0.025145939,-0.015723001,0.0055261324,-0.021710776,0.013340111,-0.007725723,-0.015465024,-0.026354356,-0.012179215,-0.003937539,0.0010166318,0.0034368606,0.012301415,-0.0077053565,0.011812617,0.016157487,0.0035947016,0.0043041375,0.02507805,0.0036388293,0.016211798,0.015261359,-0.018615056,-0.019986406,0.0018974865,-0.022973504,0.00077053567,-0.010570256,-0.0074338024,-0.011221986,-0.021072622,0.00852002,0.01925321,-0.030169696,0.023448724,0.004025794,-0.00068991794,0.018397812,0.0120909605,0.003913778,-0.011221986,-0.017909015,-0.01012898,-0.018805144,-0.016510509,0.04480648,0.014989804,0.017963326,0.0066225333,0.22039357,-0.017298017,0.002445687,0.02818735,0.0213306,0.0011668354,0.012993879,0.027684974,-0.027128287,0.0034928687,0.00995247,-0.013088923,-0.0050373343,0.0037202956,0.007094359,0.003280717,-0.037094332,-0.037393045,-0.030604184,0.028513214,0.012247104,0.013971475,0.013543776,-0.026462978,0.05148672,-0.0013297681,-0.011751518,-0.0005409873,-0.0014977923,-0.004395787,-0.006564828,-0.009131017,0.029708054,0.03698571,-0.010590622,-0.017732503,-0.003470805,0.0048777964,0.018873032,-0.0032909,-0.012539025,0.0069925264,0.0046843137,-0.011479964,-0.009816692,-0.0036659846,-0.009674126,-0.010135769,0.0030634734,0.022308195,-0.016293265,-0.010217235,0.017976902,0.027033243,-0.0014307523,-0.015872356,0.0054718214,-0.014623205,0.0048268796,-0.006826199,-0.024331275,0.0073183915,-0.023842478,0.021398488,-0.032613687,0.0021435828,-0.03293955,-0.0049999957,0.023570923,-0.019687695,-0.032070577,0.003375761,-0.014283762,-0.003163609,-0.027535617,-0.009735226,0.027970105,0.020950424,0.015750157,0.013414788,-0.0069076656,-0.0054242997,-0.011690418,-0.018682944,-0.0024796312,-0.0075220573,0.015125581,-0.019633384,0.012722325,0.0026849944,0.004144599,0.0017218248,-0.017990481,0.013306167,-0.010672089,0.018628633,0.0013637124,0.02228104,0.013536988,-0.025526116,-0.009287161,0.061208367,0.007657835,0.0069925264,0.001972164,0.0058825477,-0.007101148,0.01607602,0.020067872,-0.01906312,0.015288514,-0.050183255,0.002666325,-0.0031822782,-0.0124507705,-0.0061031855,-0.0051154066,-0.0019229448,0.0022572961,-0.019348253,-0.0027630662,-0.02435843,-0.021249134,0.005278339,0.011364552,-0.019714851,-0.02549896,-0.0019501002,-0.008649008,-0.0027240303,-0.01429734,-0.01120162,0.0022589932,0.016673442,-0.012403248,-0.025471805,-0.011575007,-0.026259312,0.006232174,0.009490827,0.002540731,-0.010468422,0.0038764393,-0.022796994,0.007338758,-0.0138560645,0.016062444,0.01191445,0.0047216523,-0.0160896,0.0012296324,0.0034504384,-0.013665976,-0.01602171,0.021914441,-0.006004747,-0.0237067,-0.015302092,-0.005617782,-0.0033621832,-0.017094351,-0.027209753,0.020040717,0.013156812,-0.028377436,-0.008479287,-0.17509829,0.011541063,0.01966054,-0.051785428,0.0141072525,0.027508462,0.018805144,0.015804468,-0.034650344,-0.011534274,0.021493532,-0.02233535,-0.0019517974,-0.011948395,-0.0048302743,-0.011289875,-0.013774598,0.014229451,0.013957897,0.014582472,0.05260009,-0.015274936,-0.007902234,0.012043439,-0.017542416,-0.018750833,0.01150033,0.033265416,0.013353689,-0.05159534,-0.017189395,-0.012885258,0.03188049,0.020244382,-0.011846562,0.0045417477,0.018737255,-0.02674811,-0.022511862,0.0077460897,-0.0052036615,0.013957897,-0.008947718,0.0055668657,-0.01626611,0.004069922,0.036442604,0.0018601478,0.0010879149,-0.011235564,0.014351651,-0.00574677,0.00720977,-0.0058655753,0.0019925307,0.009538349,0.017488105,-0.0061337356,-0.005814659,-0.020746758,-0.020325849,-0.012620492,0.01417514,-0.003859467,0.014650361,0.014243029,-0.014215874,0.02043447,-0.037202954,0.018438546,0.015044115,-0.018099101,-0.0040665274,-0.0018601478,8.342873e-05,0.026530867,0.0054616383,0.009524771,0.028703302,0.00020005924,-0.010577044,0.04086894,-0.020814646,0.022267463,-0.0061507076,-0.010386957,0.0023862845,0.034052923,-0.012729113,-0.028295971,0.010122191,-0.040543076,0.020773914,-0.0022674794,0.017827548,0.012382882,0.005797687,-0.018886609,0.008703319,-0.008133055,-0.020217227,-0.002460962,-0.009796326,0.021181244,0.0185879,-0.011527485,-0.014365229,0.014677516,0.037148643,0.0037508453,-0.009185328,-0.015016959,0.004779358,0.011276297,0.0030515927,0.01274948,-0.009999991,-0.002423623,-0.018995231,0.023272214,0.047087535,0.0095519265,-0.015736578,0.014854027,-0.013122868,-0.016605552,-0.09542422,-0.004283771,0.017746082,0.0212084,0.0074134357,0.043828882,-0.0001825567,0.012491504,-0.042281024,0.024222653,-0.043883193,-0.018845877,0.0057807146,-0.004103866,-0.0030736567,-0.0115682185,0.0008834005,-0.025349606,-0.022416817,0.02859468,0.003385944,-0.048988417,0.00030146784,-0.010040725,0.018126257,-0.02435843,-0.021520687,0.010319068,-0.0010819746,-0.0008396972,0.018058369,-0.03896806,-0.023598079,-0.016048865,-0.010095036,0.024820074,-0.0184657,0.0040563443,0.0049830237,0.00075653364,0.022701949,-0.001037847,-0.022063797,0.00012400278,-0.020923268,-0.01186014,-0.023638813,0.028784769,0.002311607,-0.011147309,-0.027562773,-0.0051866896,-0.009918525,-0.004585875,-0.022443973,0.00803801,0.009422938,0.030006763,0.015573646,0.017284438,-0.013957897,0.011072632,0.00593007,0.023245059,0.022023063,-0.016632708,-0.023693124,-0.00053080404,0.007216559,-0.0059063085,-0.022416817,-0.0008842491,-0.00021013645,-0.005183295,-0.033455506,-0.007162248,-0.014080097,-0.002073997,-0.003005768,-0.009918525,0.009151384,-0.008873041,0.0065919836,-0.0044331257,-0.023638813,0.01941614,0.014677516,-0.017746082,-0.026775265,-0.01763746,-0.012382882,0.021887286,0.01352341,-0.025634738,0.005610993,0.017718926,0.0079226,-0.007528846,0.0045689032,0.010203658,-0.008350299,-0.003533602,-0.04559399,0.021140512,-0.0012279352,-0.025159517,-0.008465709,-0.018791566,0.011697207,-0.010916487,-0.0016267807,0.010162924,-0.012281049,0.004694497,-0.031364534,0.0184657,-0.018316345,0.0050203623,0.011703995,-0.020814646,0.007284447,0.0012686683,-0.011473174,0.012471137,0.010828232,0.023163592,-0.01071961,-0.039456856,-0.028431747,0.009239639,-0.009334683,0.008710108,0.02162931,-0.008133055,0.004701286,0.028078727,-0.00983027,-0.008295988,0.013095712,0.0064494177,0.035111986,-0.024317699,-0.005926675,-0.0401629,0.00032968403,-0.007854711,0.0321792,0.009246428,0.0025016952,0.0006827048,-0.017298017,-0.0020112,0.02186013,0.0071215145,-0.0038764393,-0.033808526,-0.011717573,0.009124229,-0.011283087,-0.016551241,0.020054294,0.0019229448,0.029436499,-0.016347576,0.023394413,-0.008187366,0.004714864,0.00085454783,0.0007645954,0.021072622,0.013828909,-0.025553271,-0.0005647483,-0.007956545,0.0059674084,0.037094332,0.031527467,0.015709423,0.010482,0.0025152727,-0.0071826144,0.04103187,0.031500313,-0.0051493505,-0.02662591,-0.015465024,0.011731151,0.006201624,0.013618454,0.011018321,0.0026221974,0.0048133023,-0.010149347,0.0036082794,-0.002423623,0.005536316,0.0009054643,0.002690086,-0.0049966015,-0.010658511,-0.006266118,0.015831623,-0.0054718214,0.0010191777,-0.014052941,-0.021289866,-0.03961979,-0.0030838398,-0.027318375,-0.025865559,-0.024399165,0.0051629283,-0.0018584506,-0.0026629306,-0.0015928363,0.021221979,-0.0103801675,0.025648315,-0.015179892,-0.014663938,-0.03204342,0.018424967,0.009314316,-0.0038119452,0.009680915,-0.001120162,0.03592665,0.0048744017,0.019171743,-0.0291921,0.033564128,0.015641535,0.01304819,0.001569924,-0.00019698303,-0.011059054,-0.009633393,-0.029002013,-0.003234892,0.0048099076,-0.0041751494,0.06767136,-0.0017320081,0.009789537,0.03103867,0.008553964,0.026069224,0.021710776,0.0060862135,-0.004521381,-0.017569572,0.013530199,-0.0074202246,-0.020244382,-0.016958574,0.013136445,-0.0018974865,0.0029243017,0.021493532,-0.01691784,-0.0187101,-0.0011558034,-0.0058893366,0.001530888,0.011724362,0.010264757,0.007406647,0.0141072525,0.0066700554,-0.011649685,-0.039511167,0.002484723,0.024005411,-0.04176507,-0.010712822,-0.0056381486,0.017501682,-0.0074338024,-0.010631355,0.020108605,-0.0096062375,0.00037381164,0.0016149002,-0.016632708,-0.017922591,-0.01811268,0.006785466,0.00019178531,-0.013713499,-0.025173094]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006071497,0.02065939,0.010913754,0.002612985,-0.006363527,0.022302905,0.0022004084,-0.002460179,-0.015878256,-0.040449485,0.0058915257,0.024652725,-0.003945794,-0.002502625,-0.010642099,-0.01305304,0.012883255,0.015348528,0.005932274,0.009039332,-0.0024109413,0.011273698,-0.0032649578,-0.0073890258,-0.014261907,0.039335698,0.02988209,-0.015715262,0.015144787,-0.02227574,0.015756011,-0.0041733054,-0.016109163,-0.028632475,-0.014682973,0.02836082,0.01074397,0.0021987106,0.027464356,0.016788302,0.01688338,0.023117872,-0.004512875,0.004601163,0.0061631813,0.016204242,0.015402859,-0.015008959,-0.023389528,0.009698097,0.0013226221,0.019124538,-0.02163735,-0.004200471,-0.022343654,0.0016146515,-0.0024873444,0.004675868,0.009996917,-0.0296376,0.018132996,0.009745636,-0.018635558,0.017942837,-0.026649391,-0.004186888,0.0065333117,0.00090580084,-0.0059254826,0.014696555,0.014071748,0.023036376,-0.008720137,-0.017548937,0.017114287,-0.023538938,-0.008645432,-0.012095455,-0.007979876,0.017195784,0.009446816,-0.010574185,-0.011423108,0.016679639,-0.0042038667,-0.021922588,0.02075447,0.017752677,-0.034310073,0.009555478,0.009304197,0.026377736,0.017114287,-0.0061428067,0.0062446776,-0.0058066333,-0.006098663,0.026241908,0.015104038,0.0077082207,-0.010913754,-0.00323949,-0.0592752,-0.01801075,-0.011158244,-0.0075180624,0.011205784,0.0042717806,0.0004340119,0.0003525153,-0.03284313,0.03390259,-0.006835528,-0.03390259,-0.0072599896,-0.0010271969,0.034962047,-0.0072192415,-0.0037828009,-0.012523312,0.02129778,0.008428108,0.03126753,-0.02497871,0.024625558,-0.0072803637,-0.01737236,-0.006825341,-0.02340311,-0.02266964,0.024462566,0.024666307,0.018825717,0.018404651,-0.024068665,0.03341361,-0.013290739,0.017127872,-0.023199368,-0.008842382,0.011891713,0.020700138,0.0044891047,0.023498189,0.0057726763,0.01074397,0.013908755,0.027600184,0.0030357486,-0.009990126,-0.0128221335,-0.014805217,0.029175786,0.029420275,0.0042717806,0.011803426,-0.005490834,-0.0003686448,-0.012781384,-0.0031376195,-0.00917516,0.024734221,-0.008516395,-0.0092634475,0.022099163,0.013575977,0.024095831,-0.012122621,0.008455273,-0.0010178587,-0.024992293,0.038140412,9.145819e-06,-0.008774469,0.0007130953,0.013127745,0.023416692,-0.028034834,-0.011980002,-0.0078100916,-0.026961794,0.014628641,-0.0093381535,0.0031291302,-0.030534063,0.022954877,0.0042412193,-0.029746262,-0.0030391444,-0.005996792,0.016503064,0.016041249,-0.0004045118,-0.0007653041,-0.6680549,-0.021433609,0.0075791846,-0.019355444,0.007076622,0.016163494,0.011246532,-0.0066861175,-0.038086083,0.03316912,-0.021528687,-0.015199117,0.003626599,-0.02958327,-0.019219616,-0.03998767,0.00040557297,-0.02571218,-0.012190535,0.0053855674,-0.011606475,0.0014143058,-0.006431441,0.016747553,-0.0010526645,0.007830466,0.0043125288,-0.024937963,0.021759594,-0.007443357,0.0059118997,0.023932837,0.005338028,0.00016893567,0.023036376,-0.0375971,-0.02614683,0.010227825,-0.002568841,0.03254431,-0.02531828,-0.01746744,0.008828799,0.0047030332,-0.021746011,0.017331611,0.019559186,0.006207325,0.010411192,0.01030253,-0.0057964465,0.014791635,0.013046249,-0.011178618,0.018065082,-0.023824176,0.02434032,-0.013358653,0.011721929,-0.019233199,0.016380819,-0.0027199492,-0.029365946,-0.0020017605,0.0028082372,0.013922337,-0.0069747516,0.0017173713,-0.0053617978,-0.031240366,0.022465898,0.011273698,-0.00020193757,0.01295796,0.016516646,-0.0011494417,0.041943587,0.011701555,0.018730637,0.0039084414,0.016761135,-0.015307779,-0.019151703,0.0020883507,0.008394151,0.0012402765,0.008713346,-0.0020798615,0.0039627724,-0.006010375,0.027287781,0.028197827,-0.002757302,-0.02516887,-0.011946045,0.017440274,0.0029814176,0.009541894,-0.000749599,-0.04381801,-0.004431378,0.00473699,0.03134903,0.022248574,0.020917462,0.007979876,-0.01756252,-0.0030612163,0.016516646,-0.024965128,0.017005626,-0.0054331073,-0.015375693,0.003616412,-0.0052972795,-0.024299571,0.014560727,0.017888505,0.00873372,-0.0043295072,0.009012166,-0.005402546,0.016177077,-0.008991793,0.00082303083,0.006658952,-0.015389277,-0.013596351,0.008523187,-0.0105809765,0.015022541,0.006397484,0.019830842,-0.025535604,-0.0030306552,0.012435025,0.008265114,-0.00789838,0.01697846,-0.0015323061,-0.027342113,0.0154164415,-0.0009788083,-0.0204149,-0.021596601,-0.013188868,0.0062378864,0.009840716,0.008326237,-0.006825341,0.006057915,-0.023185786,-0.0057149497,-0.00026104384,0.0012428233,0.003341361,-0.004974689,-0.0336581,0.006791384,-0.026853133,0.012027541,0.038602225,-0.023579685,-0.01981726,-0.0042072623,-0.024584811,-0.007864423,0.034663223,-0.00414614,-0.013345069,0.00785084,-0.015973335,0.0073822346,0.0160005,-0.017956419,0.0032547708,-0.009874673,-0.018961545,-0.005524791,-0.021338528,0.012373902,0.0031121518,-0.015022541,0.001389687,0.035315197,0.021012543,0.006275239,0.021161953,-0.028578144,0.027124789,0.00015174499,0.013718596,-0.021420026,0.034473065,0.020292656,0.0138883805,0.031376194,-0.013243199,-0.02075447,0.017426692,0.02762735,0.025793677,0.022452315,-0.010947711,-0.002361704,-0.011015625,0.0040204995,-0.010125954,0.021433609,0.020442067,0.027219867,0.011280489,-0.00677101,-0.012339945,0.006044332,0.0025229992,-0.008387359,0.022234991,-0.011165035,0.0029304822,0.009202326,-0.004764156,0.04490463,0.009732054,-0.01211583,-0.012061498,-0.0018200909,0.024448983,0.034391567,-0.0023107685,-0.0036028293,-0.01697846,-0.012849298,0.012346736,0.004125766,0.0077082207,0.00030900797,-0.0018116017,0.033875424,-0.008224366,0.015443607,0.013596351,-0.00036249013,-0.0148188,0.022112746,-0.0003898679,0.029528938,0.02635057,0.004281968,0.002923691,0.0074569397,-0.0086250575,-0.003056123,0.021066874,-0.0022326675,-0.020700138,0.011654015,0.009440023,0.005402546,0.030208075,0.0296376,0.014628641,0.017087122,-0.0054636686,0.03262581,-0.022859799,-0.028768303,-0.0006757427,-0.015715262,0.0018438607,0.0030781948,0.008652223,0.007355069,-0.023145037,0.011497813,0.0027963524,0.034636058,0.014098913,0.009467189,0.026377736,-0.02565785,-0.017807009,0.009738845,0.0086250575,-0.012577644,-0.005008646,-0.011443482,0.010485897,-0.020102497,0.032082498,0.0037114914,0.03384826,-0.017290864,-0.008108913,-0.009596226,0.0009822039,0.02580726,-0.017915672,-0.009929003,-0.008570727,-0.0038235493,0.006601225,-0.012971544,-0.013718596,0.008251532,-0.0018200909,-0.006594434,-0.036890797,0.01894796,-0.0102957385,0.026228325,-0.023375943,0.0017963211,0.000738563,0.025196034,0.022071999,-0.0052667186,0.020251907,0.02836082,0.020374153,0.0004064219,-0.027410027,-0.008978209,0.0038778803,0.0074094,0.02693463,0.004954315,0.02218066,-0.010485897,-0.008129287,0.0002671985,0.0021341925,0.019423358,0.009711679,-0.014737303,-0.009575851,-0.006934003,0.0034568144,0.015959753,-0.009996917,0.005633453,-0.03857506,-0.034418736,-0.019545604,-0.021854674,0.013868006,0.017182201,0.02434032,0.0014210971,-0.001972897,0.0021528688,0.009412859,0.0035586853,0.0030204682,0.018866464,0.020482814,0.022357237,0.0054942295,-0.013949503,0.019314697,0.014533562,-0.0039525856,-0.014519979,0.023769844,-0.0038303407,-0.013161702,-0.0017963211,0.013086997,-0.011721929,-0.019695014,0.002944065,-0.011395942,0.04338336,0.0057523022,0.009446816,0.0009847507,-0.03056123,-0.025196034,-0.00498148,-0.013025874,-0.013216034,-0.0424869,-0.015565853,-0.020143244,-0.021759594,-0.018458981,0.03890105,-0.006397484,0.017725512,0.0031732742,-0.024584811,0.0051784306,-0.03268014,-0.012910421,-0.0036571603,-0.027708847,-0.046235744,-0.016788302,0.005073164,0.02678522,-0.0035518939,-0.013120954,-0.004159723,0.015715262,-0.007355069,-0.0075452277,0.015973335,-0.011076747,-0.007076622,0.019369027,-0.03238132,0.0007304982,0.00655029,0.027029708,0.0056402446,0.02674447,-8.876551e-05,-0.041726265,0.007178493,-0.007287155,-0.022628892,0.020849548,-0.0044619394,-0.019640682,0.0073346947,-0.0109001715,-0.0029372736,-0.012007167,-0.021352112,0.00083831145,0.0076267244,-0.01570168,0.012333154,-0.01746744,-0.004886401,-0.011423108,0.012482564,-0.011226158,0.021623766,0.006947586,0.018228075,0.011491022,-0.0011010531,0.00037798297,0.0028184245,-0.0160005,0.0051071206,0.010200659,-0.022601726,0.005090142,-0.002249646,0.0016783208,-0.014519979,0.01506329,-0.024598394,0.03254431,-0.014587893,0.00014070899,-0.020944629,-0.0051444736,-0.018513313,0.008903504,-0.002877849,-0.0064450237,0.0009915421,0.010377235,-0.018594809,0.0070426655,-0.007891588,-0.015199117,-0.017494606,0.023375943,0.009161578,0.028224992,-0.009365318,0.012238074,-0.024245242,-0.031729348,-0.022778302,-0.017155036,-0.0051444736,-0.0031053603,0.0020238324,0.026554313,0.03572268,5.093538e-05,0.034228574,0.009922212,-0.015986918,0.017481023,-0.007986668,0.008170035,-0.022438733,0.0015306082,0.013922337,0.0025603517,0.028089164,0.004546832,0.015579435,6.7980163e-06,-0.018187327,-0.010105579,-0.014791635,-0.012761011,-0.0018998897,0.00868618,-0.009650556,-0.0025943087,-0.022153495,0.0055519566,-0.018431816,-0.017182201,0.00083449134,0.0043159244,0.0063601313,-0.0034585122,0.008394151,0.0029491584,0.0038710888,-0.025209617,-0.036483314,-0.017318029,-0.010227825,0.0047471775,-0.0075927675,0.024014333,0.016367236,0.012577644,-0.014886714,-0.014003834,-0.027803926,-0.013949503,0.004838861,-0.010832258,-0.0024279198,-0.02139286,-0.026866715,0.015348528,0.012625183,0.004730199,-0.02550844,0.004132557,0.004044269,-0.017820591,0.009528312,0.014356986,0.024163745,0.012421441,0.02227574,0.019219616,-0.025413359,-0.0007364407,0.0022615308,0.007287155,0.0005925483,0.028170662,0.022085581,-0.010384026,-0.020835966,-0.004071435,0.014764469,-0.01673397,-0.017970001,0.03134903,0.01187134,-0.021311363,-0.0065536858,-0.015267031,-0.026690139,-0.008692971,-0.007674264,0.0012428233,-0.0050527896,-0.00328703,0.0048558395,-0.008604684,0.008557144,0.021650933,-0.0023430276,-0.011884922,0.012598017,0.009786384,-0.006889859,-0.0021664514,0.012136203,0.022873381,0.0244354,0.020482814,-0.017956419,-0.0053210496,-0.002745417,0.007674264,-0.014750886,0.028333655,-0.019233199,0.012910421,0.0058847344,-0.0035417068,0.0051410776,-0.009847507,-0.015511521,-0.008183618,-0.0077829263,0.0064552105,0.031756513,-0.0022326675,-0.023036376,-0.0068423194,-0.0077285953,-0.024299571,-0.0148188,0.0054398985,-0.011586101,-0.03792309,-0.0029016188,-0.016448732,0.011110704,0.023471024,0.0044279825,-0.010669264,0.013562394,0.015117621,-0.02738286,0.0020017605,0.007945919,-0.0007928941,-0.014533562,-0.0005501021,-0.0248293,-0.022438733,0.011633641,0.014085331,-0.054195244,-0.013127745,0.025820842,0.0041529313,0.016258573,0.00041660896,-0.008366985,0.013963086,-0.00039793266,0.009446816,0.009874673,0.03012658,0.006472189,0.008333028,0.0007113975,0.0035586853,0.005721741,-0.03066989,-0.0014703346,0.0042378237,-0.030343903,0.0049373363,-0.0070290826,0.008149661,-0.011850965,-0.0011358589,-0.028469482,-0.012733845,-0.012068289,0.04868064,0.022900548,-0.029338779,-0.029067123,-0.0038948588,0.003959377,0.015810343,-0.020469232,-0.034554563,0.0018132995,-0.017399525,-0.029338779,-0.02281905,-0.013976668,0.037352614,0.010506271,-0.0076403073,-0.02591592,-0.014832383,-0.013861215,-0.013671056,-0.023511771,-0.004716616,-0.009623392,0.02330803,-0.01579676,0.018975127,-0.0080545815,0.0017326518,-0.021827508,0.008645432,-0.0057285326,-0.019165285,0.025603518,0.0015823925,-0.033875424,0.01854048,0.006227699,-0.016611725,-0.0028914318,0.000590426,-0.020632224,-0.015253449,-0.004421191,-0.00027781006,0.013236407,0.029501772,-0.0025620495,-0.013949503,-0.012183743,0.025114538,-0.015769593,0.005548561,-0.021732429,0.013331487,-0.0077421777,-0.015430025,-0.026377736,-0.012244865,-0.0039390028,0.0009805061,0.0033889005,0.01231957,-0.0076674726,0.011878131,0.016149912,0.0035960379,0.004322716,0.025060207,0.003616412,0.016204242,0.015348528,-0.018608393,-0.019980252,0.0018540479,-0.02291413,0.0007653041,-0.010533437,-0.0075588105,-0.011246532,-0.021121204,0.008570727,0.019246783,-0.030208075,0.023471024,0.004037478,-0.0007334695,0.018391067,0.012047916,0.0039627724,-0.01128728,-0.017929254,-0.010024083,-0.0188393,-0.016475897,0.044795968,0.01506329,0.017929254,0.0066215997,0.2204755,-0.017318029,0.0024856466,0.028224992,0.02139286,0.0010535135,0.012985126,0.027681682,-0.02708404,0.003538311,0.009969751,-0.013154911,-0.005096934,0.0037182828,0.0071173706,0.0032666558,-0.037108123,-0.03743411,-0.030642726,0.028496647,0.0122177,0.013976668,0.013501272,-0.026527146,0.051587354,-0.0012997012,-0.011735512,-0.0005955195,-0.0014728814,-0.004370256,-0.0065536858,-0.009161578,0.0296376,0.03694513,-0.010594559,-0.017725512,-0.0034415338,0.0048897965,0.018907214,-0.0032955192,-0.012489355,0.0069509814,0.0046385154,-0.011402734,-0.009806759,-0.0036537647,-0.009698097,-0.010214241,0.0030391444,0.02233007,-0.01634007,-0.0102617815,0.017956419,0.02704329,-0.0014856153,-0.01594617,0.0054874383,-0.014615059,0.0048796097,-0.006791384,-0.024367485,0.0073618605,-0.023864923,0.021379277,-0.032598644,0.0021919191,-0.03292463,-0.004974689,0.023579685,-0.019722179,-0.032109663,0.003363433,-0.014289072,-0.0031596913,-0.027545854,-0.009698097,0.027980503,0.020971794,0.015756011,0.013467315,-0.006981543,-0.005501021,-0.011694764,-0.018676307,-0.0024245242,-0.0074501485,0.015171952,-0.019599935,0.012747427,0.002669014,0.004180097,0.0017156734,-0.017983586,0.013324696,-0.010662473,0.018662723,0.0013803488,0.022248574,0.013528437,-0.025535604,-0.009290613,0.06128545,0.0077014295,0.0069306074,0.0018693284,0.00588813,-0.007090205,0.016027667,0.02007533,-0.019070206,0.015280615,-0.050174747,0.002636755,-0.0032021375,-0.012468982,-0.006105454,-0.0050324155,-0.0019576165,0.0023056748,-0.019409776,-0.0028065394,-0.024408234,-0.0212027,0.0053753806,0.011314446,-0.019654265,-0.025494855,-0.001918566,-0.0086590145,-0.0028421942,-0.01418041,-0.011158244,0.0022411568,0.016666057,-0.01241465,-0.025426943,-0.011552144,-0.02625549,0.0062175123,0.009453607,0.0025229992,-0.0104383575,0.0038507148,-0.022791885,0.0073958174,-0.013827258,0.016041249,0.0119664185,0.004764156,-0.016054831,0.0012818738,0.00337362,-0.013623516,-0.0160005,0.021922588,-0.0060409363,-0.02370193,-0.015267031,-0.005633453,-0.0033498502,-0.017046373,-0.027138371,0.020088915,0.01314812,-0.028387986,-0.008523187,-0.17516337,0.011497813,0.019667849,-0.051804677,0.014085331,0.027545854,0.018866464,0.015864674,-0.03458173,-0.011511396,0.021460773,-0.022343654,-0.0020357175,-0.011946045,-0.004852444,-0.01128728,-0.013827258,0.014234741,0.014017417,0.014587893,0.052673973,-0.015321363,-0.007939128,0.011959627,-0.017508188,-0.018703472,0.011545353,0.03322345,0.013358653,-0.051587354,-0.01722295,-0.012876464,0.03189234,0.02026549,-0.011830591,0.0045909756,0.018703472,-0.026798801,-0.02257456,0.0077489694,-0.0051682433,0.013990251,-0.0089442525,0.005480647,-0.016272156,0.0040850174,0.03645615,0.0018625371,0.0011485928,-0.011280489,0.014356986,-0.0057353238,0.0072803637,-0.005854173,0.0020153432,0.009541894,0.01746744,-0.0061156414,-0.005755698,-0.020822383,-0.020346986,-0.012577644,0.014098913,-0.003881276,0.014587893,0.014221158,-0.014289072,0.02045565,-0.037162453,0.018486148,0.015049707,-0.018119413,-0.0040171035,-0.0019304509,3.5469064e-05,0.026554313,0.0054500857,0.009528312,0.028795468,0.00020894117,-0.010635307,0.040965628,-0.020835966,0.022248574,-0.006064706,-0.010390817,0.0024109413,0.034011252,-0.01265914,-0.028279323,0.010159911,-0.040585313,0.020727305,-0.0023362362,0.017820591,0.012373902,0.005799842,-0.018866464,0.008754094,-0.008129287,-0.02026549,-0.0024958337,-0.009772802,0.021257032,0.018594809,-0.01153177,-0.014343403,0.014655807,0.037135288,0.0038099664,-0.009147994,-0.0149546275,0.0048218826,0.011212575,0.003110454,0.012747427,-0.010098788,-0.002470366,-0.01898871,0.023294447,0.047023542,0.009603017,-0.015742429,0.014845965,-0.013114163,-0.016570978,-0.0952967,-0.004309133,0.017779844,0.021216284,0.007355069,0.04392667,-0.00018994653,0.012550478,-0.04229674,0.024245242,-0.043899506,-0.018825717,0.005789655,-0.0040510604,-0.0030815906,-0.011565727,0.0008973116,-0.025386194,-0.022438733,0.028578144,0.0034279511,-0.049006626,0.0002973353,-0.010071622,0.018173743,-0.024394652,-0.021569436,0.010377235,-0.0010051249,-0.0008340668,0.018037915,-0.03895538,-0.023634017,-0.016122745,-0.010112371,0.024815718,-0.01849973,0.004044269,0.0049984585,0.00077421777,0.022723971,-0.0010238012,-0.022058416,0.0001207593,-0.020971794,-0.011891713,-0.023661181,0.0288498,0.0023294447,-0.011124287,-0.027681682,-0.0051784306,-0.009895046,-0.0045502274,-0.022411568,0.008088538,0.009460398,0.030017918,0.015511521,0.017236533,-0.01393592,0.01113787,0.005901713,0.023321614,0.02203125,-0.016611725,-0.023647599,-0.0004787926,0.0072396155,-0.005874547,-0.02242515,-0.00084764964,-0.00020236202,-0.00522597,-0.033440776,-0.0071377447,-0.014098913,-0.00207307,-0.0029831154,-0.009895046,0.009181951,-0.008855965,0.0065876427,-0.004485709,-0.023620434,0.01947769,0.014615059,-0.017766261,-0.026771637,-0.017739095,-0.012346736,0.02188184,0.013501272,-0.025630683,0.0055825175,0.017725512,0.007945919,-0.0075248536,0.004628328,0.010146328,-0.008319446,-0.0035111455,-0.045665268,0.021134786,-0.0012716866,-0.025155287,-0.00844169,-0.018771386,0.011728721,-0.010879797,-0.0016019177,0.010187076,-0.012272031,0.0046249325,-0.031240366,0.018431816,-0.018282406,0.0049645016,0.011660807,-0.020849548,0.0073822346,0.0012394276,-0.011457065,0.012489355,0.010839049,0.023185786,-0.010710013,-0.039471526,-0.028442316,0.009161578,-0.009331361,0.008720137,0.021664515,-0.008156452,0.004716616,0.028089164,-0.009806759,-0.008373776,0.013086997,0.006465398,0.035152204,-0.024285989,-0.0059254826,-0.04017783,0.00027568775,-0.007837257,0.032082498,0.009297404,0.0025569561,0.00057896547,-0.017318029,-0.0020136454,0.021949753,0.0071173706,-0.003881276,-0.033793926,-0.011783051,0.009114037,-0.011253323,-0.016503064,0.020034583,0.0020051561,0.029556103,-0.01628574,0.023375943,-0.008156452,0.004655494,0.0007894984,0.0007653041,0.02109404,0.013759344,-0.025535604,-0.0005450086,-0.007918754,0.0059662308,0.037135288,0.031484857,0.015660932,0.01054702,0.002460179,-0.007253198,0.041101456,0.031512022,-0.005161452,-0.026581477,-0.0154164415,0.011694764,0.0061971378,0.013609934,0.0110427905,0.0025824239,0.004866027,-0.010112371,0.003616412,-0.0024432004,0.005504417,0.0009227793,0.0026724096,-0.005083351,-0.010642099,-0.0062990086,0.015878256,-0.0054093376,0.0010339882,-0.0140309995,-0.021324946,-0.03955302,-0.0030849862,-0.027342113,-0.025875174,-0.02438107,0.005161452,-0.0018387672,-0.0026741074,-0.0016469107,0.021229867,-0.010417983,0.025685014,-0.015144787,-0.014669389,-0.032001,0.018431816,0.009290613,-0.0038031752,0.009691305,-0.0011740605,0.035912838,0.004886401,0.019124538,-0.029230118,0.033549435,0.015620183,0.013025874,0.0015484355,-0.0002270869,-0.0110427905,-0.009684513,-0.029012792,-0.0032887277,0.0049169622,-0.0041529313,0.06764218,-0.0017148245,0.0097999675,0.031104539,0.008584309,0.026065333,0.021773178,0.0060273535,-0.0045298534,-0.017589685,0.013514854,-0.007443357,-0.020238325,-0.01697846,0.013175285,-0.001940638,0.0029287843,0.021528687,-0.016910546,-0.018703472,-0.0010653983,-0.005867756,0.0014924066,0.011728721,0.01025499,0.0074365656,0.014126079,0.006635182,-0.011735512,-0.039525855,0.0024347112,0.024055082,-0.04175343,-0.0107235955,-0.0056300573,0.017548937,-0.007443357,-0.01064889,0.020102497,-0.009616599,0.00035569875,0.0015857882,-0.016625308,-0.017942837,-0.01810583,0.006791384,0.00012065318,-0.013650682,-0.025223201]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7494828089957476
0.7323748012029329
0.7487152391021464
0.7220762480930598
0.7024446760077323
0.7337484147746429
0.7470200722247278
0.7220319192425751
0.7825334803526257
0.7762240176368941
0.6744282663787289
0.759230645288422
0.7166972395631734
0.8195887588831947
0.6814525376831977
0.7710777554247661
0.738168758089586
0.7382662538095591
0.7396656731852579
0.7709463328352839
0.775864803211803
0.7822301981352923
0.7423141182734833
0.7667363177450548
0.7672705469804153
0.7572022130350136
0.7562872002442672
0.7699054235193699
0.671407107236151
0.6800140723362351
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060405335,0.02069876,0.010906236,0.002636579,-0.006325753,0.022315001,0.002200262,-0.0024685035,-0.015890779,-0.040419623,0.005911506,0.024610335,-0.003908181,-0.0024752945,-0.010648181,-0.0130929155,0.012855234,0.015361087,0.0059420653,0.009052312,-0.0023921055,0.011259365,-0.003215506,-0.0073885336,-0.014301701,0.03936024,0.029907262,-0.01575496,0.015143777,-0.022287838,0.015768543,-0.004220564,-0.01610809,-0.02865773,-0.014681994,0.02835893,0.010729671,0.0022138436,0.027516855,0.0167736,0.016868673,0.023089167,-0.004475224,0.004600856,0.0061695613,0.016216744,0.015374669,-0.014967213,-0.023347223,0.009711032,0.0013208361,0.019136844,-0.02164949,-0.004176423,-0.022301419,0.0016501964,-0.0024939696,0.004685743,0.00998267,-0.02966279,0.018118205,0.00976536,-0.018579988,0.017995968,-0.026647616,-0.0041967956,0.006509108,0.00092526444,-0.005962438,0.014681994,0.014057228,0.023062004,-0.008726347,-0.017507022,0.017153893,-0.02355095,-0.0086720195,-0.0120946495,-0.007938599,0.017167475,0.009466559,-0.010525944,-0.011429138,0.016664946,-0.0042273547,-0.021921126,0.020753087,0.017724331,-0.034307785,0.009500513,0.00933074,0.026362397,0.01712673,-0.0061254203,0.0062442617,-0.005840201,-0.0060846745,0.026212996,0.015075868,0.007721289,-0.0109741455,-0.0031815514,-0.05927125,-0.01800955,-0.011171082,-0.00751077,0.011191455,0.0042613097,0.00038283877,0.00038368764,-0.032840945,0.033927493,-0.0068486542,-0.03390033,-0.007232342,-0.001022884,0.03498688,-0.007232342,-0.0037961307,-0.012508896,0.021282779,0.0083868,0.03126545,-0.024990628,0.0246375,-0.0072798785,-0.017371204,-0.0068860045,-0.023387969,-0.022668129,0.02442019,0.0246375,0.018797299,0.018417006,-0.024026316,0.033411384,-0.013317017,0.017085984,-0.023143495,-0.00888933,0.011897712,0.020725923,0.00444806,0.02353737,0.0057621053,0.010729671,0.013907827,0.027571183,0.003044035,-0.00998946,-0.012841651,-0.0148178125,0.029173842,0.02939115,0.004278287,0.011782266,-0.0054870727,-0.0003577972,-0.01268546,-0.00315269,-0.009154175,0.024759736,-0.008495456,-0.009296785,0.022097692,0.013575072,0.024053479,-0.0121557675,0.0084615005,-0.0009965692,-0.024990628,0.03813787,9.675751e-06,-0.008787465,0.0007206876,0.013113289,0.023428714,-0.028005801,-0.012026739,-0.0077824076,-0.026946416,0.014614085,-0.009364694,0.0030966648,-0.030504864,0.02296693,0.0042171683,-0.029717116,-0.0030423373,-0.0059318785,0.016529128,0.016080925,-0.0004138224,-0.0007665264,-0.66801035,-0.021486508,0.007633007,-0.019408483,0.007042196,0.016148835,0.011232201,-0.0067264177,-0.038083542,0.033221237,-0.021554416,-0.015184523,0.0036501256,-0.02952697,-0.019218337,-0.039985005,0.00045414353,-0.02573763,-0.012189722,0.0053784177,-0.011619284,0.0014396774,-0.0064615714,0.016719274,-0.0010602341,0.007897853,0.0043122414,-0.024963465,0.02179889,-0.00743607,0.0059284833,0.023931243,0.0053410674,0.00012998267,0.023034839,-0.03762176,-0.026145086,0.0101863975,-0.002617904,0.032569308,-0.025316592,-0.01742553,0.008875747,0.0047196974,-0.021758145,0.017344039,0.019476391,0.006196725,0.010376544,0.010267888,-0.005782478,0.014831395,0.013106498,-0.011130337,0.018050296,-0.023863334,0.024311535,-0.013323808,0.011687193,-0.019286245,0.016352562,-0.0027112793,-0.029336825,-0.00196937,0.0028657732,0.013934991,-0.0069810776,0.0017172568,-0.0053818133,-0.031292614,0.02251873,0.0113069005,-0.00019608815,0.012957097,0.01655629,-0.0011578539,0.041940793,0.011693984,0.018756554,0.003914972,0.0167736,-0.01530676,-0.019150427,0.002135748,0.008407174,0.0012639621,0.008739929,-0.0020814205,0.0039591133,-0.006030347,0.027313126,0.028223112,-0.0027299544,-0.025140028,-0.011992785,0.017439112,0.0030134758,0.0095820045,-0.0007291763,-0.043869417,-0.004403919,0.004784211,0.03134694,0.02223351,0.020902487,0.008013299,-0.017615676,-0.0030695011,0.016515546,-0.024949882,0.017004492,-0.0053648357,-0.015361087,0.0036569166,-0.0053037177,-0.024325117,0.014573339,0.017887313,0.008712765,-0.0043495917,0.009011566,-0.005402186,0.016162418,-0.008997984,0.0008000566,0.0066313446,-0.015388251,-0.013541117,0.008515828,-0.010587063,0.014980795,0.0063766846,0.019815939,-0.025547484,-0.003018569,0.012420613,0.008271354,-0.007891063,0.017004492,-0.0015016447,-0.027381036,0.015415414,-0.0010254306,-0.020495031,-0.021608744,-0.013140452,0.006261239,0.009833269,0.0083392635,-0.006801118,0.0060099745,-0.02326573,-0.0056840098,-0.00029625438,0.0012359496,0.0033088813,-0.004964171,-0.033655856,0.0067909313,-0.026864925,0.012026739,0.038599655,-0.023605278,-0.019802356,-0.0041662366,-0.024596754,-0.007863899,0.034633752,-0.004132282,-0.013350971,0.007857108,-0.016013017,0.007422488,0.015999435,-0.017955223,0.003261345,-0.009874014,-0.019001026,-0.0055006547,-0.021337107,0.012359495,0.003108549,-0.01502154,0.0013819545,0.035340007,0.02099756,0.0062374705,0.021174125,-0.02857624,0.027150145,0.00015778306,0.013683727,-0.021418598,0.03449793,0.020304885,0.013934991,0.03140127,-0.01326948,-0.020753087,0.017466275,0.027611928,0.025764795,0.022464402,-0.010946982,-0.002319103,-0.011001309,0.0040066494,-0.010145652,0.02136427,0.020454286,0.027218053,0.011293319,-0.0067909313,-0.012345914,0.006037138,0.002490574,-0.008393591,0.022247091,-0.011130337,0.002930287,0.009208503,-0.00474007,0.044847313,0.0097314045,-0.0120946495,-0.012060694,-0.0017962013,0.024447354,0.034389276,-0.0023106143,-0.003616171,-0.017018074,-0.012848442,0.01231875,0.004132282,0.007734871,0.00029413222,-0.0018590175,0.033873167,-0.008244191,0.015415414,0.013575072,-0.00032341812,-0.014804231,0.022056945,-0.00042634318,0.02952697,0.026362397,0.0042545185,0.002853889,0.0075311433,-0.008685602,-0.00302536,0.021079052,-0.002230821,-0.02069876,0.011673612,0.009452976,0.005442932,0.030178899,0.02966279,0.014600502,0.017113147,-0.005439536,0.032677963,-0.02289902,-0.02879355,-0.00067612214,-0.01567347,0.0018675062,0.003145899,0.008638065,0.0073342063,-0.023116332,0.0115174195,0.0028097478,0.034579422,0.014138719,0.009432604,0.026375977,-0.025669722,-0.01779224,0.009724614,0.008699183,-0.012563223,-0.005038871,-0.011429138,0.010525944,-0.020101158,0.032107525,0.003768967,0.033818837,-0.01727613,-0.00809479,-0.009588796,0.0009677077,0.02580554,-0.017873732,-0.0099147605,-0.008570155,-0.0038097126,0.0065770173,-0.012970679,-0.013683727,0.008244191,-0.0018318538,-0.0066143675,-0.03688834,0.01896028,-0.010301843,0.026226578,-0.02340155,0.001760549,0.0006803665,0.025194356,0.022070527,-0.0053037177,0.02026414,0.028331766,0.020372795,0.00042146217,-0.027394617,-0.008984403,0.0038674355,0.0074089062,0.026932834,0.0049573802,0.022152018,-0.010464826,-0.008149118,0.0002903123,0.0021510276,0.019422064,0.009677078,-0.014709158,-0.009541259,-0.006960705,0.0034616773,0.015945107,-0.010016624,0.0055957274,-0.038599655,-0.03441644,-0.019517137,-0.021893963,0.013839918,0.017181057,0.02435228,0.0014210024,-0.0019999293,0.002135748,0.009371486,0.0035822163,0.0030117782,0.018892372,0.020508613,0.02238291,0.005473491,-0.013975737,0.019299828,0.014532593,-0.0039591133,-0.0145461755,0.023781842,-0.003813108,-0.01319478,-0.001779224,0.013086125,-0.011721148,-0.0196937,0.0029455665,-0.011429138,0.04338047,0.005802851,0.009466559,0.00095497473,-0.030559191,-0.025235102,-0.004981148,-0.01305217,-0.013283062,-0.042484067,-0.015551233,-0.020101158,-0.021744562,-0.018471334,0.038871292,-0.006393662,0.017765077,0.0032205994,-0.024583172,0.0052086446,-0.032677963,-0.012868815,-0.0036738939,-0.027693419,-0.046178333,-0.01676002,0.005072826,0.026783435,-0.0035109115,-0.013133662,-0.004176423,0.015727798,-0.007368161,-0.0074632335,0.015972272,-0.011062427,-0.0070761507,0.019354155,-0.03237916,0.00075464224,0.0065430626,0.02704149,0.005639869,0.026715524,-8.446221e-05,-0.04169632,0.0071915966,-0.0073070424,-0.022654548,0.020888906,-0.004488806,-0.019625792,0.007340997,-0.010906236,-0.0029353802,-0.011938457,-0.02135069,0.0008378312,0.0075854706,-0.015714215,0.0123051675,-0.017479857,-0.0048860754,-0.011415556,0.012420613,-0.011238991,0.021635909,0.006974287,0.01822686,0.0114902565,-0.0010958866,0.00039408624,0.0027876773,-0.016013017,0.005079617,0.010179606,-0.022545893,0.0050626392,-0.0022325187,0.0016849999,-0.0145461755,0.0150894495,-0.024623917,0.032569308,-0.014573339,0.0002020302,-0.020943232,-0.0051577124,-0.018498497,0.008964029,-0.0028555868,-0.0063970573,0.0010152443,0.010362961,-0.018607153,0.0070761507,-0.007891063,-0.015184523,-0.017466275,0.02340155,0.0091881305,0.028223112,-0.00940544,0.012230468,-0.024216462,-0.031700067,-0.022803947,-0.01720822,-0.005188272,-0.003164574,0.0019914408,0.026552543,0.035693135,5.687405e-05,0.03425346,0.0099147605,-0.015972272,0.017507022,-0.007979345,0.008210236,-0.022464402,0.0015220175,0.013907827,0.002566972,0.028087292,0.004543133,0.015591979,-8.143812e-06,-0.018199697,-0.010057369,-0.014777067,-0.0127601605,-0.0019184381,0.0087331375,-0.009649914,-0.0026026245,-0.022138437,0.005582146,-0.01844417,-0.017181057,0.000835709,0.0043054507,0.0063664983,-0.0035448663,0.008420755,0.0029591485,0.0038640401,-0.025235102,-0.036480885,-0.017344039,-0.010240724,0.0047468613,-0.0075786794,0.023999153,0.016338982,0.012570014,-0.014885722,-0.013989319,-0.027815655,-0.01392141,0.004824957,-0.010817954,-0.0024854809,-0.021418598,-0.026864925,0.015374669,0.01261076,0.004709511,-0.025493156,0.004132282,0.004050791,-0.017765077,0.009548049,0.014328865,0.024134971,0.012420613,0.022219928,0.019204754,-0.02545241,-0.0007346939,0.002269869,0.007300251,0.0005806246,0.02814162,0.022124855,-0.010403707,-0.020793833,-0.004077954,0.0148178125,-0.016719274,-0.017955223,0.03134694,0.011836594,-0.021309944,-0.0065294807,-0.015252432,-0.026688362,-0.008692392,-0.0077280803,0.0012189723,-0.0050286846,-0.003261345,0.004862307,-0.008590529,0.008570155,0.021635909,-0.002358151,-0.0119112935,0.01261076,0.009792523,-0.006906377,-0.0021578185,0.012135395,0.02288544,0.024433771,0.020508613,-0.01794164,-0.0052969265,-0.0027656069,0.0076669618,-0.014763485,0.028331766,-0.019231917,0.012943516,0.0058741556,-0.0034548864,0.005116967,-0.00984685,-0.015496906,-0.008176281,-0.007762035,0.0064072437,0.031754397,-0.0022274256,-0.023089167,-0.0068554454,-0.007721289,-0.024297953,-0.014790649,0.0054191635,-0.0115174195,-0.03792056,-0.002847098,-0.016447635,0.011109964,0.023510205,0.004475224,-0.010648181,0.0135479085,0.015116613,-0.027381036,0.0020304886,0.007931808,-0.00081660954,-0.014559757,-0.0005806246,-0.0247869,-0.022423657,0.0116396565,0.014111556,-0.054191634,-0.013120079,0.025791958,0.004162841,0.016298236,0.00041615678,-0.008325682,0.01392141,-0.00040957806,0.009439395,0.009887597,0.030151736,0.006468362,0.008291727,0.0007117745,0.0035991936,0.005667032,-0.030667847,-0.0014931561,0.004244332,-0.030341882,0.004937007,-0.0070082415,0.0081287455,-0.011863757,-0.0011604005,-0.028467584,-0.012746578,-0.012067486,0.04870456,0.02289902,-0.029336825,-0.029119514,-0.0039489265,0.0039319494,0.015809288,-0.020467868,-0.034579422,0.0018131788,-0.017398367,-0.029363988,-0.02281753,-0.013975737,0.037350122,0.010505571,-0.007639798,-0.025914194,-0.014844976,-0.013894246,-0.01363619,-0.02355095,-0.004723093,-0.009649914,0.023320058,-0.015782125,0.018933117,-0.008033672,0.0016883954,-0.021812472,0.008658438,-0.005724755,-0.019136844,0.025628975,0.001582287,-0.03390033,0.018566407,0.006227284,-0.016583454,-0.0028318185,0.0006056662,-0.020671595,-0.015225269,-0.0044208965,-0.00031386837,0.013221944,0.029472644,-0.0025771586,-0.013934991,-0.01217614,0.025112865,-0.015727798,0.0055210274,-0.021771727,0.013371344,-0.007707707,-0.015442578,-0.026348814,-0.012230468,-0.0039489265,0.0010016623,0.0033954657,0.012325541,-0.0077144983,0.011918085,0.016175998,0.0036263575,0.0042986595,0.025072118,0.0036399392,0.016203163,0.0153339235,-0.018539242,-0.020006085,0.0018692039,-0.022926185,0.00076313096,-0.010559899,-0.0075718886,-0.011238991,-0.021119798,0.008556574,0.019286245,-0.030178899,0.023483042,0.0039896723,-0.0007117745,0.018403424,0.0120946495,0.003965904,-0.011252574,-0.017914478,-0.010077742,-0.018824462,-0.016488383,0.044792984,0.015007959,0.017928058,0.006665299,0.2204608,-0.017316876,0.0024888765,0.028168784,0.02135069,0.0011128639,0.013018216,0.027652673,-0.027068652,0.0035550527,0.010003042,-0.013126871,-0.005076221,0.0037451987,0.00714406,0.0032817177,-0.037078485,-0.037431616,-0.030640682,0.028521912,0.012244049,0.013975737,0.013500372,-0.026525378,0.051583916,-0.0013233828,-0.0117618935,-0.0005445478,-0.0014651435,-0.0044073146,-0.0065939943,-0.009160967,0.029635625,0.03702416,-0.01057348,-0.017697167,-0.0034718637,0.004892866,0.018933117,-0.0033088813,-0.012529269,0.0069539137,0.004638206,-0.011435929,-0.009778941,-0.0036535212,-0.009677078,-0.01020677,0.0030100804,0.022328584,-0.016352562,-0.010274679,0.01794164,0.027000744,-0.0014549571,-0.015904361,0.005517632,-0.01465483,0.0048521208,-0.006780745,-0.02435228,0.0073477877,-0.023863334,0.021391435,-0.032623634,0.0021968663,-0.0329496,-0.0050117075,0.023591695,-0.019707283,-0.03208036,0.0033003928,-0.014233792,-0.0031357126,-0.027544018,-0.0097314045,0.027978638,0.02098398,0.015795706,0.013452835,-0.0069946595,-0.005470095,-0.01166682,-0.018675061,-0.002409083,-0.0075039794,0.015157359,-0.019625792,0.012746578,0.0026705337,0.004176423,0.001690942,-0.018023131,0.013323808,-0.010668553,0.018688643,0.0013700704,0.02223351,0.013541117,-0.025493156,-0.009276412,0.061335694,0.00765338,0.006919959,0.0018929722,0.0058707604,-0.007130478,0.016053762,0.020087576,-0.019068936,0.01530676,-0.050198566,0.0026773247,-0.0032019243,-0.012502105,-0.0061050477,-0.005072826,-0.002001627,0.002300428,-0.019408483,-0.0027893751,-0.024406608,-0.02121487,0.0053648357,0.011354437,-0.019707283,-0.025520321,-0.0019744632,-0.008638065,-0.002847098,-0.014165883,-0.011137128,0.0022155414,0.01669211,-0.012413823,-0.025465993,-0.011564956,-0.026308069,0.006227284,0.00947335,0.0024837833,-0.010417289,0.0038368762,-0.022776784,0.007368161,-0.013826337,0.015999435,0.011979203,0.0047638384,-0.016067345,0.001297068,0.003388675,-0.013615818,-0.016026597,0.021907546,-0.006006579,-0.02370035,-0.015293178,-0.005612705,-0.003310579,-0.017085984,-0.027150145,0.020033248,0.013133662,-0.02835893,-0.008570155,-0.1751517,0.011544583,0.019625792,-0.051774062,0.014111556,0.027544018,0.018865207,0.015809288,-0.034633752,-0.0115174195,0.021540835,-0.022355746,-0.0019846498,-0.011918085,-0.004855516,-0.011272946,-0.013839918,0.014233792,0.014002901,0.014627666,0.052616138,-0.0153339235,-0.007925017,0.011979203,-0.017588513,-0.018715808,0.011558166,0.033221237,0.013378135,-0.05155675,-0.017194638,-0.012868815,0.031944543,0.020277722,-0.011856967,0.0045940652,0.018715808,-0.026783435,-0.022559475,0.007762035,-0.0051645036,0.014016483,-0.008943656,0.005459909,-0.016298236,0.004077954,0.03645372,0.0018658084,0.0010687228,-0.011272946,0.014356029,-0.0057756873,0.007259506,-0.0058503873,0.0019812542,0.009541259,0.017466275,-0.0061220247,-0.0057722917,-0.020793833,-0.02033205,-0.012576805,0.014097974,-0.0038572492,0.014627666,0.014206628,-0.01428812,0.020427123,-0.037132815,0.018471334,0.0150622865,-0.018104624,-0.004023627,-0.001906554,7.8414036e-05,0.02653896,0.005442932,0.009520886,0.02879355,0.00019237435,-0.010648181,0.040935736,-0.02084816,0.022260673,-0.0060914657,-0.01042408,0.0024005943,0.034008984,-0.012705833,-0.028277438,0.01013207,-0.040555444,0.020725923,-0.00231401,0.01786015,0.012379868,0.005809642,-0.018865207,0.008726347,-0.008121954,-0.020236976,-0.0024922718,-0.009812896,0.021242034,0.01859357,-0.0115174195,-0.014383193,0.014695575,0.037132815,0.0037587807,-0.009140594,-0.014953631,0.004794398,0.011252574,0.0030966648,0.012739788,-0.010057369,-0.0024956674,-0.019001026,0.023279313,0.047047574,0.009602377,-0.015768543,0.014844976,-0.0130929155,-0.016569873,-0.09529035,-0.0043665688,0.017765077,0.021242034,0.0073613697,0.043869417,-0.00021773424,0.012549642,-0.042321086,0.024216462,-0.043896582,-0.018865207,0.005751919,-0.0040847454,-0.003113642,-0.0115174195,0.0008972518,-0.025411665,-0.022437237,0.028603403,0.0034531886,-0.04905769,0.00029604218,-0.010070952,0.018186115,-0.024406608,-0.021540835,0.0103697525,-0.0010008136,-0.00081660954,0.017968806,-0.038979948,-0.023632443,-0.01612167,-0.010145652,0.024814064,-0.018471334,0.004037209,0.005001521,0.00083104026,0.022722457,-0.0010279772,-0.02208411,0.00011640081,-0.021011142,-0.011870548,-0.023659606,0.02887504,0.002319103,-0.011089591,-0.027639091,-0.0051848763,-0.009867224,-0.0045906696,-0.022437237,0.008074418,0.00947335,0.03004308,0.01552407,0.017235383,-0.013934991,0.011177873,0.005877551,0.023292895,0.022043364,-0.016624201,-0.023659606,-0.00049488916,0.007252715,-0.0058809468,-0.022477983,-0.00084377325,-0.00017189546,-0.005198458,-0.03346571,-0.0071372692,-0.014125138,-0.0020797227,-0.0029914053,-0.009887597,0.009174548,-0.008882538,0.006590599,-0.004471828,-0.023659606,0.019476391,0.01465483,-0.017765077,-0.026742687,-0.017724331,-0.01239345,0.0218668,0.013513953,-0.025710467,0.0055651683,0.017724331,0.007952181,-0.007537934,0.004644997,0.010172815,-0.008312101,-0.003507516,-0.045662224,0.021133378,-0.0012317052,-0.025180774,-0.008427546,-0.018729389,0.011768685,-0.010872281,-0.0016238815,0.01020677,-0.012298377,0.00466537,-0.03126545,0.01844417,-0.018267605,0.0049302164,0.011626075,-0.020807413,0.007368161,0.0012758463,-0.011476674,0.012474941,0.010845117,0.023157077,-0.010695717,-0.039496057,-0.028440421,0.009174548,-0.0093375305,0.008739929,0.021622326,-0.0081287455,0.0047502564,0.028087292,-0.009806105,-0.0084003825,0.013072543,0.006481944,0.035122696,-0.02428437,-0.005921692,-0.040147986,0.00026123866,-0.007884271,0.03208036,0.009303576,0.002517738,0.0005772292,-0.017289711,-0.0020253954,0.021907546,0.0071372692,-0.0038640401,-0.033818837,-0.011721148,0.009160967,-0.011279738,-0.016501963,0.020073993,0.0020050225,0.029499806,-0.016271072,0.023428714,-0.008169491,0.004726488,0.0008059987,0.00073426944,0.02106547,0.013731264,-0.025574649,-0.0005802002,-0.0079182265,0.00597602,0.037132815,0.03150992,0.01575496,0.010471616,0.0024718991,-0.0072730877,0.041098718,0.031537086,-0.005161108,-0.026579706,-0.015401833,0.011693984,0.0062069115,0.013656563,0.010980937,0.0025975313,0.004835143,-0.010138861,0.003629753,-0.0024430377,0.0055583776,0.0009303576,0.0026841157,-0.005059244,-0.010621017,-0.0062680296,0.015877198,-0.0054225586,0.0010508967,-0.014030064,-0.021309944,-0.039604712,-0.0030745943,-0.027381036,-0.025859866,-0.024433771,0.005147526,-0.0018488311,-0.002656952,-0.0016349168,0.021242034,-0.010390125,0.025751213,-0.015170941,-0.01465483,-0.03199887,0.018403424,0.009303576,-0.0038300853,0.009711032,-0.0011417253,0.03593761,0.004838539,0.019177591,-0.029255332,0.033574365,0.015619143,0.01305217,0.0015551234,-0.00020415238,-0.011042055,-0.009690659,-0.028983695,-0.0032698335,0.004855516,-0.004162841,0.067746334,-0.0017333853,0.009792523,0.03104814,0.008597319,0.026063595,0.021730982,0.00605072,-0.004519365,-0.017588513,0.013527536,-0.007456443,-0.020250557,-0.016977329,0.013201571,-0.0019557883,0.0029761258,0.021540835,-0.016923001,-0.01866148,-0.0010797582,-0.005867365,0.0014829696,0.011768685,0.0101863975,0.007442861,0.014138719,0.0066585084,-0.011741521,-0.039550386,0.0024820855,0.024026316,-0.041804973,-0.01071609,-0.0056330776,0.017479857,-0.00743607,-0.010654972,0.020101158,-0.009656705,0.0003628904,0.0016315213,-0.016610619,-0.017968806,-0.01814537,0.0067603723,0.00014048739,-0.013629399,-0.025235102]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060513783,0.020702977,0.010962396,0.0026302962,-0.006380589,0.022304911,0.002195873,-0.0024538117,-0.01587002,-0.0404285,0.005963136,0.02459921,-0.003913202,-0.0025013268,-0.010636578,-0.013127724,0.0128833605,0.015299839,0.005908833,0.009088946,-0.0023604787,0.0112271225,-0.003232719,-0.0073580416,-0.014335963,0.039369594,0.029893741,-0.015693534,0.015177658,-0.022223458,0.015747838,-0.004194898,-0.016114382,-0.028617622,-0.014743234,0.028346108,0.010738396,0.0022315094,0.027436534,0.016766017,0.016888198,0.023105878,-0.004493564,0.0046191392,0.0061803474,0.016209412,0.0153677175,-0.014919719,-0.02339097,0.009686278,0.0012888451,0.019114617,-0.021653278,-0.0041745347,-0.02225061,0.0015934504,-0.00244363,0.004687018,0.010032459,-0.02970368,0.018150741,0.009788096,-0.018571587,0.018001407,-0.02659484,-0.004157565,0.006543498,0.00086502807,-0.005963136,0.014688931,0.014050872,0.023038,-0.0087427655,-0.017512681,0.017159713,-0.023499575,-0.008640947,-0.012062029,-0.007941797,0.017132562,0.009435127,-0.010521185,-0.01145791,0.016670987,-0.0042356253,-0.021965519,0.020798007,0.01777062,-0.034292273,0.009496218,0.009326521,0.026364053,0.017146137,-0.006163378,0.006244832,-0.005820591,-0.006054772,0.02624187,0.01513693,0.007724586,-0.010962396,-0.0031869009,-0.05924445,-0.018014984,-0.011159244,-0.0075141625,0.011193183,0.0042967163,0.00038139295,0.00038690807,-0.032798946,0.033939306,-0.0068659214,-0.033939306,-0.007249436,-0.0010326033,0.03491676,-0.007215496,-0.003811384,-0.01250324,0.021259582,0.008403372,0.03135992,-0.024965754,0.024639936,-0.0072901626,-0.017376924,-0.006896467,-0.023431696,-0.022712184,0.024449876,0.02462636,0.018815951,0.018367952,-0.024015453,0.03347773,-0.013256692,0.017118985,-0.02313303,-0.008858159,0.0119194845,0.020702977,0.004456231,0.023553878,0.0057391366,0.010697669,0.013915115,0.027558716,0.003083386,-0.009978156,-0.012829058,-0.014811113,0.02916065,0.029405015,0.0042763525,0.011824454,-0.0054574404,-0.00034766577,-0.012767967,-0.0031529614,-0.009136461,0.024734966,-0.008525553,-0.009272219,0.022074124,0.01358251,0.024029028,-0.012136696,0.008450887,-0.0010046035,-0.024965754,0.03814778,-1.5696929e-05,-0.0087427655,0.00071399816,0.013134511,0.023404544,-0.02802029,-0.0119941505,-0.007819616,-0.026934233,0.014621053,-0.009367248,0.003076598,-0.030518223,0.022970121,0.004249201,-0.029757984,-0.0030426588,-0.0059393784,0.016521653,0.016073655,-0.00045224125,-0.0007708465,-0.66814274,-0.021408914,0.007588829,-0.019413283,0.007079739,0.016141534,0.011220334,-0.006662286,-0.038039174,0.033179063,-0.021571822,-0.015177658,0.0035907784,-0.029540772,-0.019236797,-0.039966926,0.0004403625,-0.025766721,-0.012211362,0.0053997436,-0.011607243,0.0014704204,-0.0064314986,0.01672529,-0.0010300579,0.0078535555,0.004293322,-0.024938602,0.021816185,-0.007398769,0.0059325905,0.02389327,0.005321683,0.00017118137,0.023038,-0.0375776,-0.026092539,0.010161428,-0.0025946598,0.03255458,-0.02535945,-0.01741765,0.008864947,0.004703988,-0.021721156,0.017336197,0.019467585,0.006224469,0.010392215,0.010290397,-0.005793439,0.014838264,0.013114148,-0.011145668,0.018042134,-0.023811817,0.024300544,-0.0133652985,0.011681909,-0.019250374,0.016372321,-0.002737205,-0.029350711,-0.0020075098,0.002835629,0.013928691,-0.006947376,0.0017096925,-0.005331865,-0.031278465,0.022508547,0.011288213,-0.00021063582,0.012985178,0.016521653,-0.0011547848,0.04197613,0.011634394,0.018707344,0.0039505353,0.016779592,-0.015299839,-0.019128192,0.0021059338,0.008403372,0.0012421786,0.008769916,-0.0020906613,0.0039810804,-0.006054772,0.027314354,0.0281832,-0.002730417,-0.025142238,-0.011987363,0.01741765,0.002988356,0.0095708845,-0.0007547253,-0.043876734,-0.0044290796,0.0047548967,0.03133277,0.022237033,0.020920187,0.007982525,-0.01758056,-0.0030460528,0.016548805,-0.024992904,0.016983228,-0.0054133194,-0.015313415,0.0036281117,-0.0053081075,-0.024314119,0.01456675,0.017879225,0.008708825,-0.0043204734,0.008987128,-0.005426895,0.01618226,-0.008966764,0.00082175544,0.0066215587,-0.01542202,-0.013568934,0.008518766,-0.010589063,0.014974021,0.0063636196,0.01980698,-0.025522357,-0.0030579313,0.012455725,0.008247252,-0.007894282,0.016942501,-0.0015213294,-0.02735508,0.015435596,-0.0010597549,-0.020445038,-0.021598974,-0.013215966,0.006255014,0.009835611,0.008328706,-0.006798043,0.006014045,-0.023255212,-0.005701803,-0.0002855144,0.0011929666,0.0033192642,-0.0050128354,-0.033722095,0.0068082246,-0.026866354,0.012034878,0.038609356,-0.02358103,-0.019779827,-0.004194898,-0.02459921,-0.007846767,0.03461809,-0.004157565,-0.0133652985,0.0078535555,-0.015951473,0.0074055563,0.016019352,-0.017919954,0.0033005974,-0.009849186,-0.018965283,-0.0055355006,-0.021341035,0.012381058,0.0030681132,-0.014987597,0.0013711479,0.03535118,0.020988066,0.006255014,0.021191703,-0.028563319,0.027151445,0.00014827235,0.01367754,-0.021341035,0.034509487,0.02030928,0.0138676,0.03138707,-0.013263481,-0.020730129,0.01745838,0.02764017,0.02573957,0.022481397,-0.010969183,-0.0023163576,-0.011003123,0.0040150196,-0.010175004,0.02144964,0.02037716,0.027205747,0.011322152,-0.0067810733,-0.012340331,0.0060276208,0.0025250844,-0.008389796,0.022237033,-0.011098153,0.0029510227,0.009217915,-0.004741321,0.044881336,0.009706642,-0.012082392,-0.012109544,-0.0018462982,0.024395572,0.034346577,-0.002345206,-0.0035907784,-0.017051106,-0.0128833605,0.012333544,0.0041304133,0.00771101,0.00032221127,-0.0018276316,0.033857852,-0.008247252,0.015394869,0.013548571,-0.00032815067,-0.014797538,0.0220877,-0.00041130197,0.029540772,0.026364053,0.0042763525,0.0028661743,0.0074598594,-0.008627372,-0.0029917497,0.021069521,-0.0021941762,-0.020716552,0.011634394,0.009475854,0.005416713,0.030219557,0.029567923,0.014621053,0.017078258,-0.005471016,0.03269034,-0.022875091,-0.02875338,-0.00066521036,-0.015693534,0.0018310256,0.00306981,0.008647735,0.007371617,-0.023119455,0.011519,0.002783023,0.03461809,0.014105176,0.009482642,0.026377628,-0.025617387,-0.017811347,0.009754157,0.008579857,-0.012598271,-0.005084108,-0.011451121,0.010514397,-0.02011922,0.032120157,0.0037774446,0.033857852,-0.017322622,-0.008118282,-0.009611611,0.0010139367,0.025780296,-0.017892802,-0.009937429,-0.008559492,-0.0038249597,0.0065808315,-0.0129580265,-0.013697904,0.008192948,-0.001803874,-0.006594407,-0.036925964,0.018951708,-0.010270034,0.026201144,-0.023377394,0.0017886014,0.00067242247,0.025182964,0.022033397,-0.005311501,0.020268554,0.02837326,0.020390734,0.00036187784,-0.027395807,-0.008966764,0.0038996262,0.007398769,0.026920658,0.004972108,0.022101276,-0.010446519,-0.008131858,0.00029230225,0.0021772063,0.019426858,0.009659126,-0.014716083,-0.00955052,-0.0069575575,0.003473688,0.015924321,-0.010025671,0.0056135613,-0.038555052,-0.03442803,-0.019508313,-0.021870488,0.0138472365,0.017241167,0.024354845,0.0014178145,-0.0019684797,0.0021873883,0.009407976,0.0035534452,0.0030002347,0.01888383,0.020485764,0.022345638,0.005471016,-0.013942267,0.01926395,0.01447172,-0.003960717,-0.014498872,0.023811817,-0.0038249597,-0.013161663,-0.0018225407,0.013100572,-0.011722636,-0.019671222,0.002934053,-0.0114375455,0.043360855,0.005762894,0.009441915,0.00097236113,-0.030572526,-0.025196541,-0.00496532,-0.013032693,-0.013249905,-0.04246486,-0.015544202,-0.020105645,-0.021748306,-0.018422255,0.03888087,-0.006387377,0.017743468,0.003271749,-0.024585633,0.005213077,-0.03266319,-0.012869785,-0.0036722329,-0.027721625,-0.046157457,-0.016806744,0.0050569563,0.026757749,-0.003519506,-0.013107359,-0.0041915043,0.015707111,-0.007364829,-0.0075141625,0.015951473,-0.011071001,-0.007086527,0.01935898,-0.032418825,0.0007670283,0.0065740435,0.027029263,0.00568144,0.026744172,-8.972704e-05,-0.04175892,0.0071611935,-0.007283375,-0.022671456,0.020852309,-0.004463019,-0.019589767,0.00733089,-0.010928457,-0.002984962,-0.011926272,-0.021395339,0.00084339175,0.007670283,-0.015707111,0.012367483,-0.017471954,-0.004894048,-0.011403606,0.012367483,-0.0112271225,0.021598974,0.006943982,0.018245772,0.011471486,-0.0010919971,0.00044715035,0.0027982958,-0.0159922,0.005087502,0.010242882,-0.02256285,0.005050169,-0.0022162367,0.0017461773,-0.014553174,0.015082628,-0.02459921,0.03252743,-0.01456675,0.00017775712,-0.020947339,-0.005131623,-0.018490134,0.008926038,-0.002889932,-0.006445074,0.0010190277,0.010378639,-0.018612316,0.007079739,-0.0078535555,-0.015150506,-0.017431227,0.023295939,0.009183976,0.028210351,-0.009319733,0.012231725,-0.024191936,-0.031712886,-0.022793638,-0.017173288,-0.0051995018,-0.003081689,0.0019616918,0.026567688,0.035676997,3.4920362e-05,0.03421082,0.00990349,-0.0159922,0.017512681,-0.007962161,0.008186161,-0.022399941,0.0014729658,0.013969419,0.0025522357,0.028047442,0.004541079,0.015598505,-2.9511286e-05,-0.01821862,-0.010073186,-0.014797538,-0.012761178,-0.0019328435,0.008708825,-0.009625187,-0.002642175,-0.02211485,0.005518531,-0.018422255,-0.017186865,0.0008612099,0.0043170797,0.0063364683,-0.003470294,0.008430524,0.002978174,0.0038419294,-0.025210116,-0.03646439,-0.017309045,-0.010236095,0.004734533,-0.0076091923,0.024015453,0.01634517,0.012584695,-0.014865416,-0.013969419,-0.027762352,-0.013955843,0.0048567145,-0.0108198505,-0.0024266604,-0.021395339,-0.02687993,0.015381293,0.012598271,0.0047515025,-0.02548163,0.0041304133,0.0040150196,-0.017757045,0.0095708845,0.014322387,0.024096906,0.012381058,0.022223458,0.019141769,-0.0253866,-0.000659271,0.0021755095,0.007276587,0.0005964833,0.028156048,0.02211485,-0.010371852,-0.020838734,-0.004059141,0.014797538,-0.01675244,-0.01796068,0.03138707,0.011817666,-0.021313883,-0.0065672556,-0.015245536,-0.026744172,-0.008654523,-0.0076838587,0.0012336938,-0.005043381,-0.0032445977,0.004846533,-0.008579857,0.008634159,0.02161255,-0.0023859332,-0.011865181,0.01250324,0.009808459,-0.0069270125,-0.0021839943,0.012143483,0.022888668,0.0244363,0.020485764,-0.01796068,-0.005297926,-0.002786417,0.007677071,-0.014743234,0.028346108,-0.019209648,0.012924087,0.0058341664,-0.0035059303,0.0051044715,-0.009835611,-0.015462748,-0.008152221,-0.007778889,0.006465438,0.031794343,-0.0022315094,-0.023024425,-0.006835376,-0.0076974346,-0.024259815,-0.014770386,0.0054370766,-0.011566515,-0.037903417,-0.0028576895,-0.016467351,0.0110913655,0.023513151,0.0044188974,-0.010650154,0.013541783,0.015096203,-0.027409384,0.0020142978,0.007921434,-0.00081539183,-0.014553174,-0.0005773924,-0.024748541,-0.022427093,0.011580091,0.014050872,-0.054167133,-0.013080209,0.025821023,0.004157565,0.016290866,0.00041002923,-0.008308342,0.013955843,-0.00044418065,0.009441915,0.009849186,0.030138103,0.006485801,0.00831513,0.000720786,0.0035568392,0.00566447,-0.030653981,-0.0014899356,0.0042050797,-0.03027386,0.0049585328,-0.007039012,0.00812507,-0.011858393,-0.0011810878,-0.028454714,-0.012740815,-0.012082392,0.048736844,0.022888668,-0.02932356,-0.029079197,-0.0039233835,0.003913202,0.015788564,-0.02047219,-0.034563787,0.001805571,-0.017404076,-0.029350711,-0.022807213,-0.013915115,0.037333235,0.010466882,-0.0076363436,-0.025916053,-0.014811113,-0.0138676,-0.013636813,-0.023553878,-0.0047175633,-0.009652339,0.023336666,-0.015815716,0.01897886,-0.008057191,0.0016452079,-0.021856913,0.008668099,-0.005735743,-0.019128192,0.025630964,0.0016189049,-0.033857852,0.018503709,0.006217681,-0.016548805,-0.0028525987,0.0006104833,-0.020635098,-0.015259111,-0.004391746,-0.0002937871,0.013243117,0.029459316,-0.0025844781,-0.013915115,-0.012197786,0.025142238,-0.015734263,0.005521925,-0.021761883,0.013358511,-0.0077042226,-0.015449172,-0.0263369,-0.012197786,-0.004001444,0.0009833914,0.0034278699,0.012299605,-0.007677071,0.011885545,0.016155109,0.0035907784,0.004306898,0.02500648,0.0036484753,0.016222987,0.015299839,-0.018585164,-0.019969888,0.001858177,-0.022983698,0.00081369485,-0.010555124,-0.007554889,-0.011267849,-0.021110248,0.008566281,0.01926395,-0.030138103,0.023486,0.004052353,-0.000745392,0.01843583,0.012068817,0.0039437474,-0.011247486,-0.017906377,-0.010107125,-0.018843103,-0.016467351,0.04471843,0.015028325,0.018001407,0.0066317404,0.22068694,-0.017309045,0.0025064177,0.028210351,0.021341035,0.0011098153,0.012998754,0.02770805,-0.027056415,0.003526294,0.009944216,-0.01316845,-0.005043381,0.0036993844,0.0071204663,0.0032276278,-0.03703457,-0.03741469,-0.030599678,0.028509017,0.012231725,0.013982994,0.013480692,-0.026459083,0.051533442,-0.0012735724,-0.011756576,-0.00056636217,-0.0015009658,-0.004351019,-0.0066249524,-0.009136461,0.029622225,0.036953114,-0.010595852,-0.017716318,-0.0034821727,0.004866896,0.018856678,-0.0032106582,-0.012462513,0.006991497,0.004690412,-0.011423971,-0.00977452,-0.0036552632,-0.009699853,-0.010175004,0.0030426588,0.022332063,-0.016372321,-0.010249671,0.01796068,0.027015688,-0.0014313902,-0.015937898,0.0054845917,-0.014607477,0.0048567145,-0.006750528,-0.024327694,0.0073444657,-0.023825392,0.021368187,-0.03266319,0.0022060547,-0.032961853,-0.00498229,0.023621757,-0.0197391,-0.032093007,0.0033650822,-0.014281659,-0.0031631433,-0.027545141,-0.009713429,0.027938837,0.020988066,0.015802141,0.013467116,-0.0069507696,-0.005454046,-0.011702272,-0.018639466,-0.0024656905,-0.007439496,0.015150506,-0.019589767,0.012700088,0.0026829022,0.004154171,0.001687632,-0.017987832,0.013344935,-0.010690881,0.018666618,0.0013864207,0.022264184,0.013528207,-0.025522357,-0.009258643,0.06125366,0.00771101,0.0069134366,0.0019107829,0.0058884695,-0.007086527,0.016060079,0.020064918,-0.019087465,0.015326991,-0.050148718,0.0026710234,-0.0032123553,-0.012482877,-0.006098893,-0.0050569563,-0.0019871464,0.0023129636,-0.019399706,-0.0028067806,-0.024409149,-0.021246005,0.005331865,0.01132894,-0.019684797,-0.02548163,-0.0019345405,-0.008593433,-0.0028475078,-0.014173054,-0.011111729,0.002243388,0.016738866,-0.012435362,-0.025468055,-0.011539364,-0.02630975,0.006244832,0.009421552,0.0025115085,-0.010405791,0.0038996262,-0.02282079,0.0073580416,-0.0138472365,0.016019352,0.011980575,0.004795624,-0.016046504,0.0013304207,0.0033905366,-0.013643601,-0.016046504,0.021884063,-0.006017439,-0.023743939,-0.015286263,-0.0055999854,-0.003327749,-0.017064683,-0.027178597,0.020051342,0.013093784,-0.02837326,-0.00850519,-0.1750725,0.011519,0.01964407,-0.051804956,0.014105176,0.027504412,0.018843103,0.015856443,-0.034563787,-0.011519,0.021517519,-0.022332063,-0.002000722,-0.011953424,-0.0048227753,-0.011295001,-0.013799721,0.014240933,0.014010145,0.014621053,0.052646652,-0.015313415,-0.007907858,0.011987363,-0.01755341,-0.018761648,0.0115733035,0.03326052,0.0133652985,-0.051587746,-0.017214015,-0.012842633,0.031902947,0.020227825,-0.011831242,0.0045818062,0.018680194,-0.026798476,-0.022522124,0.0077449493,-0.0052062892,0.013969419,-0.008966764,0.005487986,-0.01625014,0.0040930803,0.036410086,0.0018683588,0.0011081183,-0.011274638,0.014363114,-0.0057391366,0.007249436,-0.0058341664,0.002005813,0.009536945,0.01745838,-0.0061226506,-0.0057968334,-0.020825157,-0.020322856,-0.012577906,0.014105176,-0.0038385354,0.014593901,0.014200205,-0.014322387,0.020431463,-0.03717033,0.018476557,0.0150419,-0.018069286,-0.004008232,-0.0019192677,5.2818043e-05,0.026554113,0.0055287126,0.0095097935,0.028807683,0.00021127217,-0.010670518,0.040890075,-0.020798007,0.022237033,-0.00606156,-0.010392215,0.0024300544,0.034047913,-0.012727239,-0.028237503,0.010141064,-0.040509954,0.020770855,-0.002306176,0.017824924,0.012387847,0.0057832575,-0.018870253,0.008749553,-0.008097919,-0.020254977,-0.0025013268,-0.009815247,0.021205278,0.018612316,-0.011519,-0.014390266,0.014607477,0.037143175,0.0037842325,-0.0091636125,-0.015001173,0.0047515025,0.0112271225,0.0030545376,0.012713664,-0.010079973,-0.0024707813,-0.018951708,0.023295939,0.047026303,0.009604824,-0.015747838,0.014811113,-0.013086996,-0.016562382,-0.095355876,-0.004401928,0.01777062,0.021218853,0.007371617,0.043876734,-0.00015633293,0.012523604,-0.0422748,0.02424624,-0.043876734,-0.018843103,0.005773076,-0.004062535,-0.0030867797,-0.011532576,0.0010037549,-0.025373025,-0.022440668,0.02859047,0.003424476,-0.04900836,0.0002959083,-0.010032459,0.018137164,-0.024409149,-0.021585397,0.010385428,-0.0009935732,-0.00082302815,0.01802856,-0.03893517,-0.023621757,-0.01608723,-0.010093549,0.024802845,-0.018503709,0.004052353,0.0049551385,0.0007937555,0.02265788,-0.0009952701,-0.022060549,0.00015113597,-0.020960916,-0.0119194845,-0.023662483,0.028834835,0.0022909031,-0.0110913655,-0.027653746,-0.005179138,-0.00993064,-0.004612352,-0.022427093,0.008084343,0.009407976,0.030029498,0.01551705,0.017241167,-0.013969419,0.011132092,0.0059020454,0.023241637,0.022019822,-0.01663026,-0.02367606,-0.0004454534,0.007215496,-0.005868106,-0.022399941,-0.00085993716,-0.00022357517,-0.005175744,-0.03345058,-0.0071408297,-0.014105176,-0.0020295705,-0.0029951436,-0.009842399,0.009224704,-0.008864947,0.0065672556,-0.004486776,-0.023662483,0.019467585,0.014621053,-0.017729893,-0.02681205,-0.017716318,-0.012374271,0.021884063,0.013528207,-0.02567169,0.005630531,0.017743468,0.007921434,-0.0075005866,0.004642897,0.010175004,-0.0083354935,-0.003512718,-0.045614425,0.021164551,-0.0012557543,-0.025142238,-0.0084101595,-0.0187888,0.011715848,-0.010860578,-0.0016061777,0.010168216,-0.01227924,0.0046666544,-0.031305615,0.018449407,-0.018300073,0.004992472,0.011593667,-0.020811582,0.007391981,0.0012786633,-0.011485061,0.012469301,0.010792699,0.023173757,-0.010758759,-0.039451048,-0.028454714,0.0091704,-0.009306158,0.0086816745,0.021639701,-0.008131858,0.004710776,0.028074594,-0.009842399,-0.008349069,0.01307342,0.0064790132,0.03513397,-0.024286967,-0.005956348,-0.040184136,0.00027321142,-0.007867131,0.032093007,0.009279006,0.0025250844,0.0006092105,-0.017336197,-0.0019973281,0.021911215,0.007079739,-0.003858899,-0.0338307,-0.011743,0.009129673,-0.011295001,-0.016494503,0.020037767,0.0020075098,0.02951362,-0.016290866,0.02339097,-0.008179372,0.004734533,0.00079545245,0.000743695,0.021096673,0.013745419,-0.025522357,-0.00053539255,-0.0079282215,0.0059325905,0.03706172,0.031468526,0.015707111,0.0104736695,0.0024181756,-0.007249436,0.041107286,0.031522825,-0.0051553803,-0.026621992,-0.015435596,0.011661545,0.0062210746,0.013691116,0.010996335,0.002591266,0.004853321,-0.010107125,0.0036315056,-0.002445327,0.005579622,0.00095114904,0.0027202354,-0.005050169,-0.010616215,-0.0062821656,0.015842868,-0.0054099252,0.0010555123,-0.014010145,-0.021313883,-0.0395325,-0.0031207192,-0.027341504,-0.025848174,-0.024422724,0.0051587746,-0.0018734497,-0.0026829022,-0.001612117,0.021246005,-0.010378639,0.025685266,-0.015177658,-0.014688931,-0.032038704,0.018381529,0.0092993695,-0.0038045961,0.009699853,-0.0011624212,0.03592136,0.0048635025,0.019128192,-0.029242106,0.033532035,0.015625656,0.013066633,0.0015586626,-0.00021572672,-0.01104385,-0.009672702,-0.029052045,-0.0032802338,0.0048567145,-0.0041405954,0.0676614,-0.0017665408,0.009767733,0.031061253,0.008579857,0.026078962,0.021721156,0.006102287,-0.004500352,-0.017566985,0.013514631,-0.007419132,-0.020241402,-0.016996805,0.0132295415,-0.001903995,0.0029730832,0.021503944,-0.016942501,-0.018720921,-0.0010283609,-0.0058375606,0.001469572,0.011743,0.010242882,0.007412344,0.0141323265,0.006662286,-0.011695485,-0.0395325,0.0025132056,0.02405618,-0.04175892,-0.01072482,-0.0055864095,0.017471954,-0.007439496,-0.010609427,0.020105645,-0.009618399,0.00035784754,0.0016087231,-0.016603108,-0.017987832,-0.018137164,0.006784467,0.00018602982,-0.013616449,-0.025182964]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7357295201643934
0.7459409272128139
0.7167102025470471
0.7425756305007283
0.7892730350485572
0.7232595909921706
0.724982565236612
0.6798893151855083
0.7450592884736706
0.6874447736005084
0.7583353448039833
0.7679876820866921
0.7375631033114138
0.7286740392215042
0.7943762567944121
0.7549228014038849
0.8121726324181787
0.715800399120013
0.6772995881330552
0.7267257357919521
0.7727366817209695
0.7524368056654822
0.6713940914142457
0.7444685848942894
0.8051898699906462
0.7263881593088919
0.6828384821197349
0.6987226599607039
0.771733979826997
0.7337783457255864
0.7284556803879059
0.7555255184778472
0.6657531627599205
0.7993207338202729
0.7025666243039722
0.7625211125613092
0.7422545790507492
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006109793,0.020678256,0.010909375,0.002627211,-0.0063270302,0.022293955,0.0022114057,-0.0024252483,-0.015871884,-0.04043325,0.0059095277,0.024642833,-0.003913662,-0.0025355641,-0.010664983,-0.013136055,0.012871297,0.015410256,0.005933288,0.009096803,-0.0024252483,0.011309906,-0.003255162,-0.007386061,-0.01432407,0.039292756,0.029924408,-0.01573611,0.015125132,-0.022198914,0.015763266,-0.004195391,-0.0161027,-0.028620986,-0.014690658,0.028349439,0.010793967,0.0022589262,0.02743976,0.016795142,0.016890183,0.023108594,-0.0044974866,0.004565373,0.006191257,0.016184162,0.015342369,-0.014935049,-0.023420872,0.009721359,0.0012762678,0.019157596,-0.021642245,-0.004195391,-0.022307534,0.0016521899,-0.0024778605,0.0046943575,0.010047215,-0.029680017,0.018112142,0.009741725,-0.018505884,0.017989947,-0.02657081,-0.004175025,0.0064899577,0.0008952544,-0.0059943856,0.014717813,0.014079679,0.023054285,-0.008737004,-0.017541895,0.01710742,-0.023515914,-0.008682694,-0.012110967,-0.007922365,0.017188884,0.009436236,-0.010556364,-0.011418524,0.016632214,-0.0042055743,-0.021886636,0.020746142,0.017772708,-0.03437777,0.009504123,0.00931404,0.026380729,0.017120998,-0.006136948,0.0062896926,-0.005814486,-0.0060622725,0.02627211,0.015097978,0.007698339,-0.010936529,-0.0032381902,-0.059197105,-0.0180171,-0.011160555,-0.007487891,0.01118771,0.0043243757,0.000410926,0.00038228635,-0.032802798,0.033888984,-0.0068769115,-0.033834677,-0.0072570764,-0.0010064187,0.034948017,-0.007243499,-0.0038491695,-0.012450401,0.02120777,0.008390782,0.031254984,-0.025050152,0.024615677,-0.007277442,-0.017365389,-0.006788659,-0.023461604,-0.022687698,0.024466326,0.024615677,0.018804586,0.01842442,-0.024018275,0.0334002,-0.013278617,0.017080266,-0.023149326,-0.00881168,0.011920885,0.02070541,0.0044873036,0.023502337,0.005719445,0.010698927,0.013930328,0.027575532,0.003017559,-0.010026849,-0.0128441425,-0.014840009,0.029191233,0.02935416,0.0043006153,0.011771535,-0.0054546874,-0.000377407,-0.012776256,-0.0031618178,-0.009117169,0.024778605,-0.008526555,-0.009259731,0.022076719,0.013509431,0.024126893,-0.012165276,0.008445092,-0.0010429077,-0.02500942,0.038179416,1.9093102e-05,-0.00881168,0.00069711043,0.013136055,0.023407295,-0.028023584,-0.011975194,-0.0077933804,-0.02692382,0.014622771,-0.0093276175,0.0031058115,-0.030548966,0.022986399,0.004256489,-0.029734327,-0.0030362278,-0.0059468653,0.01649644,0.015966926,-0.00042556407,-0.0007408124,-0.6682213,-0.021479316,0.007562566,-0.019401986,0.0069923187,0.016157009,0.011228442,-0.0066732517,-0.038043644,0.03315581,-0.021547204,-0.01523375,0.0036013834,-0.029517088,-0.019225482,-0.039998777,0.0004234426,-0.02571544,-0.012206009,0.005451293,-0.011588241,0.0014468329,-0.006415283,0.01675441,-0.0010412106,0.007834112,0.004293827,-0.024927955,0.021805173,-0.007487891,0.0059400764,0.023950389,0.005380012,0.000177778,0.022999976,-0.03760917,-0.02617707,0.010196566,-0.0025355641,0.032531254,-0.025348851,-0.0174197,0.0088591995,0.004738484,-0.02176444,0.017338235,0.01948345,0.006231989,0.010352705,0.0103323385,-0.0057771485,0.014812853,0.013027436,-0.011167344,0.018030679,-0.023882503,0.02424909,-0.01336687,0.011751168,-0.019266214,0.016401399,-0.0027884417,-0.029381316,-0.0020043515,0.0028138992,0.013930328,-0.0069923187,0.0016997105,-0.0053324914,-0.03122783,0.02252477,0.011275962,-0.00019252213,0.012939184,0.016469287,-0.0012100785,0.041899603,0.011662916,0.01872312,0.003862747,0.016781565,-0.0153559465,-0.019144017,0.002145216,0.008411149,0.0011786809,0.008716638,-0.0020756323,0.003967971,-0.0059808083,0.027358295,0.028186511,-0.0027561956,-0.025118038,-0.011900519,0.017392544,0.0030056788,0.009510911,-0.0007599055,-0.043854736,-0.004354925,0.004772427,0.031309295,0.022266801,0.02090907,0.00799704,-0.017555472,-0.003059988,0.01651002,-0.024995841,0.016998801,-0.005437716,-0.015342369,0.0036522984,-0.0052985484,-0.024289822,0.014514153,0.017894905,0.008723427,-0.004307404,0.008988184,-0.0054241386,0.016184162,-0.008961029,0.00079257594,0.0066528856,-0.015423833,-0.013543375,0.008485824,-0.010617463,0.014975782,0.006377945,0.01989077,-0.025498202,-0.0030141645,0.01240288,0.008295741,-0.00789521,0.016998801,-0.0015333883,-0.02733114,0.015464565,-0.0010123587,-0.020365976,-0.02156078,-0.013190364,0.0062727206,0.009843555,0.008377205,-0.0068192077,0.006079244,-0.023203636,-0.0057364167,-0.00028766942,0.0011727408,0.0033332314,-0.0049082004,-0.033671748,0.0067852647,-0.026842358,0.0120295035,0.038613893,-0.023597378,-0.019877193,-0.0041682366,-0.0246021,-0.007922365,0.03462216,-0.0040935613,-0.013292194,0.007915576,-0.015953349,0.0073656947,0.016021235,-0.018003523,0.0032432817,-0.009877498,-0.018940357,-0.0055259685,-0.021329965,0.012382514,0.0030650795,-0.014989358,0.0014044038,0.035355337,0.020976955,0.0062387777,0.021112729,-0.028539522,0.02712748,0.00012219587,0.013713091,-0.02141143,0.034459233,0.020284513,0.013930328,0.03133645,-0.01325825,-0.020746142,0.01746043,0.027602687,0.02581048,0.022429729,-0.010956896,-0.002359059,-0.011031571,0.0040358575,-0.010128679,0.021438586,0.020433864,0.027222522,0.011330272,-0.006778476,-0.012260318,0.005987597,0.0024761632,-0.008397571,0.022253225,-0.01108588,0.0029445807,0.009232576,-0.0046909633,0.044832304,0.009728149,-0.012063446,-0.01209739,-0.0018108748,0.024425594,0.03437777,-0.002297961,-0.0035810175,-0.01705311,-0.0128780855,0.0123417815,0.0041139275,0.007623664,0.00031821837,-0.0018176634,0.033916138,-0.008200699,0.015410256,0.013624839,-0.00038695356,-0.014812853,0.022076719,-0.000377407,0.029544244,0.026462192,0.0041987854,0.0028732999,0.007501468,-0.008641963,-0.0030243476,0.021085575,-0.002143519,-0.020678256,0.011622184,0.009476968,0.0054580816,0.0301688,0.029598553,0.014595617,0.017080266,-0.0054682647,0.032639872,-0.02287778,-0.028783914,-0.00070517196,-0.015776843,0.0018091776,0.0031075086,0.008601231,0.00733854,-0.023176482,0.0114931995,0.0028495395,0.034595005,0.014147565,0.0094837565,0.026353573,-0.025620399,-0.017772708,0.009741725,0.008648751,-0.01260654,-0.0050337906,-0.011452467,0.010481689,-0.02009443,0.03204247,0.0036760587,0.033834677,-0.017270349,-0.00804456,-0.009619529,0.0010386648,0.02576975,-0.017894905,-0.009965751,-0.008574076,-0.0038186207,0.006584999,-0.012986705,-0.013645205,0.008227855,-0.0018227549,-0.0066019706,-0.036875997,0.019008245,-0.010291607,0.026204223,-0.023393719,0.0017854173,0.0007573598,0.0251995,0.022090295,-0.0052985484,0.02024378,0.028376594,0.020393131,0.00043998996,-0.027385449,-0.009001762,0.003906873,0.007263865,0.026950976,0.0049014115,0.02211745,-0.010461323,-0.008159968,0.00024630103,0.0021774624,0.019442718,0.009646684,-0.014744967,-0.009538066,-0.006951587,0.0034452444,0.01593977,-0.010040427,0.0056515583,-0.038505275,-0.034404922,-0.01948345,-0.021859482,0.013842075,0.017120998,0.024303399,0.0014553188,-0.0020247174,0.0021163644,0.009429447,0.003564046,0.0030243476,0.018858895,0.02050175,0.02237542,0.005478448,-0.013984637,0.019279791,0.014554884,-0.0039408165,-0.01452773,0.023787461,-0.0038661412,-0.013183575,-0.0018074805,0.013142844,-0.01174438,-0.019700687,0.002927609,-0.0114592565,0.043420263,0.0057771485,0.009436236,0.0009860527,-0.03057612,-0.025172347,-0.004945538,-0.013020648,-0.013183575,-0.04252416,-0.015613915,-0.02014874,-0.021723708,-0.018465152,0.038885437,-0.006408494,0.017786287,0.003267042,-0.024588522,0.005159381,-0.032667026,-0.012850931,-0.003699819,-0.02768415,-0.04616288,-0.016808718,0.0050880997,0.026747316,-0.0035165253,-0.013136055,-0.004181814,0.015668225,-0.007399638,-0.0075150453,0.01599408,-0.011065514,-0.0070941485,0.0193341,-0.03239548,0.0007255379,0.0066087595,0.027005285,0.0056244037,0.02673374,-9.6101954e-05,-0.04170952,0.007168824,-0.0072570764,-0.022646965,0.02085476,-0.0044669374,-0.019578492,0.0072910194,-0.010936529,-0.0029496723,-0.011961617,-0.021370698,0.00081421476,0.0076304525,-0.015654648,0.012294261,-0.017501162,-0.004948932,-0.011371003,0.012457189,-0.011194498,0.021642245,0.0069244322,0.018234339,0.011425313,-0.0010683652,0.00043617134,0.0028206878,-0.015980503,0.0050880997,0.010182988,-0.022565503,0.005057551,-0.002224983,0.0016513412,-0.01452773,0.014975782,-0.024615677,0.032476943,-0.014554884,0.00014585009,-0.020949801,-0.0051152543,-0.018546617,0.0088591995,-0.0028580255,-0.0064831693,0.0009572009,0.010359493,-0.018533038,0.007107726,-0.007840901,-0.015138709,-0.017474009,0.02333941,0.009130746,0.02824082,-0.009341194,0.012226375,-0.024181202,-0.031770922,-0.02278274,-0.017175307,-0.0051356205,-0.0031126002,0.0019822884,0.026557233,0.03568119,5.149834e-05,0.03413338,0.009891076,-0.01599408,0.017528318,-0.008003829,0.008200699,-0.022388997,0.0015927891,0.013943905,0.002584782,0.028132202,0.004551796,0.015559606,7.922312e-06,-0.018152874,-0.01007437,-0.014799276,-0.012721946,-0.0018634868,0.008730215,-0.009633107,-0.0025695073,-0.02211745,0.005546334,-0.018410843,-0.017161729,0.00085367385,0.0043175872,0.006313453,-0.0035436798,0.008424725,0.0029208204,0.0038423808,-0.025240233,-0.036414366,-0.017297503,-0.010278029,0.0047452725,-0.007555777,0.024031853,0.016360668,0.012647271,-0.014867163,-0.014011792,-0.027779192,-0.013998214,0.0048504965,-0.010814333,-0.002474466,-0.021397853,-0.026869511,0.015396678,0.012613328,0.0047486667,-0.025484625,0.004141082,0.0039713653,-0.017745554,0.009524489,0.014337648,0.024167625,0.012375725,0.022212492,0.019198326,-0.025389584,-0.0007251136,0.0022521375,0.0073113856,0.00051551376,0.028159358,0.02211745,-0.010379859,-0.020827606,-0.004073195,0.014772122,-0.016740832,-0.017922059,0.03133645,0.011852998,-0.021357121,-0.0065408726,-0.015315214,-0.02672016,-0.008669117,-0.0076847617,0.0012092298,-0.0050643394,-0.0032738307,0.004874257,-0.008662329,0.008580864,0.021628667,-0.0024082768,-0.011832632,0.012592962,0.009809612,-0.006890489,-0.0021316388,0.012138122,0.022850625,0.024479903,0.02050175,-0.017935637,-0.005284971,-0.0026934005,0.0076304525,-0.014772122,0.028349439,-0.019239059,0.012884875,0.0058959504,-0.0035674402,0.0051288316,-0.009918231,-0.015450988,-0.008234643,-0.007705128,0.006466198,0.03174377,-0.0022147999,-0.023027131,-0.006842968,-0.007705128,-0.02424909,-0.014785699,0.0054614763,-0.011595029,-0.03785356,-0.0028342651,-0.01645571,0.01108588,0.023529492,0.004409234,-0.0106785605,0.013495854,0.015125132,-0.02733114,0.00199926,0.007922365,-0.0007840901,-0.014500575,-0.00060419063,-0.024778605,-0.022348264,0.011601818,0.014079679,-0.054200653,-0.013088535,0.025796903,0.004134293,0.016319936,0.00047435754,-0.008329685,0.01386923,-0.00040286448,0.00941587,0.00987071,0.030087337,0.006466198,0.008302529,0.0007238407,0.0035742288,0.0057228394,-0.030739047,-0.0014918078,0.0042395173,-0.030358883,0.0049319607,-0.006999107,0.008227855,-0.011886941,-0.0011617092,-0.028485212,-0.012789833,-0.012083813,0.048715416,0.02293209,-0.029327007,-0.029028306,-0.0039102677,0.0039917314,0.01579042,-0.020474596,-0.03456785,0.0017752342,-0.017392544,-0.029327007,-0.022823472,-0.013930328,0.03731047,0.010522421,-0.0076847617,-0.025905522,-0.0148535855,-0.01386923,-0.013631627,-0.02354307,-0.0047045406,-0.009680628,0.02333941,-0.01584473,0.018967513,-0.008037772,0.0017081962,-0.021805173,0.008696272,-0.0057296283,-0.019144017,0.025579667,0.001597032,-0.03386183,0.018546617,0.006245566,-0.016591482,-0.002876694,0.00061352504,-0.020569636,-0.015301637,-0.0043820795,-0.00030718683,0.013224307,0.029435625,-0.0025830846,-0.013943905,-0.012212797,0.02515877,-0.015776843,0.0055599115,-0.021669399,0.01331256,-0.0077322824,-0.015464565,-0.02632642,-0.01219922,-0.0039306334,0.0010233903,0.0033620833,0.012287472,-0.0076711844,0.011866576,0.016129853,0.0036624814,0.004287038,0.025063729,0.0036930304,0.016224895,0.015328792,-0.018587347,-0.019972235,0.0019279792,-0.022972822,0.0007993646,-0.010542788,-0.0075422,-0.011248807,-0.021099152,0.008519767,0.019266214,-0.030195955,0.02348876,0.00399852,-0.0007297808,0.018356534,0.012090601,0.003927239,-0.011248807,-0.017922059,-0.01007437,-0.01877743,-0.016482864,0.044805147,0.015002936,0.017976368,0.006595182,0.22049564,-0.017351812,0.0024761632,0.02824082,0.021343544,0.0010912769,0.012952761,0.027724883,-0.027113903,0.0035606516,0.009952174,-0.013102111,-0.00501003,0.0037473396,0.007100937,0.00332814,-0.037038922,-0.03741909,-0.030603275,0.028485212,0.012219586,0.01397106,0.013523009,-0.026502924,0.05148519,-0.0013806435,-0.011785111,-0.0006114036,-0.0014943535,-0.0043651075,-0.00655445,-0.009144324,0.029680017,0.036957458,-0.010583519,-0.017718399,-0.0034062096,0.0049183834,0.01883174,-0.0032755279,-0.012538653,0.006951587,0.0046604145,-0.011418524,-0.009829978,-0.0036624814,-0.009694205,-0.010155833,0.0030430164,0.022348264,-0.01629278,-0.010237298,0.017935637,0.027005285,-0.0014502272,-0.015926193,0.0055497284,-0.014622771,0.0048776516,-0.006778476,-0.024371285,0.0073181745,-0.023868924,0.021329965,-0.03261272,0.002145216,-0.032965727,-0.005016819,0.023583801,-0.019714266,-0.03204247,0.0033366259,-0.014296915,-0.0031855782,-0.0275348,-0.009748514,0.027969275,0.020963378,0.01574969,0.013495854,-0.006948192,-0.00544111,-0.011662916,-0.018668812,-0.0024456144,-0.007447159,0.015179441,-0.019551337,0.01270837,0.0026475769,0.0041614478,0.0017531712,-0.017976368,0.01331256,-0.010705715,0.018668812,0.0013093626,0.02222607,0.013570529,-0.025552511,-0.009300463,0.06120655,0.007698339,0.0069278264,0.0019704083,0.005916316,-0.0071552466,0.016034812,0.02009443,-0.019076131,0.015328792,-0.050181765,0.0026815203,-0.0031567265,-0.012450401,-0.0061063985,-0.005108466,-0.0019687111,0.0022640177,-0.019401986,-0.00277147,-0.024425594,-0.021221347,0.0053324914,0.011371003,-0.019673534,-0.025471048,-0.0019602252,-0.008628385,-0.0028155963,-0.0141611425,-0.011119823,0.0022623206,0.016672947,-0.012348571,-0.025430316,-0.011547509,-0.026285687,0.0062252004,0.00946339,0.002505015,-0.010413803,0.0038355922,-0.022809893,0.0073724836,-0.013808132,0.016061967,0.011961617,0.004769033,-0.01604839,0.0012643877,0.0034180898,-0.013624839,-0.01604839,0.021927368,-0.006035118,-0.023678843,-0.015315214,-0.0056108264,-0.0033552947,-0.01706669,-0.027168212,0.020121586,0.013142844,-0.028376594,-0.008540133,-0.17509308,0.011520354,0.019646378,-0.051729582,0.014106833,0.02749407,0.018845318,0.015817575,-0.034595005,-0.0115271425,0.02146574,-0.02232111,-0.0019839855,-0.011941251,-0.0048369193,-0.011255597,-0.013814921,0.014242606,0.013984637,0.014582039,0.052679993,-0.015301637,-0.007908788,0.012015927,-0.017555472,-0.018763853,0.0115271425,0.03326443,0.01336687,-0.051566653,-0.017243193,-0.012871297,0.031906698,0.020230204,-0.011819055,0.004561979,0.018709544,-0.026788048,-0.022565503,0.0077730143,-0.0051797465,0.013957483,-0.008933875,0.005532757,-0.016224895,0.004134293,0.03644152,0.0019296763,0.0011218259,-0.011221653,0.0143512245,-0.0057228394,0.007223133,-0.0058484296,0.0020213232,0.009504123,0.017501162,-0.006140342,-0.0057296283,-0.020827606,-0.020325245,-0.012552231,0.014133988,-0.0038797185,0.014622771,0.014201874,-0.014229029,0.020420285,-0.03720185,0.018505884,0.015043668,-0.01807141,-0.0040630125,-0.0019076131,3.6701185e-05,0.026462192,0.0054546874,0.009551643,0.028756758,0.00017098934,-0.010603885,0.04084057,-0.02080045,0.022239646,-0.006089427,-0.010400225,0.0024133683,0.034051914,-0.01270158,-0.02829513,0.010094736,-0.040541872,0.020691833,-0.0022215885,0.017799864,0.012375725,0.005790726,-0.018858895,0.008750581,-0.0081328135,-0.020311667,-0.0024761632,-0.009782458,0.021234926,0.018587347,-0.011554297,-0.014364802,0.014649926,0.037174694,0.003801649,-0.009151112,-0.014989358,0.0047554555,0.011255597,0.0031075086,0.01275589,-0.010087947,-0.0024456144,-0.018899627,0.023298677,0.047004674,0.009633107,-0.015722534,0.014840009,-0.013136055,-0.016564328,-0.09542139,-0.004341347,0.017772708,0.021234926,0.0074132155,0.04388189,-0.0001988865,0.012538653,-0.04236123,0.024221934,-0.043909047,-0.018886048,0.0057873316,-0.0040969555,-0.0030294391,-0.011588241,0.0008884657,-0.025389584,-0.022416152,0.02859383,0.0033773577,-0.049014118,0.0003052775,-0.010067581,0.01816645,-0.024357708,-0.021547204,0.010345916,-0.001019996,-0.0007930002,0.018030679,-0.038885437,-0.023610955,-0.016116276,-0.010155833,0.02480576,-0.018451575,0.0040324633,0.004976087,0.00080488034,0.02278274,-0.0009979329,-0.02211745,0.00017130756,-0.020895492,-0.011907307,-0.02363811,0.028838223,0.0022657148,-0.011119823,-0.027697729,-0.005220479,-0.00987071,-0.004585739,-0.022497615,0.00809887,0.009443024,0.029951563,0.015505297,0.01725677,-0.013984637,0.011106246,0.0058959504,0.0232851,0.022049565,-0.016645791,-0.023705997,-0.00051551376,0.0072502876,-0.0058857673,-0.022429729,-0.0008502795,-0.0002031294,-0.0052306615,-0.03345451,-0.0071416693,-0.014106833,-0.0020875125,-0.0029717353,-0.009945385,0.009198633,-0.008920298,0.006584999,-0.0044873036,-0.02363811,0.019442718,0.014582039,-0.017718399,-0.02677447,-0.01766409,-0.012368936,0.021927368,0.0135026425,-0.025633976,0.0056006433,0.017718399,0.007956308,-0.0075354115,0.00455519,0.0102101425,-0.0082889525,-0.0035402856,-0.045619786,0.021153461,-0.001210927,-0.025145192,-0.008431515,-0.01877743,0.011690071,-0.0108346995,-0.0016190951,0.010162623,-0.012307839,0.004619682,-0.03133645,0.018478729,-0.018288648,0.005037185,0.011717225,-0.020868337,0.007345329,0.0012321416,-0.011472833,0.012470766,0.010821123,0.023162903,-0.010664983,-0.039455686,-0.028403748,0.009130746,-0.009286885,0.008737004,0.02161509,-0.0081328135,0.0047486667,0.028132202,-0.009863921,-0.008370416,0.013095323,0.0064390427,0.035192408,-0.024344131,-0.005875584,-0.04018886,0.00030697466,-0.007840901,0.032123934,0.009293674,0.0025745989,0.0006627428,-0.017283926,-0.0020315063,0.021873059,0.0070873597,-0.003923845,-0.03380752,-0.011710436,0.009171478,-0.011248807,-0.016537173,0.019999389,0.0020264147,0.029435625,-0.016333513,0.023393719,-0.00815318,0.004670597,0.000842218,0.0007789986,0.021071997,0.013774189,-0.025525358,-0.00054988137,-0.007935942,0.0059706257,0.03714754,0.03149938,0.015722534,0.010495267,0.0024999236,-0.007229922,0.041030653,0.031526532,-0.0051050712,-0.026611542,-0.015410256,0.011696859,0.006181074,0.013631627,0.011011205,0.0026289083,0.0048165536,-0.010094736,0.0036590872,-0.0024082768,0.005532757,0.00090119446,0.002639091,-0.0050270017,-0.010624251,-0.0062795095,0.015885461,-0.0054648705,0.0010106616,-0.013998214,-0.02126208,-0.039564304,-0.003071868,-0.027303986,-0.025837636,-0.024425594,0.0050948886,-0.0018499095,-0.0026934005,-0.0016810417,0.021234926,-0.010440958,0.025647553,-0.015111554,-0.01467708,-0.032069623,0.018397266,0.009334406,-0.0038152263,0.009660262,-0.001070911,0.03589843,0.004891229,0.019157596,-0.029245542,0.03356313,0.015613915,0.013054591,0.0015384798,-0.00018202091,-0.011031571,-0.009694205,-0.02900115,-0.0032500704,0.0048471023,-0.0041648424,0.06772366,-0.001737048,0.009782458,0.031092057,0.008540133,0.026041295,0.021710131,0.0060520894,-0.004494092,-0.017569048,0.013509431,-0.0074539473,-0.020230204,-0.01690376,0.013129266,-0.0019534365,0.0029411865,0.021479316,-0.016917339,-0.018695967,-0.0010632736,-0.005906133,0.0014960507,0.011737592,0.010305184,0.007379272,0.0141611425,0.00665628,-0.011696859,-0.039455686,0.0024456144,0.02404543,-0.041736674,-0.010712503,-0.005648164,0.017487586,-0.0074335816,-0.010664983,0.020135162,-0.009633107,0.00035810177,0.0016021235,-0.016632214,-0.017962791,-0.018084988,0.0068158135,0.00017056505,-0.013713091,-0.025226656]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7041707230806985
0.7615744148710669
0.7455588190725941
0.6711315057699166
0.7711862469085813
0.7780517044122548
0.7240005962874193
0.7532492047256408
0.7672585026897595
0.7748997636652704
0.6981700337439001
0.7472932855231117
0.7444905756623454
0.7003948989063431
0.7254241155783475
0.7590675829108424
0.7693864927229858
0.7631953848941782
0.7639379035187615
0.662415491258291
0.7949989106589271
0.6770713293310976
0.7250940223402769
0.7401632174478293
0.7562057376048151
0.7093798735156277
0.7704073461457575
0.7458827523690682
0.7028403299856023
0.7645343984046643
0.7416971761320764
0.7512360759323685
0.7685993482805027
0.7230497413393016
0.710898528899796
0.7819141167180149
0.7268949823315629
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060881246,0.020712527,0.010913124,0.0025992522,-0.0063326,0.022355946,0.0021968859,-0.0024413615,-0.015863758,-0.04044715,0.0058945813,0.02463772,-0.00393538,-0.0024821076,-0.010634693,-0.013106614,0.012855348,0.015361224,0.0059319315,0.009086347,-0.0024006157,0.01129342,-0.003207046,-0.0073750173,-0.014288248,0.039333425,0.029934693,-0.01574152,0.015116748,-0.022288036,0.015782265,-0.004237579,-0.016176144,-0.02868515,-0.014695707,0.028359182,0.010736559,0.0022359341,0.02744919,0.016760169,0.016828079,0.023170864,-0.004499032,0.0045839194,0.0061696162,0.016216889,0.01540197,-0.014940183,-0.02341534,0.009704327,0.0012648223,0.01910985,-0.021649683,-0.004196833,-0.0223152,0.0016145584,-0.0024753166,0.0046925754,0.010003131,-0.029663052,0.018131949,0.009731492,-0.018593736,0.017982546,-0.026675018,-0.004179856,0.0065091657,0.0008590605,-0.005921745,0.014722871,0.014057353,0.023062209,-0.008726425,-0.017507177,0.017099718,-0.023510415,-0.008651724,-0.012081175,-0.007979415,0.017167628,0.009385151,-0.010532829,-0.011463194,0.016719423,-0.0042070197,-0.021921322,0.020780435,0.01773807,-0.034280926,0.009548135,0.009364778,0.026389794,0.017126882,-0.006125475,0.006285063,-0.005826671,-0.006084729,0.026240394,0.015157495,0.00773494,-0.010913124,-0.0032155348,-0.05921745,-0.018036874,-0.011137227,-0.007490464,0.011171182,0.0042953026,0.000405974,0.00033530523,-0.03284124,0.033900633,-0.0068826703,-0.03387347,-0.0072391974,-0.0010313819,0.034987193,-0.0072052428,-0.0038233285,-0.012488634,0.021310134,0.0084479945,0.031292893,-0.024990851,0.02463772,-0.0072731525,-0.017330611,-0.0068555064,-0.02341534,-0.022749823,0.024393244,0.02471921,0.018797467,0.018403588,-0.02402653,0.033438846,-0.013323926,0.0171133,-0.023170864,-0.008862245,0.011904609,0.02073969,0.0045058234,0.023496833,0.0057451795,0.010709395,0.013901161,0.02765292,0.0030610396,-0.009975967,-0.012828184,-0.014817945,0.029201265,0.029472906,0.0042885114,0.011802744,-0.005456562,-0.00035122162,-0.012814602,-0.0031374383,-0.009161049,0.024746375,-0.008522695,-0.009228958,0.022070725,0.013568402,0.024080858,-0.01211513,0.008495531,-0.0010186489,-0.024977269,0.038111046,1.095578e-05,-0.008801126,0.00072748505,0.013133778,0.023456087,-0.02800605,-0.011958937,-0.007836805,-0.026933074,0.014587051,-0.009357987,0.0030644352,-0.030505136,0.02300788,0.004237579,-0.029690217,-0.0030202938,-0.0059455135,0.01650211,0.016013158,-0.0003936653,-0.0007491313,-0.6680163,-0.021445952,0.007585538,-0.019395074,0.0070762136,0.016135396,0.011232301,-0.006716291,-0.038083885,0.03322153,-0.021527445,-0.015211822,0.0036229943,-0.029608725,-0.019232089,-0.039958198,0.00041212834,-0.025697114,-0.012237367,0.0054158163,-0.011565059,0.0014677031,-0.0064650243,0.016746586,-0.0010381729,0.007850387,0.0042817206,-0.024963686,0.02177192,-0.0074089724,0.005914954,0.023931457,0.0053648837,0.00012945328,0.023062209,-0.037594933,-0.026172483,0.01013216,-0.0025703905,0.0325696,-0.025357565,-0.017439267,0.008841871,0.0046857847,-0.021758338,0.017357776,0.01950373,0.006223944,0.010376636,0.01033589,-0.0058198804,0.014804362,0.013025123,-0.011177973,0.018104784,-0.0238228,0.024325334,-0.013364673,0.011673715,-0.019286418,0.01636629,-0.0027995864,-0.029337086,-0.0020203202,0.0027792135,0.013935116,-0.00698114,0.0017283075,-0.005361488,-0.031292893,0.022546094,0.011279838,-0.00024681012,0.012950422,0.01650211,-0.0011782371,0.041941166,0.011666925,0.018688811,0.0039150068,0.01670584,-0.015293314,-0.019137016,0.0020848345,0.008414039,0.001259729,0.00876038,-0.0020763457,0.00393538,-0.0060405876,0.027354116,0.028169036,-0.0027537472,-0.025181,-0.011986101,0.017384939,0.0030016184,0.009548135,-0.00079327275,-0.043842643,-0.004437913,0.00475709,0.03134722,0.02224729,0.020902673,0.007999789,-0.017547924,-0.0030457599,0.016529275,-0.024990851,0.01697748,-0.0054599578,-0.015347642,0.0036399718,-0.0052698096,-0.024325334,0.014505559,0.017901056,0.008726425,-0.004319071,0.008998064,-0.0054124207,0.016244054,-0.008977692,0.00082298333,0.0066347993,-0.015442716,-0.013527656,0.008495531,-0.010573574,0.01499451,0.006380137,0.019816115,-0.025547711,-0.003066133,0.012468262,0.008251055,-0.007850387,0.017018227,-0.0015296709,-0.027354116,0.015442716,-0.0010025202,-0.02046805,-0.021595355,-0.013228852,0.0062273396,0.009846939,0.008332547,-0.0068011787,0.0059862593,-0.023238774,-0.005755366,-0.00031068787,0.0012028544,0.003246094,-0.0049098874,-0.033683322,0.0067808055,-0.026824418,0.0119996825,0.038572837,-0.023578325,-0.019829696,-0.0042239972,-0.024610555,-0.0078300135,0.034661222,-0.0041492963,-0.013344299,0.007836805,-0.015999578,0.0073818085,0.016040323,-0.017955383,0.003227419,-0.009840148,-0.018933285,-0.0054905172,-0.021337297,0.01239356,0.0030899013,-0.015035257,0.0013581984,0.035367485,0.02101133,0.0062409216,0.021160731,-0.02854933,0.027191132,0.00016945647,0.013704222,-0.021364462,0.034471076,0.020318648,0.013880787,0.031374384,-0.01327639,-0.020780435,0.017439267,0.02765292,0.025778607,0.022464601,-0.010947079,-0.0022919597,-0.01102178,0.004020267,-0.010138951,0.02141879,0.020454468,0.02717755,0.0113205835,-0.006763828,-0.012373188,0.006016819,0.002514365,-0.008414039,0.02224729,-0.011123645,0.0029404995,0.009188212,-0.004716344,0.044902038,0.0097450735,-0.01211513,-0.012135503,-0.0018437543,0.024406826,0.034416746,-0.0022936575,-0.0036060167,-0.01704539,-0.012916467,0.012325651,0.0041730646,0.007687403,0.00027800625,-0.0018064039,0.03387347,-0.00821031,0.01540197,0.01354803,-0.00035504156,-0.014858691,0.022057142,-0.0003841155,0.029581562,0.026430542,0.0042477655,0.0028997536,0.007490464,-0.008597396,-0.0030457599,0.02102491,-0.0021646288,-0.020698944,0.011646551,0.009453061,0.005456562,0.030233497,0.029608725,0.014641379,0.017140463,-0.0054939124,0.032623924,-0.022885643,-0.028766641,-0.00066466833,-0.015687192,0.0018505453,0.0030882037,0.008638142,0.0073342714,-0.023130119,0.011497149,0.0028063774,0.034606896,0.014098099,0.0094666425,0.026362631,-0.025615621,-0.017765235,0.009751865,0.00862456,-0.012563336,-0.0050626844,-0.011476777,0.010512455,-0.02011492,0.032080647,0.0037656052,0.033846304,-0.017357776,-0.008101653,-0.009568508,0.00096092536,0.02580577,-0.017901056,-0.00992843,-0.008577023,-0.0038912385,0.006580471,-0.012950422,-0.013717804,0.008223891,-0.0018047062,-0.0065736803,-0.03688867,0.018933285,-0.010281562,0.026226811,-0.02333385,0.0017928219,0.00070838537,0.025181,0.022057142,-0.0052664145,0.020223575,0.02833202,0.020386558,0.0004322891,-0.02738128,-0.0090048555,0.0038844475,0.0074361362,0.026973821,0.0049947747,0.022125052,-0.010492083,-0.0081424,0.00028946603,0.0022070725,0.019449402,0.009704327,-0.014682124,-0.009541344,-0.0069403937,0.0034769878,0.01594525,-0.009996341,0.0056229415,-0.038572837,-0.034471076,-0.01950373,-0.021880576,0.013846832,0.017167628,0.024366079,0.0015042047,-0.0019507124,0.0022546093,0.009432688,0.003565271,0.0030033162,0.018933285,0.020495214,0.022328781,0.005517681,-0.01396228,0.019245671,0.014519141,-0.0039285887,-0.014491977,0.023727726,-0.00379956,-0.013194897,-0.0018047062,0.013099824,-0.011721252,-0.019693878,0.0029235221,-0.011402076,0.043380857,0.0057485746,0.009398733,0.0009779029,-0.030586628,-0.02519458,-0.00502873,-0.013011541,-0.013249226,-0.04251161,-0.015605699,-0.02011492,-0.021799084,-0.01841717,0.03887164,-0.006414092,0.017710907,0.0032121392,-0.024583392,0.0051781316,-0.032678254,-0.012882512,-0.0036603448,-0.027707247,-0.046205908,-0.016814496,0.0050558937,0.026715763,-0.0035516887,-0.013113406,-0.00417646,0.015700774,-0.0073274802,-0.007510837,0.015999578,-0.011035362,-0.0071033775,0.019340746,-0.032406613,0.00073300273,0.0065974486,0.027014567,0.0056229415,0.026756508,-0.00010929253,-0.041723855,0.0071916603,-0.0073003164,-0.02265475,0.020861927,-0.0044481,-0.019598803,0.0073206895,-0.010899542,-0.0029846411,-0.0119385645,-0.021432372,0.0008221345,0.007612702,-0.015700774,0.012346024,-0.017466431,-0.0049336557,-0.011463194,0.012379979,-0.011239092,0.021608938,0.0069437893,0.018240605,0.011469985,-0.0010806166,0.00041149167,0.002809773,-0.016013158,0.0050830576,0.010193279,-0.02258684,0.0050626844,-0.0022172588,0.0017087833,-0.014546305,0.015048838,-0.024624137,0.032542434,-0.014587051,0.00019916706,-0.02094342,-0.005164549,-0.018512243,0.00890299,-0.002870892,-0.0064446516,0.0010381729,0.010369845,-0.018580154,0.007069423,-0.007884342,-0.015171076,-0.017493596,0.02334743,0.009181421,0.028169036,-0.009357987,0.012244158,-0.024203096,-0.031727515,-0.02279057,-0.017194793,-0.005167945,-0.003079715,0.001960899,0.02655278,0.03574778,1.8370163e-05,0.03417227,0.009975967,-0.01595883,0.017480014,-0.007904715,0.008155981,-0.022423856,0.0015313687,0.01396228,0.0025483198,0.028114706,0.004539778,0.015592118,-1.24943645e-05,-0.018213442,-0.010111787,-0.014831526,-0.012787438,-0.0019354328,0.008651724,-0.009629627,-0.002539831,-0.022138635,0.005565218,-0.018444335,-0.017167628,0.00080261036,0.00431228,0.006356369,-0.0035143383,0.008393666,0.0029591748,0.003840306,-0.025208162,-0.036508374,-0.01731703,-0.01020007,0.004729926,-0.0075787473,0.023985785,0.01635271,0.012563336,-0.014817945,-0.013989444,-0.027775157,-0.013880787,0.004858955,-0.010804469,-0.002439664,-0.021391625,-0.026838,0.015347642,0.012610872,0.0047401125,-0.025506966,0.004118737,0.003996499,-0.017751653,0.009554925,0.014342575,0.02416235,0.012413934,0.022233708,0.01917776,-0.025411893,-0.00070965866,0.0022274454,0.0072867344,0.00060652045,0.028196199,0.022125052,-0.010383427,-0.020821182,-0.004047431,0.014777198,-0.016760169,-0.0179418,0.03140155,0.01184349,-0.021337297,-0.0065125613,-0.015252568,-0.026715763,-0.008699261,-0.007673821,0.001259729,-0.0050423117,-0.0032579785,0.004858955,-0.008563441,0.0086313505,0.021663265,-0.0023293102,-0.011891027,0.012522589,0.0098061925,-0.00691323,-0.0021510466,0.012149084,0.02285848,0.024474734,0.020508796,-0.017928218,-0.005273205,-0.0027656315,0.007673821,-0.014817945,0.02833202,-0.019191343,0.012902885,0.005880999,-0.003497361,0.005106826,-0.009833356,-0.015497044,-0.0081695635,-0.007782477,0.006427674,0.03175468,-0.0022359341,-0.023021463,-0.0068419245,-0.00773494,-0.024271006,-0.014790781,0.005436189,-0.011565059,-0.03797523,-0.002882776,-0.016474947,0.011076108,0.02348325,0.004451495,-0.010621112,0.013541238,0.015103166,-0.027367698,0.0020372977,0.007986207,-0.0008234078,-0.014573469,-0.0005742633,-0.024814285,-0.022478184,0.011578642,0.014125263,-0.054246444,-0.013086242,0.025846515,0.0041662734,0.016271217,0.00041149167,-0.008332547,0.013935116,-0.00042868138,0.009459852,0.009846939,0.030097676,0.006492188,0.008318965,0.000680797,0.00355848,0.005718015,-0.030640956,-0.0014855295,0.0042409743,-0.030342152,0.0049336557,-0.007008304,0.008162772,-0.01184349,-0.0011773882,-0.02841351,-0.012712738,-0.01204722,0.048732158,0.02287206,-0.029391414,-0.02909261,-0.003959148,0.003908216,0.015782265,-0.020454468,-0.034552567,0.0017996129,-0.017357776,-0.029391414,-0.022831315,-0.013948698,0.037323292,0.010526038,-0.00766703,-0.02587368,-0.014831526,-0.01389437,-0.013649894,-0.023564743,-0.004729926,-0.009670373,0.02333385,-0.015823012,0.018974032,-0.008033743,0.001663793,-0.021812666,0.008617769,-0.005741784,-0.01909627,0.025588458,0.0016086162,-0.033900633,0.018539408,0.006220549,-0.016597185,-0.0028437278,0.00054412824,-0.02060387,-0.01526615,-0.0044243312,-0.00024850786,0.013262807,0.02950007,-0.0025635995,-0.013948698,-0.012183039,0.025140252,-0.01574152,0.005565218,-0.021731174,0.013330718,-0.007707776,-0.015442716,-0.02641696,-0.012244158,-0.003962544,0.0009668675,0.0034124735,0.0122781135,-0.007646657,0.011870654,0.016176144,0.0035924348,0.0042885114,0.025072342,0.0036297853,0.016257634,0.0153068965,-0.018593736,-0.019979099,0.0018675228,-0.022926388,0.0007597422,-0.010593948,-0.007558374,-0.011225509,-0.021065658,0.008563441,0.019272836,-0.030124841,0.023456087,0.004044036,-0.0006871635,0.018430753,0.012074384,0.0039795213,-0.011252674,-0.0179418,-0.010111787,-0.018851794,-0.016474947,0.044820547,0.015035257,0.017968966,0.0066212174,0.22057143,-0.017289866,0.0024990852,0.028196199,0.021337297,0.001097594,0.0130183315,0.027680084,-0.02709606,0.0035211295,0.009935222,-0.013140569,-0.0050423117,0.00373165,0.0071101687,0.0032562807,-0.037078816,-0.037377622,-0.030640956,0.02846784,0.012223786,0.0139758615,0.0135140745,-0.026498452,0.051557213,-0.0013199991,-0.011741625,-0.0005882697,-0.0014541212,-0.0043632123,-0.006580471,-0.009133885,0.02963589,0.03697016,-0.010573574,-0.01772449,-0.0034413352,0.004940447,0.018906122,-0.0032545829,-0.012488634,0.0069539757,0.0046857847,-0.011422449,-0.009779029,-0.0036195987,-0.009677163,-0.010193279,0.0030559464,0.022355946,-0.01635271,-0.010220443,0.017968966,0.027014567,-0.0014362949,-0.015904503,0.005449771,-0.014627797,0.0049098874,-0.0067672236,-0.024366079,0.0073546446,-0.023849964,0.02141879,-0.032623924,0.0021204872,-0.03292273,-0.004991379,0.023578325,-0.019707458,-0.03205348,0.0033530523,-0.014288248,-0.0031323452,-0.027530681,-0.009711118,0.027978888,0.020957,0.015755102,0.0135140745,-0.006933603,-0.0054667485,-0.0116601335,-0.018648064,-0.0024736188,-0.0075176284,0.015171076,-0.01958522,0.012739901,0.0026603711,0.0041492963,0.001697748,-0.017968966,0.013323926,-0.010682231,0.018634481,0.0013531052,0.022260873,0.01355482,-0.025493383,-0.00924254,0.061281912,0.007707776,0.00698114,0.0019303395,0.005935327,-0.0070897955,0.016081069,0.020101337,-0.019069105,0.0153068965,-0.050199013,0.0026264163,-0.003227419,-0.012502217,-0.006064356,-0.005089849,-0.0019473169,0.0023072395,-0.019381491,-0.0028131686,-0.024393244,-0.021242224,0.0053682793,0.0113205835,-0.019666713,-0.025493383,-0.0019320372,-0.008658515,-0.0028403325,-0.014206755,-0.011177973,0.0022104678,0.016692258,-0.012420724,-0.025479803,-0.011537896,-0.026267556,0.0062273396,0.009453061,0.0024905964,-0.010410591,0.0038572834,-0.022817733,0.007388599,-0.013873997,0.016067486,0.01197931,0.004709553,-0.016053906,0.0013115103,0.0034039847,-0.013629521,-0.016013158,0.021948487,-0.006064356,-0.023714144,-0.015279732,-0.005639919,-0.0033581455,-0.017086135,-0.027150387,0.020033427,0.013140569,-0.02833202,-0.008502322,-0.17515327,0.011510732,0.019748205,-0.051774524,0.014138846,0.027503518,0.018851794,0.01587734,-0.034661222,-0.011524313,0.0214867,-0.022274453,-0.0019965516,-0.0119114,-0.0048759324,-0.011252674,-0.013778923,0.014206755,0.0139758615,0.014614215,0.052616607,-0.015347642,-0.007911506,0.012013265,-0.01752076,-0.018743139,0.011558268,0.033248696,0.013330718,-0.051584374,-0.017221956,-0.012834975,0.0318905,0.02026432,-0.011836699,0.0045703375,0.018715974,-0.026810836,-0.022559676,0.007714567,-0.0051917136,0.013948698,-0.008977692,0.0054599578,-0.0162848,0.0040983637,0.036426883,0.0018828026,0.0011672018,-0.011279838,0.014396903,-0.005680665,0.0072120335,-0.0058436487,0.001979574,0.009520971,0.017480014,-0.006122079,-0.005826671,-0.020780435,-0.020345813,-0.012576917,0.014138846,-0.003860679,0.014627797,0.014206755,-0.014274665,0.020440886,-0.037214637,0.018430753,0.015048838,-0.018131949,-0.0040168716,-0.001892989,7.406426e-05,0.026525615,0.005463353,0.009541344,0.028766641,0.00018325068,-0.010648276,0.0409361,-0.020794017,0.022288036,-0.0060677514,-0.010390217,0.0023887313,0.03400929,-0.012685573,-0.028304854,0.010138951,-0.040555805,0.020726107,-0.00234459,0.0177924,0.012400351,0.005765552,-0.018919704,0.008719633,-0.008101653,-0.020223575,-0.0025432266,-0.009812984,0.021228641,0.018580154,-0.01150394,-0.014383321,0.014614215,0.03710598,0.0037757915,-0.009147466,-0.015021674,0.0047808583,0.011184764,0.003105181,0.012726319,-0.01005746,-0.0024345706,-0.019001195,0.023279522,0.047075156,0.00958888,-0.015755102,0.014845109,-0.013093033,-0.016583603,-0.095454186,-0.004325862,0.017778818,0.021215059,0.0073614353,0.043842643,-0.00013603206,0.0125565445,-0.042321462,0.02423026,-0.043896973,-0.01882463,0.005779134,-0.0041017593,-0.0031102744,-0.011517523,0.0008853756,-0.025343983,-0.022437438,0.028576495,0.0034073803,-0.049030963,0.00032851423,-0.010037086,0.018159112,-0.024393244,-0.02156819,0.010383427,-0.0010220443,-0.0008170412,0.018036874,-0.038980294,-0.023632653,-0.016081069,-0.010111787,0.024800703,-0.018457916,0.0040644086,0.004991379,0.00080558145,0.02272266,-0.0010415684,-0.022097887,0.00011947902,-0.020984165,-0.011918191,-0.023659816,0.028820971,0.002351381,-0.011096481,-0.027612174,-0.005164549,-0.009914848,-0.0045907106,-0.022410274,0.008040534,0.009432688,0.030016186,0.01553779,0.017262701,-0.0139758615,0.011130436,0.0058979765,0.023293102,0.022029977,-0.016624348,-0.023673398,-0.00048216042,0.0072527793,-0.0058436487,-0.022437438,-0.00084802514,-0.00018802559,-0.005215482,-0.033466008,-0.0071509145,-0.014098099,-0.0020576706,-0.0029863387,-0.00986052,0.009201794,-0.008930154,0.006553307,-0.004451495,-0.023605488,0.019449402,0.014573469,-0.017778818,-0.026742926,-0.01772449,-0.012359605,0.021894159,0.013527656,-0.02566995,0.0056365235,0.017751653,0.00794546,-0.0075447923,0.004628061,0.010152533,-0.008346129,-0.003504152,-0.045608304,0.021160731,-0.0012155876,-0.025126671,-0.008427621,-0.018743139,0.011714461,-0.01088596,-0.0015712657,0.010152533,-0.012298486,0.004668807,-0.03134722,0.018430753,-0.018322097,0.004991379,0.011673715,-0.020794017,0.0073614353,0.0012317161,-0.011449613,0.012441098,0.010831633,0.023170864,-0.01075014,-0.039523575,-0.028440675,0.009113511,-0.009290077,0.008685679,0.021608938,-0.008189936,0.0046925754,0.028087543,-0.009772237,-0.008366502,0.013072659,0.0065091657,0.035150174,-0.024284588,-0.0059692822,-0.04017551,0.0003280898,-0.00787076,0.032189302,0.009262913,0.0025211559,0.0006561796,-0.017303448,-0.0020101336,0.021921322,0.0070965867,-0.003833515,-0.033791978,-0.011768789,0.0091270935,-0.011252674,-0.01650211,0.020047009,0.001979574,0.029472906,-0.016311962,0.02334743,-0.0081695635,0.004702762,0.00078138855,0.0007639866,0.021079239,0.013744968,-0.025574876,-0.00056195457,-0.007925088,0.0059183496,0.037133146,0.03148304,0.015700774,0.0104852915,0.002471921,-0.0072459886,0.041126247,0.031455874,-0.00513399,-0.026620688,-0.015388388,0.011700879,0.0062171533,0.013643103,0.011055735,0.0025975544,0.0048419773,-0.010118579,0.0036467628,-0.0024905964,0.0055618226,0.00091253954,0.0026994194,-0.0050491025,-0.010634693,-0.006291854,0.01589092,-0.005456562,0.001064488,-0.014003025,-0.021296551,-0.039550737,-0.0030678306,-0.027354116,-0.025860097,-0.024393244,0.005161154,-0.0018369633,-0.0026586733,-0.0015721146,0.021228641,-0.010390217,0.025656369,-0.015184659,-0.014668543,-0.032026317,0.01841717,0.009303659,-0.0038369105,0.009690746,-0.0011646551,0.035965092,0.0048725368,0.019137016,-0.02922843,0.033520337,0.015632864,0.013031914,0.0015084491,-0.00019725709,-0.011062526,-0.009649999,-0.028983954,-0.0032494897,0.0048419773,-0.0041017593,0.06763828,-0.0017342496,0.009833356,0.031048415,0.0085702315,0.02609099,0.021731174,0.006071147,-0.0045431736,-0.017602252,0.013480119,-0.007415763,-0.02026432,-0.017004644,0.0132356435,-0.0019320372,0.0029676636,0.021554608,-0.01690957,-0.018702392,-0.0011425844,-0.005860626,0.0014940182,0.011768789,0.010261189,0.0074293455,0.014111682,0.0066789407,-0.01170767,-0.039550737,0.0024498503,0.02402653,-0.04175102,-0.010695812,-0.005633128,0.017507177,-0.0074700913,-0.010661857,0.020142082,-0.00958209,0.00036204475,0.0016349313,-0.016651513,-0.0179418,-0.018118367,0.006774015,0.00016054329,-0.013643103,-0.025208162]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060441294,0.020699445,0.010899806,0.002624782,-0.0063735005,0.022342904,0.0022512684,-0.0025381946,-0.015945636,-0.040420964,0.0059456574,0.024638316,-0.003986409,-0.0024278383,-0.010655324,-0.013113723,0.0128964055,0.015375178,0.005925284,0.009086567,-0.0023853937,0.011300484,-0.0032648484,-0.007388778,-0.01432934,0.039307214,0.029935418,-0.01567399,0.01521219,-0.022193499,0.015782647,-0.0042037256,-0.016122205,-0.028713008,-0.014709645,0.028332705,0.010730027,0.0022920154,0.027490601,0.01680132,0.01690998,0.023117097,-0.00449235,0.0045670527,0.006224095,0.0162037,0.015402342,-0.014967709,-0.023375161,0.009670607,0.0012682484,0.019110315,-0.021650206,-0.0042614504,-0.022356486,0.0016332731,-0.0024668875,0.004709667,0.010003373,-0.029663771,0.018132387,0.009718144,-0.018567022,0.017996565,-0.02666208,-0.004162979,0.0065025324,0.000933784,-0.005949053,0.014709645,0.01409844,0.023035603,-0.0086790975,-0.017534766,0.017059384,-0.023538148,-0.008624769,-0.012067885,-0.008054311,0.017181626,0.0093989605,-0.010533083,-0.011477054,0.016638333,-0.0041969344,-0.02185394,0.02084885,0.017833577,-0.034336086,0.009541575,0.009317466,0.026417598,0.017140878,-0.0061459965,0.006325962,-0.0058234166,-0.0059931953,0.026281776,0.015103532,0.0077079623,-0.010974509,-0.0032020302,-0.059273213,-0.018010147,-0.011137497,-0.0075313924,0.011137497,0.0043123844,0.00036375131,0.00036842024,-0.032814868,0.033901453,-0.0068964195,-0.033928618,-0.0071918345,-0.0009974511,0.03493371,-0.007212208,-0.0038200254,-0.012421025,0.021229155,0.00845499,0.03129365,-0.024977872,0.024597568,-0.007293702,-0.01733103,-0.0068862326,-0.023415906,-0.022736792,0.02436667,0.024679063,0.018797921,0.018417615,-0.023999946,0.033439655,-0.013290293,0.017100131,-0.023212172,-0.008835294,0.011932062,0.020685863,0.004539888,0.023538148,0.0057622963,0.01074361,0.013935453,0.027531348,0.0030713005,-0.009976208,-0.012855659,-0.014818303,0.029201971,0.02936496,0.0042784284,0.01180303,-0.0055042324,-0.0003348889,-0.012747001,-0.0031748656,-0.009168061,0.024692645,-0.008543274,-0.009235973,0.022003347,0.013527984,0.024095023,-0.0121765435,0.0085161105,-0.0009643442,-0.024977872,0.03811197,-1.8742e-05,-0.008835294,0.00068081345,0.013086558,0.023402324,-0.028033894,-0.012020347,-0.0077555007,-0.026879396,0.014519492,-0.009358213,0.0031052562,-0.030505875,0.022954108,0.0042037256,-0.029690936,-0.0030152735,-0.00595924,0.016488928,0.0159728,-0.00036672244,-0.0007792852,-0.66803247,-0.021419307,0.0075789304,-0.01938196,0.007042429,0.016135788,0.011178243,-0.0067232447,-0.038084805,0.033222336,-0.021527966,-0.0151850255,0.0035517747,-0.029555112,-0.019218972,-0.040040657,0.00044524518,-0.025697736,-0.012271619,0.005371805,-0.011599295,0.0014592498,-0.006424434,0.016746992,-0.0010517803,0.007952444,0.004298802,-0.024923544,0.021813193,-0.007381987,0.005938866,0.023932034,0.005415947,0.0001581066,0.023035603,-0.037623007,-0.026159534,0.010173152,-0.002631573,0.03254322,-0.025331013,-0.017371777,0.008828503,0.0047368314,-0.021745283,0.017344613,0.019463453,0.0062105125,0.010438007,0.010302184,-0.0057589007,0.014886214,0.012957526,-0.0111307055,0.018091641,-0.023823377,0.024271592,-0.013364996,0.0116536245,-0.019300466,0.016339522,-0.0027741874,-0.029337795,-0.0019745287,0.0028607745,0.0139762,-0.0070356377,0.0017453271,-0.00534464,-0.031320814,0.02254664,0.011307275,-0.00023535601,0.012957526,0.016434599,-0.0011536477,0.041942183,0.011694371,0.018730009,0.0039728265,0.01673341,-0.015266519,-0.01913748,0.0021018628,0.0083191665,0.0012410838,0.008760592,-0.002020369,0.003976222,-0.006040734,0.027341196,0.02819688,-0.0027249516,-0.025127279,-0.011999973,0.017426107,0.0029575485,0.0095619485,-0.0007406605,-0.04387087,-0.0044244383,0.0047775786,0.031375144,0.022261411,0.02090318,0.007999982,-0.017589094,-0.003018669,0.016434599,-0.024977872,0.016991474,-0.0054532983,-0.015402342,0.00367062,-0.005269937,-0.02431234,0.014465163,0.017874323,0.0086790975,-0.004288615,0.008957535,-0.0053887824,0.016230864,-0.009032238,0.0008590813,0.0066417507,-0.015402342,-0.013534775,0.00857723,-0.010499128,0.014994874,0.006434621,0.019775847,-0.02548042,-0.0030645092,0.012441399,0.008258046,-0.007898115,0.016937144,-0.0015594193,-0.02730045,0.015470254,-0.0010288602,-0.020454964,-0.02160946,-0.013208799,0.0062682373,0.009860759,0.0083191665,-0.0067911563,0.0059286794,-0.023157842,-0.00566722,-0.00031536433,0.0012003368,0.0032784308,-0.004910006,-0.03376563,0.0067911563,-0.026865814,0.012027138,0.038600933,-0.023510983,-0.019830177,-0.00420033,-0.024597568,-0.007877741,0.034553405,-0.0041561876,-0.013358205,0.007823412,-0.015999964,0.0073480313,0.016067876,-0.017942235,0.0032342882,-0.009853967,-0.01897449,-0.005555166,-0.02125632,0.01239386,0.0030797895,-0.015049202,0.00133616,0.035368342,0.021011837,0.006278424,0.021174826,-0.028550021,0.027286867,0.00014303873,0.013670598,-0.021405725,0.03452624,0.020346304,0.013806421,0.03140231,-0.013229173,-0.020753773,0.017453272,0.027640006,0.025819976,0.022465145,-0.010974509,-0.002278433,-0.011028838,0.0040271557,-0.010152779,0.021419307,0.020400634,0.027178207,0.011341231,-0.0068454854,-0.012325949,0.0059286794,0.0024923545,-0.008427825,0.022247829,-0.011205408,0.003074696,0.009215599,-0.0046994803,0.04482163,0.009704563,-0.012162961,-0.012122214,-0.0018437989,0.02448891,0.03441758,-0.0023480423,-0.0035993129,-0.017045802,-0.012848868,0.012346323,0.004115441,0.0076604243,0.0003024187,-0.0018285188,0.033847123,-0.008183343,0.015456672,0.013555148,-0.00039834378,-0.014818303,0.022152752,-0.00042890396,0.029500782,0.02637685,0.004234286,0.0028947303,0.007497437,-0.008611186,-0.0030118779,0.021011837,-0.0021918458,-0.020631533,0.0116536245,0.009466872,0.0054668807,0.030234229,0.029636607,0.014587404,0.017140878,-0.0055347923,0.032679044,-0.022831868,-0.028821668,-0.00075296947,-0.01567399,0.0019235951,0.0031103496,0.008706262,0.0073616137,-0.023062767,0.011544966,0.0027877698,0.03466206,0.01409844,0.00946008,0.02637685,-0.025656989,-0.017779248,0.009718144,0.008658724,-0.012550057,-0.0050458293,-0.011415934,0.010539874,-0.020115405,0.03208142,0.0037487184,0.03376563,-0.017317448,-0.008067894,-0.009595904,0.0010118823,0.025792811,-0.017833577,-0.009915088,-0.008604395,-0.0038777504,0.006590817,-0.012903197,-0.013718136,0.008217299,-0.0018454967,-0.006499137,-0.03683523,0.018960908,-0.010288602,0.026200281,-0.023334414,0.00183531,0.00067190005,0.02519519,0.02208484,-0.005269937,0.020305557,0.02830554,0.020441381,0.00039091593,-0.027381942,-0.00893037,0.0039184974,0.0073616137,0.02701522,0.004988104,0.022152752,-0.010560248,-0.008190135,0.0002281404,0.0021697744,0.019490618,0.009657024,-0.014641733,-0.009507619,-0.007028847,0.0034414185,0.015986383,-0.010064494,0.005578935,-0.03851944,-0.03436325,-0.01955853,-0.02190827,0.013867541,0.017181626,0.02431234,0.0014507608,-0.0020237647,0.0021765656,0.009412543,0.003541588,0.0030288557,0.01892016,0.020468546,0.022370068,0.0055110236,-0.014003364,0.019300466,0.014519492,-0.003986409,-0.01450591,0.023796212,-0.0038505855,-0.013161261,-0.0018047498,0.013066185,-0.011701162,-0.01966719,0.0030135757,-0.011402352,0.043381907,0.00581323,0.009426125,0.000949913,-0.030587368,-0.02519519,-0.005049225,-0.0130593935,-0.0132563375,-0.042485476,-0.01556533,-0.020128988,-0.021745283,-0.018417615,0.038926907,-0.006434621,0.017765665,0.0032376838,-0.024624733,0.005225795,-0.03270621,-0.012835286,-0.0036740156,-0.0277215,-0.0461527,-0.016896397,0.0050899717,0.026811486,-0.003541588,-0.013140888,-0.0041969344,0.015687572,-0.0073140753,-0.007524601,0.015959218,-0.011062793,-0.0071035493,0.019341214,-0.0324074,0.0007533939,0.0065976083,0.027055968,0.005619682,0.026702827,-0.000108074884,-0.041724864,0.007157879,-0.007252955,-0.022655297,0.020862432,-0.0044583944,-0.019599278,0.007307284,-0.01092697,-0.0029507575,-0.011938853,-0.021364978,0.0007555162,0.007565348,-0.015728317,0.012319158,-0.017507602,-0.0048658634,-0.0114566805,0.012291993,-0.011205408,0.021663789,0.0069745174,0.018241046,0.011504219,-0.0010551759,0.00044482073,0.0028047475,-0.016027128,0.0050763893,0.010200317,-0.022560222,0.005062807,-0.0022393838,0.0016774156,-0.0146281505,0.015022038,-0.02461115,0.032570384,-0.014560239,0.00013518646,-0.020943927,-0.0051476965,-0.018539857,0.0088760415,-0.002809841,-0.0064515984,0.0010152779,0.0104108425,-0.018621352,0.0070560114,-0.007898115,-0.015157861,-0.017534766,0.023334414,0.009174853,0.028142551,-0.009310676,0.012251246,-0.024217263,-0.031755447,-0.022777539,-0.01720879,-0.005263146,-0.0031443054,0.0019405729,0.02653984,0.035667155,-3.3531334e-05,0.034173097,0.0099558355,-0.015999964,0.01743969,-0.007945653,0.008183343,-0.02243798,0.0015144278,0.013921871,0.0025721504,0.028115388,0.0045976127,0.015592495,-5.7512607e-05,-0.018213881,-0.010057703,-0.0147911385,-0.012821703,-0.0019864133,0.008658724,-0.009643442,-0.0025857328,-0.022166334,0.005592517,-0.018417615,-0.017140878,0.0008527146,0.0043429444,0.006305589,-0.0035347969,0.0083870785,0.002955851,0.0039184974,-0.025168026,-0.036509257,-0.017358195,-0.010227482,0.004767392,-0.0076400507,0.024027111,0.016407434,0.012611178,-0.0148590505,-0.014003364,-0.027816577,-0.013921871,0.004797952,-0.010825103,-0.0024583987,-0.021473637,-0.026879396,0.0153480135,0.012604387,0.0047368314,-0.025507582,0.00416977,0.0039898045,-0.017765665,0.009466872,0.01432934,0.02413577,0.012434607,0.022207081,0.019218972,-0.025412507,-0.00067402225,0.0022495706,0.007307284,0.00060780847,0.028169716,0.02208484,-0.010404051,-0.020767355,-0.0040746937,0.014818303,-0.016774157,-0.017928652,0.03134798,0.011891315,-0.021283483,-0.00651951,-0.015239355,-0.02671641,-0.008658724,-0.007701171,0.0012198614,-0.0050594113,-0.0032835242,0.004886237,-0.008597604,0.008604395,0.021623041,-0.0023548335,-0.011836985,0.012584013,0.009792848,-0.0069100014,-0.002125632,0.012149379,0.022859031,0.024461744,0.020577203,-0.017901488,-0.0053038932,-0.0027419294,0.007680798,-0.014763974,0.02835987,-0.019164642,0.012903197,0.005867559,-0.0035721483,0.005113741,-0.009860759,-0.015456672,-0.008196926,-0.007728336,0.0064482028,0.031782612,-0.0021612856,-0.023049185,-0.006828508,-0.007687589,-0.02431234,-0.0147911385,0.0054227384,-0.011531384,-0.037921816,-0.002842099,-0.016488928,0.01104242,0.023538148,0.004356527,-0.010614578,0.01362306,0.015171443,-0.027381942,0.0019371774,0.007938862,-0.00079414085,-0.014492327,-0.00051230786,-0.024746973,-0.02243798,0.011551757,0.014125605,-0.05422059,-0.013066185,0.025860723,0.0041663744,0.016285192,0.00032639995,-0.00840066,0.0139762,-0.00045840305,0.009466872,0.0098879235,0.030098405,0.006454994,0.008325958,0.00069354684,0.0035993129,0.005738527,-0.030614533,-0.0015381969,0.0042376816,-0.030288557,0.004920193,-0.007022056,0.008183343,-0.011884524,-0.0011842079,-0.028522857,-0.012726627,-0.012108631,0.048706174,0.02284545,-0.02936496,-0.02906615,-0.0038777504,0.003942266,0.015769064,-0.020427799,-0.034499075,0.001859079,-0.017344613,-0.029392125,-0.022777539,-0.013949035,0.037324194,0.010560248,-0.007606095,-0.02590147,-0.014804721,-0.013792839,-0.013663807,-0.023565313,-0.0046655242,-0.009684189,0.023375161,-0.015809812,0.018933743,-0.008040729,0.001634122,-0.021758866,0.008665515,-0.005755505,-0.019096732,0.025494002,0.0016027129,-0.03381996,0.018567022,0.006190139,-0.01662475,-0.002850588,0.0005929528,-0.020699445,-0.015239355,-0.0044414164,-0.00029923534,0.013235964,0.029500782,-0.0025721504,-0.013921871,-0.012224081,0.025127279,-0.015714737,0.0055110236,-0.0217317,0.013351413,-0.007782665,-0.015470254,-0.026390433,-0.012169752,-0.003976222,0.00097198423,0.003461792,0.012325949,-0.007701171,0.011911688,0.016162952,0.0036027085,0.0043497356,0.025045784,0.003606104,0.016230864,0.015307266,-0.01860777,-0.020006746,0.0018692658,-0.022954108,0.00071943813,-0.010594204,-0.007470272,-0.01121899,-0.021066166,0.008597604,0.019218972,-0.030152734,0.023470236,0.0040441337,-0.0007190137,0.018417615,0.012108631,0.0039728265,-0.011212199,-0.01791507,-0.010112031,-0.018797921,-0.016475346,0.04482163,0.015008455,0.0179694,0.0066383556,0.22057676,-0.017290285,0.0024855633,0.02819688,0.021364978,0.0011638345,0.012943944,0.027694335,-0.027110295,0.0034804677,0.009976208,-0.013086558,-0.005062807,0.0037147626,0.007083176,0.0032003324,-0.037134044,-0.03740569,-0.030641697,0.028550021,0.012264828,0.013962617,0.0135483565,-0.02648551,0.051422637,-0.0012597595,-0.011782656,-0.00051612785,-0.0015067878,-0.0043972735,-0.0065500704,-0.009120523,0.029609442,0.03705255,-0.010567039,-0.017724918,-0.0034566985,0.004848886,0.018947326,-0.003252964,-0.012434607,0.0069813086,0.0047130627,-0.0114906365,-0.009731727,-0.0036604332,-0.0097249355,-0.010139196,0.003057718,0.022329321,-0.016298775,-0.010213899,0.01791507,0.027028803,-0.0015101834,-0.015864141,0.0054770675,-0.014641733,0.00481493,-0.006828508,-0.024339505,0.0073276577,-0.02378263,0.021460054,-0.03259755,0.002125632,-0.032923527,-0.0049914997,0.023578895,-0.019694353,-0.032054257,0.00333276,-0.014275011,-0.0031171408,-0.02754493,-0.009765683,0.027979564,0.020984674,0.015755482,0.013432907,-0.007001682,-0.005449903,-0.011667207,-0.01867568,-0.0025042389,-0.007544975,0.015157861,-0.019694353,0.012726627,0.0026859024,0.0041935393,0.0017317449,-0.018010147,0.013317457,-0.010641742,0.018662097,0.0013989782,0.022261411,0.013494028,-0.025521165,-0.009324257,0.061337724,0.007680798,0.0069745174,0.0020169734,0.0058709546,-0.007123923,0.016067876,0.020074658,-0.019069567,0.015252938,-0.050200228,0.0026366664,-0.0032003324,-0.012516101,-0.0060882717,-0.0050560157,-0.001964342,0.002302202,-0.019314049,-0.0027181604,-0.024407417,-0.021242738,0.00531408,0.011348022,-0.019721517,-0.025521165,-0.0019524575,-0.008672306,-0.0027996541,-0.014220681,-0.011225781,0.0022580596,0.016719827,-0.012387069,-0.025398925,-0.011565339,-0.026254611,0.0062580504,0.009494036,0.002519519,-0.010383678,0.003887937,-0.02284545,0.00734124,-0.013840376,0.016013546,0.011966017,0.004740227,-0.01604071,0.001230897,0.003429534,-0.013657016,-0.015986383,0.021976182,-0.00602036,-0.0237283,-0.015252938,-0.005592517,-0.003365018,-0.017059384,-0.02713746,0.020074658,0.013174843,-0.028387034,-0.008509319,-0.1751575,0.011558549,0.019694353,-0.05174861,0.014084858,0.027531348,0.018825086,0.015809812,-0.034689225,-0.0114906365,0.021514384,-0.022329321,-0.0019609465,-0.011966017,-0.0048556766,-0.01133444,-0.0137453005,0.014288593,0.013935453,0.014573822,0.052617878,-0.015307266,-0.007884532,0.011993182,-0.017548349,-0.018702844,0.011551757,0.0332495,0.013344622,-0.051558457,-0.017195208,-0.012876033,0.031864107,0.020278392,-0.01186415,0.0045670527,0.018757174,-0.026743574,-0.022519475,0.0077487095,-0.0051646745,0.0139762,-0.008957535,0.00552121,-0.016312357,0.0040203645,0.036454927,0.0018217277,0.0010882828,-0.011320857,0.014342922,-0.005701176,0.0072393725,-0.0058336034,0.0020016932,0.009616277,0.01749402,-0.0060746893,-0.0058336034,-0.020780938,-0.020305557,-0.012577222,0.01415277,-0.0038981237,0.014668898,0.014193517,-0.014275011,0.020427799,-0.03721554,0.018431198,0.015049202,-0.018132387,-0.0040543205,-0.0018607768,7.9000245e-05,0.026580585,0.005473672,0.009446499,0.028713008,0.0002253815,-0.010560248,0.040937092,-0.02084885,0.022247829,-0.0061086453,-0.010376886,0.0023684157,0.034064442,-0.012760582,-0.028278375,0.010152779,-0.040556785,0.020808103,-0.0023140865,0.017806413,0.012346323,0.0057826694,-0.018933743,0.00874701,-0.0081561785,-0.02014257,-0.0024499097,-0.009738518,0.021188408,0.01860777,-0.011470263,-0.014437999,0.014655315,0.03716121,0.003823421,-0.009229181,-0.01503562,0.004763996,0.011293693,0.0030305535,0.012787747,-0.010030538,-0.0024006737,-0.01897449,0.023266502,0.047076296,0.009548366,-0.015755482,0.014831886,-0.013120514,-0.016651915,-0.095456496,-0.0043191756,0.0177385,0.02120199,0.0073955692,0.04381654,-0.00015036044,0.0125364745,-0.042322487,0.02425801,-0.043898035,-0.018865833,0.0058268122,-0.00416977,-0.0030713005,-0.011558549,0.0009354818,-0.025358178,-0.022397233,0.028604351,0.003421045,-0.04897782,0.0003068754,-0.010023747,0.018091641,-0.024393834,-0.021527966,0.010308975,-0.0010331046,-0.00078268076,0.018050894,-0.038981237,-0.023592478,-0.016054293,-0.010132405,0.024814885,-0.018485527,0.0040305513,0.004998291,0.00074957387,0.022682462,-0.0010229179,-0.022071257,9.295395e-05,-0.02090318,-0.01191848,-0.023714717,0.028875997,0.0023684157,-0.011083167,-0.02759926,-0.0051952344,-0.009928671,-0.004563657,-0.022397233,0.008006773,0.0093989605,0.03001691,0.015524583,0.01726312,-0.013921871,0.0111307055,0.0058437902,0.023239337,0.022044094,-0.016665498,-0.023714717,-0.00046646755,0.007205417,-0.005908306,-0.02249231,-0.0008735125,-0.00021349698,-0.0051952344,-0.03346682,-0.0071850433,-0.01409844,-0.0020882806,-0.00302546,-0.009901506,0.009195225,-0.00893037,0.0065874215,-0.004410856,-0.023633225,0.019409126,0.014668898,-0.017752083,-0.026811486,-0.017724918,-0.012407443,0.021840358,0.01356873,-0.025670571,0.0056298687,0.017711336,0.007925279,-0.0074906456,0.004621382,0.010179943,-0.008380287,-0.0035042367,-0.045636572,0.021134079,-0.0012325948,-0.025140861,-0.008441407,-0.018797921,0.011673998,-0.010920179,-0.0015967706,0.010139196,-0.012237663,0.004675711,-0.031429473,0.018458363,-0.01826821,0.0049609398,0.011701162,-0.02079452,0.00734124,0.0012461772,-0.011517801,0.012488936,0.010818312,0.02319859,-0.010791147,-0.039443035,-0.028387034,0.009249555,-0.009358213,0.008651934,0.021595877,-0.008142596,0.0047232495,0.028115388,-0.009786056,-0.0083191665,0.013106932,0.00648895,0.035123862,-0.024285175,-0.005891328,-0.04017648,0.00035144234,-0.007836995,0.032108586,0.00927672,0.002461794,0.0006324264,-0.017249538,-0.0019847155,0.021894688,0.0071171317,-0.0038709592,-0.03387429,-0.011728327,0.009134105,-0.01127332,-0.016529674,0.020047493,0.0019168039,0.029500782,-0.01632594,0.023361579,-0.008224091,0.0046791066,0.0008145143,0.0007347182,0.021052584,0.013786048,-0.025589077,-0.00054881035,-0.007911697,0.005938866,0.03710688,0.0314838,0.015714737,0.010499128,0.0024583987,-0.00716467,0.041045748,0.031510964,-0.0051748613,-0.026594168,-0.015483837,0.011694371,0.0061935345,0.013609477,0.0110016735,0.0026061062,0.004831908,-0.010125614,0.0036162909,-0.0024346295,0.005528001,0.0008972815,0.0026926934,-0.0050424337,-0.010648533,-0.0062512592,0.015864141,-0.005426134,0.0010280113,-0.014044112,-0.021337813,-0.03963319,-0.0031188386,-0.027341196,-0.02584714,-0.024420999,0.00516807,-0.001867568,-0.0026791112,-0.0016222375,0.021215573,-0.010349723,0.025684154,-0.015157861,-0.014723227,-0.03208142,0.018417615,0.009283511,-0.0038302122,0.00969098,-0.0011290298,0.0359388,0.004886237,0.01913748,-0.029174808,0.033575475,0.015660407,0.01303902,0.0015407436,-0.000148981,-0.011056002,-0.009663816,-0.029038984,-0.003252964,0.004794556,-0.0041765613,0.06774858,-0.0017419317,0.009833595,0.03107633,0.008556857,0.026064457,0.021758866,0.0060645025,-0.00449235,-0.017616259,0.013466863,-0.007443107,-0.020224063,-0.017005056,0.013202008,-0.0018743592,0.002955851,0.02155513,-0.016923562,-0.018716427,-0.0011417632,-0.005867559,0.0015645126,0.011728327,0.010220691,0.007415943,0.014112023,0.0066960803,-0.011673998,-0.03952453,0.0025212169,0.024040693,-0.04175203,-0.010763983,-0.0056536375,0.017466854,-0.007422734,-0.010641742,0.02008824,-0.009602695,0.0003635391,0.0016366687,-0.01662475,-0.017887905,-0.01814597,0.006763992,0.0001589555,-0.013704553,-0.025208773]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7466573444076018
0.7633105392019499
0.7018128208598514
0.730041758999295
0.7515448805604612
0.7271899544268844
0.7916865214335898
0.7160917594148027
0.7654770111121479
0.7843915260047838
0.7136995506834806
0.7736679200542665
0.7578299262033995
0.6954200826425652
0.728021365024277
0.6882689357007313
0.7157937633196297
0.6748144419413556
0.7729584951127013
0.6905106069462773
0.7345915890132819
0.7542671970037408
0.6811669313635119
0.7527947816553736
0.7286947674804437
0.6799644664300578
0.7067774961476462
0.6853681605804212
0.6752748400216039
0.6884790825112775
0.6568897197521399
0.7555098984079471
0.7306581999440157
0.6938269000593424
0.7047052887306378
0.7721069958863284
0.7843689844572751
0.7013646335793913
0.7544428563879214
0.731683930653938
0.7252331031831584
0.6801974042297688
0.7481795245064364
0.7301708983960373
0.7550601946156665
0.7359866987614878
0.7711260629662364
0.7361467730832321
0.7521755809790125
0.7323350021395255
0.7723315227083819
0.8003859418420783
0.7467978807489379
0.775677293579764
0.7394198465630055
0.7234533786186864
0.7437797823261367
0.7713918040055961
0.7883464217564053
0.7537296846592891
0.7541060425813878
0.7777758471847029
0.7511617914467849
0.6954050741560468
0.7685247752943771
0.7478405145220267
0.7551728106513163
0.7244881738058854
[{"sentences":["As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code.","LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP.","LLaMA 65B also outperforms PaLM 62B, even when it is trained longer.","The pass@1 results reported in this table were obtained by sampling with temperature 0.1.","The pass@100 and pass@80 metrics were obtained with temperature 0.8.","We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k."],"sections":[{"title":"Code generation","number":"3.5"}],"section_type":"main"},{"sentences":["Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95.","We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate.","We use a weight decay of 0.1 and gradient clipping of 1.0.","We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details)."],"sections":[{"title":"Optimizer","number":"2.3"}],"section_type":"main"},{"sentences":["The details of the hyper-parameters for our different models are given in Table 2 ."],"sections":[{"title":"SwiGLU activation function","number":null}],"section_type":"main"}]
0.7499883906818892
0.7267039340689826
0.7644098536356521
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006093221,0.02067961,0.0109372465,0.0026477505,-0.0063444176,0.022322573,0.0022319178,-0.0024440775,-0.015927238,-0.04046306,0.005916704,0.024630869,-0.0039512585,-0.002466142,-0.010638526,-0.013069025,0.0128653515,0.015343375,0.0059234933,0.009083821,-0.0023354518,0.011256334,-0.0032384025,-0.0073661776,-0.014257118,0.039349645,0.029899213,-0.015709987,0.0151532795,-0.022132479,0.015737142,-0.0041956664,-0.016130911,-0.02865002,-0.014705199,0.028324142,0.010781097,0.0022370098,0.027468715,0.01674193,0.016891291,0.023164423,-0.004484203,0.004613196,0.00617469,0.016171645,0.015397687,-0.014963185,-0.023381675,0.009640528,0.0012746543,0.019104538,-0.021616507,-0.0041786935,-0.022363309,0.001636174,-0.002496693,0.004681087,0.009986772,-0.029654806,0.018099751,0.009749154,-0.018574988,0.018018281,-0.026599709,-0.00416851,0.0064903833,0.00086815667,-0.006038908,0.0147187775,0.014067023,0.023042219,-0.008710421,-0.017556623,0.01712212,-0.023544613,-0.008683264,-0.012104972,-0.007983986,0.017135698,0.009416487,-0.010536689,-0.011453219,0.016646883,-0.0042024553,-0.021860914,0.020815393,0.017814608,-0.034298554,0.009497956,0.009301072,0.026382457,0.01712212,-0.006144139,0.0063240505,-0.005825051,-0.006052486,0.026233098,0.015098967,0.0077327895,-0.010964403,-0.0031942734,-0.059200987,-0.017991126,-0.011161286,-0.007508749,0.011188443,0.0043518157,0.00041519615,0.00038358438,-0.03280495,0.03389121,-0.006887546,-0.03386405,-0.0072168172,-0.0010005442,0.034895994,-0.0072303955,-0.0037849254,-0.012444427,0.021249896,0.008459223,0.03128419,-0.025011059,0.02461729,-0.0073118648,-0.017366529,-0.006833233,-0.023422409,-0.022689186,0.024413617,0.024671603,0.018805819,0.01841205,-0.024047006,0.03342955,-0.013293065,0.017094964,-0.023164423,-0.008852991,0.011921667,0.020733923,0.0045045703,0.023517456,0.0057537653,0.010692839,0.01394482,0.02757734,0.003087345,-0.009939248,-0.012824617,-0.014827403,0.02916599,0.029437555,0.004297503,0.011867354,-0.005485596,-0.00035027534,-0.012838195,-0.0031501441,-0.009158501,0.024725916,-0.008547482,-0.009239971,0.021983119,0.013537473,0.02408774,-0.012125339,0.008499959,-0.0010073333,-0.025011059,0.038154762,-1.1489763e-05,-0.008785101,0.0007128559,0.013150495,0.023395251,-0.028052578,-0.012023503,-0.007834626,-0.026939165,0.014555839,-0.009362174,0.0030500048,-0.03052381,0.022920014,0.004260163,-0.029681962,-0.003056794,-0.0059574386,0.016497523,0.016008707,-0.00040204223,-0.0007412853,-0.6682651,-0.02143999,0.0075970073,-0.019376103,0.007074246,0.016117333,0.011208811,-0.006683873,-0.038018983,0.033239454,-0.021548616,-0.015221171,0.0035846469,-0.02954618,-0.019199586,-0.039974242,0.00046335635,-0.025689969,-0.0122611215,0.005404127,-0.011589,0.0014070418,-0.006425887,0.016701195,-0.0010209115,0.007895728,0.004297503,-0.024929589,0.021833759,-0.007372967,0.005930282,0.023897646,0.0053837593,0.00018988275,0.02302864,-0.03763879,-0.026178785,0.010170078,-0.0026053186,0.032533385,-0.025309779,-0.017407263,0.008798678,0.0046946653,-0.021752289,0.017366529,0.019498307,0.0062188194,0.010441642,0.010299071,-0.005855602,0.01485456,0.013021502,-0.011188443,0.018086173,-0.023802599,0.024291413,-0.013320222,0.011711204,-0.019267477,0.016334584,-0.0028038998,-0.029383242,-0.0019959966,0.0028768827,0.01401271,-0.0069656204,0.0017532861,-0.005332841,-0.031257033,0.022458356,0.01129028,-0.0002116291,0.012926454,0.016470365,-0.0011889418,0.041929506,0.011690837,0.01871077,0.003907129,0.016769087,-0.015329797,-0.019145273,0.0021114114,0.008404911,0.0012033687,0.008757944,-0.0020197583,0.0039716256,-0.006015146,0.027305776,0.028188359,-0.0027512843,-0.02514684,-0.011989557,0.017393684,0.0030126648,0.009565847,-0.0007480744,-0.043830454,-0.004470625,0.004748978,0.031338505,0.022281839,0.020856127,0.008004353,-0.017556623,-0.0030296377,0.016456788,-0.024956746,0.016999917,-0.005438072,-0.015370531,0.0036219868,-0.0052853175,-0.024291413,0.01447437,0.0178825,0.008690053,-0.0043212646,0.00901593,-0.0053735757,0.016225958,-0.008961617,0.00081257086,0.0066567166,-0.015452,-0.013530684,0.00856106,-0.010523111,0.014976763,0.006422492,0.019783448,-0.025540609,-0.0030720695,0.0124308495,0.008262339,-0.007861782,0.01697276,-0.0015360347,-0.027332932,0.015506313,-0.0010200628,-0.020394469,-0.021602929,-0.013177651,0.0062595536,0.009857778,0.008330231,-0.0068298387,0.0059981733,-0.023232313,-0.005736793,-0.00031335957,0.001143964,0.0033062936,-0.004939073,-0.033673957,0.0068366276,-0.026844118,0.012077816,0.038616423,-0.02355819,-0.01985134,-0.004182088,-0.024603713,-0.0078074695,0.034597274,-0.0041549318,-0.013327011,0.007854993,-0.016022285,0.007386545,0.016035864,-0.017963968,0.0032316134,-0.00985099,-0.018955179,-0.0054991744,-0.021277051,0.01242406,0.003061886,-0.014990342,0.0013620639,0.035330497,0.020978332,0.0062391865,0.021127691,-0.028541394,0.027224306,0.00012284034,0.013707201,-0.021399256,0.034515806,0.020285843,0.01386335,0.03136566,-0.013286277,-0.020747501,0.017434418,0.027604496,0.025825752,0.022458356,-0.010950824,-0.002281139,-0.011025505,0.0040429113,-0.010095398,0.021426411,0.020394469,0.02721073,0.011297069,-0.0067992876,-0.012288278,0.006018541,0.0024712337,-0.008418489,0.022227526,-0.011154498,0.0029804166,0.009239971,-0.004674298,0.044835243,0.009701629,-0.012098183,-0.012118551,-0.0018415444,0.024454352,0.03440718,-0.0023015062,-0.003598225,-0.017081385,-0.012892508,0.012342591,0.0041311695,0.007658109,0.00033393904,-0.0018432417,0.03386405,-0.008180871,0.015397687,0.013571419,-0.0003593982,-0.014813825,0.022091744,-0.0003487902,0.029491868,0.026382457,0.0042160335,0.0028734882,0.007508749,-0.008628951,-0.0029668384,0.020978332,-0.0021538432,-0.020666033,0.011616156,0.009484379,0.0054109157,0.030197933,0.029600494,0.0146237295,0.017108541,-0.005485596,0.03264201,-0.022852125,-0.028812958,-0.00068782107,-0.015723564,0.0018653063,0.0030839504,0.008690053,0.007372967,-0.023123687,0.011507531,0.002817478,0.03462443,0.014148492,0.0094708,0.026328145,-0.025594922,-0.017773874,0.009749154,0.008622162,-0.012512319,-0.0050443043,-0.011419273,0.010509533,-0.020095747,0.032098882,0.0037679526,0.033809736,-0.017312216,-0.008085823,-0.009593003,0.0010158197,0.025798595,-0.017923234,-0.009891724,-0.008622162,-0.0038901565,0.00662956,-0.012940032,-0.013632521,0.008241972,-0.0018992518,-0.0065548797,-0.03687841,0.018968757,-0.010292281,0.02620594,-0.023313783,0.0017991125,0.0007442555,0.025214732,0.022064587,-0.0052751335,0.020258686,0.028324142,0.020421624,0.00042156092,-0.027400823,-0.008961617,0.003907129,0.0074204905,0.026979899,0.004949257,0.0221189,-0.010482376,-0.008194448,0.00028259645,0.0021911834,0.019484729,0.009626949,-0.014650886,-0.009531902,-0.007006355,0.0034895993,0.015981551,-0.010041085,0.0055942214,-0.038589265,-0.03440718,-0.019498307,-0.021901648,0.01386335,0.017176433,0.024345726,0.0014341982,-0.0020197583,0.002147054,0.009402909,0.0035676742,0.0030415186,0.018887287,0.020462358,0.022390464,0.0054584397,-0.014026289,0.019281056,0.014487947,-0.0039682314,-0.014501526,0.023775442,-0.0038494219,-0.013198018,-0.0018194799,0.013096182,-0.01174515,-0.019715557,0.0029617466,-0.011398906,0.043368794,0.0057469765,0.009436854,0.0009716905,-0.030578123,-0.025173998,-0.0049865968,-0.013048658,-0.013184439,-0.04249979,-0.015560626,-0.020122904,-0.021725133,-0.018439205,0.0389423,-0.006439465,0.017760295,0.0032757425,-0.024576556,0.00519027,-0.032696325,-0.012892508,-0.0036796944,-0.0277267,-0.046111595,-0.016850555,0.0051461407,0.026762648,-0.003499783,-0.0131301265,-0.0041956664,0.015709987,-0.007305076,-0.0074544363,0.01591366,-0.011039083,-0.0071014026,0.019335369,-0.032397605,0.0007132802,0.0065888255,0.027007055,0.0056756907,0.026776226,-6.746672e-05,-0.04173941,0.0071896613,-0.0072914977,-0.022662029,0.020883285,-0.004487598,-0.019566197,0.0072982865,-0.010903301,-0.002965141,-0.011921667,-0.02143999,0.00074765005,0.0075970073,-0.015655674,0.012315434,-0.017488731,-0.004908522,-0.011446429,0.012390114,-0.011222389,0.02167082,0.00693507,0.018249111,0.011480374,-0.001073527,0.0004646293,0.0028276618,-0.015995128,0.005054488,0.010176866,-0.022594137,0.0050714607,-0.0022471934,0.0017083082,-0.014569417,0.015031076,-0.024590135,0.03250623,-0.014582995,0.00017057624,-0.020924019,-0.005135957,-0.018547831,0.0089140935,-0.0028497265,-0.006446254,0.0010226088,0.010448431,-0.018574988,0.0070810355,-0.007861782,-0.015180436,-0.017488731,0.023354517,0.009158501,0.028215516,-0.00932144,0.012247544,-0.024209945,-0.03182732,-0.022770654,-0.017230745,-0.0052004536,-0.0030890424,0.0019501701,0.026545396,0.03568353,-2.956973e-05,0.034217082,0.009905303,-0.015954394,0.01750231,-0.007936463,0.008146925,-0.022404043,0.0015182133,0.013931241,0.0025611895,0.02810689,0.0046098013,0.0155877825,-6.0995855e-05,-0.018167643,-0.0100682415,-0.014827403,-0.012756727,-0.0019179218,0.008690053,-0.009647316,-0.0025917403,-0.02220037,0.0055874325,-0.018398471,-0.017176433,0.0008206329,0.0043246592,0.006296894,-0.0034963884,0.0083845435,0.0029566546,0.0038596056,-0.025173998,-0.03644391,-0.017339371,-0.010190445,0.0047354,-0.0075766398,0.023965538,0.016416052,0.012566632,-0.014868137,-0.014039867,-0.027794592,-0.013917663,0.0048542093,-0.010815042,-0.002411829,-0.021385677,-0.026776226,0.015356953,0.0126209445,0.004681087,-0.025499875,0.004137959,0.0039885985,-0.01773314,0.009531902,0.0143793225,0.024128476,0.012390114,0.022213949,0.019226743,-0.025418404,-0.0006721213,0.0022285234,0.0073118648,0.00053718785,0.028161203,0.02205101,-0.010360173,-0.020801814,-0.004104013,0.014800247,-0.01674193,-0.017896079,0.031338505,0.011860564,-0.021358522,-0.0065175397,-0.015289062,-0.026694756,-0.00863574,-0.0076852655,0.0012211901,-0.005088433,-0.0033012019,0.0048372364,-0.008615373,0.008615373,0.021643663,-0.0023609109,-0.011853775,0.012593788,0.009857778,-0.006884151,-0.0021317785,0.012111762,0.022852125,0.02446793,0.020503094,-0.017936813,-0.00526495,-0.002736009,0.0076309526,-0.014732355,0.028324142,-0.019186007,0.0128653515,0.0058895475,-0.0035676742,0.005085039,-0.009857778,-0.015465579,-0.008153714,-0.007766735,0.00644286,0.03174585,-0.0021843943,-0.023096532,-0.006812866,-0.0076988437,-0.024277836,-0.014745934,0.0054143104,-0.011527899,-0.037856042,-0.0028683965,-0.016456788,0.011079818,0.023476722,0.004385761,-0.0105977915,0.013584997,0.0151532795,-0.027292198,0.001999391,0.007970408,-0.00085160823,-0.014460791,-0.000542704,-0.024766652,-0.022444777,0.011575422,0.014134915,-0.054204207,-0.013075815,0.025839329,0.004151537,0.01629385,0.00042813786,-0.008357387,0.013890507,-0.0004217731,0.009477589,0.009871357,0.03008931,0.0064360704,0.008343808,0.0007319503,0.0035642795,0.005791106,-0.030659594,-0.0015148189,0.004256768,-0.030306559,0.0049017332,-0.0070335115,0.008180871,-0.01189451,-0.0011762122,-0.028459923,-0.012770304,-0.012084605,0.04871861,0.022852125,-0.029383242,-0.029057365,-0.003886762,0.0039580474,0.015791455,-0.020435203,-0.03454296,0.0017838371,-0.017380105,-0.029383242,-0.022811389,-0.013904084,0.03736723,0.010523111,-0.0076105855,-0.025920799,-0.014800247,-0.013822615,-0.013680045,-0.023544613,-0.0046709036,-0.009694841,0.023327362,-0.015805034,0.018982334,-0.0080450885,0.0016692709,-0.02182018,0.008683264,-0.0057707382,-0.019118117,0.025554188,0.001605623,-0.033809736,0.018547831,0.0061848736,-0.01659257,-0.0028887638,0.0005528876,-0.020638876,-0.015343375,-0.004409523,-0.00027220065,0.013204807,0.02946471,-0.0026239885,-0.013904084,-0.012227176,0.025119685,-0.015750721,0.005478807,-0.021725133,0.013272698,-0.0077327895,-0.015438423,-0.026328145,-0.012206809,-0.0039885985,0.0009886633,0.0033911574,0.01234938,-0.0076716873,0.011833408,0.016130911,0.0035948306,0.0043450263,0.02507895,0.0036559324,0.016225958,0.015289062,-0.018615723,-0.0200007,0.0019145273,-0.022974327,0.0007641985,-0.0105774235,-0.0074748034,-0.011222389,-0.021073379,0.008547482,0.019226743,-0.030170778,0.023463143,0.004008966,-0.00069376157,0.018439205,0.012104972,0.003941075,-0.011222389,-0.017909655,-0.010102186,-0.01879224,-0.016511101,0.04478093,0.014949607,0.017963968,0.0066261655,0.22040147,-0.017298637,0.0024508666,0.028215516,0.021331364,0.0011125644,0.012994345,0.027685966,-0.027102103,0.0034794158,0.009918881,-0.013109759,-0.0050069643,0.0037238235,0.007060668,0.0032893207,-0.03706851,-0.037394386,-0.030578123,0.028514236,0.012206809,0.013971976,0.013523895,-0.026531817,0.051488563,-0.0013493344,-0.01174515,-0.00057792244,-0.0015292456,-0.004378972,-0.006548091,-0.009172079,0.029654806,0.036987036,-0.0105977915,-0.017760295,-0.003462443,0.004888155,0.018873708,-0.0032587699,-0.012491951,0.0069791987,0.004684482,-0.011473586,-0.009823834,-0.0036661162,-0.009667684,-0.0101429215,0.0030177566,0.022295417,-0.016348163,-0.0102379685,0.017950391,0.027020633,-0.0014418359,-0.015927238,0.005472018,-0.014637308,0.0048609986,-0.0068400223,-0.024359304,0.007318654,-0.023843333,0.021426411,-0.032614857,0.0021453567,-0.032913577,-0.005047699,0.023585347,-0.019715557,-0.03212604,0.003345331,-0.014284275,-0.0031823926,-0.02749587,-0.009688051,0.027971108,0.020978332,0.015723564,0.013476371,-0.0069282805,-0.005461834,-0.011684048,-0.01864288,-0.0024678393,-0.0074815927,0.015180436,-0.01962051,0.0126956245,0.0026613288,0.0041481424,0.001746497,-0.018004704,0.013313433,-0.010658893,0.018670036,0.0013697017,0.022295417,0.013503527,-0.025581343,-0.009253548,0.06121056,0.0076648984,0.00697241,0.0019790237,0.005892942,-0.007128559,0.016049441,0.02008217,-0.019077381,0.01530264,-0.050185055,0.002663026,-0.0032078517,-0.012464794,-0.006093221,-0.005091828,-0.0019196192,0.002281139,-0.019416837,-0.0027954136,-0.024440775,-0.021236317,0.0052853175,0.011344593,-0.019688401,-0.02545914,-0.0019213164,-0.00863574,-0.0027903216,-0.014216384,-0.011174865,0.0022234316,0.016701195,-0.012403693,-0.025499875,-0.011548266,-0.026260253,0.006242581,0.009450433,0.0025221522,-0.010400907,0.0038697892,-0.022824967,0.0073322323,-0.013856561,0.016090177,0.011962401,0.0047387946,-0.016076598,0.0012619247,0.0034200111,-0.013646099,-0.016008707,0.021928806,-0.0060355132,-0.023693973,-0.015289062,-0.005634956,-0.0034030383,-0.017081385,-0.02721073,0.020041434,0.013123338,-0.028378455,-0.0084795905,-0.17510457,0.011534687,0.019661244,-0.051787283,0.014107758,0.027523028,0.018819395,0.015805034,-0.034597274,-0.011500742,0.02152146,-0.022336151,-0.0019603537,-0.011942034,-0.004888155,-0.011263124,-0.013802248,0.014229962,0.013985554,0.014569417,0.052683447,-0.01530264,-0.007916095,0.011975979,-0.017529467,-0.01871077,0.011507531,0.033239454,0.01334059,-0.05159719,-0.017162854,-0.012906087,0.031908788,0.02023153,-0.011867354,0.0045724614,0.018751506,-0.026735492,-0.022553403,0.007759946,-0.0051699025,0.013971976,-0.008934461,0.0055670654,-0.016266692,0.004063279,0.036416754,0.00187549,0.001122748,-0.011242756,0.014352166,-0.0057232147,0.0072236066,-0.0058657858,0.0019790237,0.009559059,0.017515888,-0.0061645065,-0.0058148676,-0.020801814,-0.020340156,-0.012593788,0.014134915,-0.0038528165,0.014637308,0.014216384,-0.014257118,0.020421624,-0.037231445,0.018452784,0.015085389,-0.018126907,-0.0040734624,-0.0018907654,4.0761144e-05,0.026558975,0.0054652286,0.00954548,0.0287858,0.00019645969,-0.01060458,0.040870406,-0.020856127,0.022241104,-0.006123772,-0.010387329,0.0023914617,0.034054145,-0.012722781,-0.02826983,0.010115765,-0.04051737,0.020733923,-0.0022675607,0.017787453,0.012362958,0.005804684,-0.018900866,0.008723998,-0.008126558,-0.02023153,-0.0024576555,-0.009817044,0.021195583,0.018602144,-0.01151432,-0.0143793225,0.014637308,0.037095662,0.0037441908,-0.009165291,-0.015031076,0.0047795293,0.011235967,0.003068675,0.0126956245,-0.010102186,-0.002447472,-0.0189416,0.023286626,0.04703491,0.009593003,-0.015737142,0.014908873,-0.013116549,-0.01659257,-0.09531902,-0.00431787,0.01773314,0.021182004,0.0074137016,0.043830454,-0.00019401986,0.012505529,-0.04228254,0.024209945,-0.043911923,-0.018860132,0.0057945,-0.004032728,-0.003061886,-0.011568633,0.0008936158,-0.025350515,-0.022417622,0.028622862,0.0033775792,-0.04901733,0.00029532603,-0.0100682415,0.018154064,-0.024372883,-0.02152146,0.010366961,-0.0010370356,-0.0008159654,0.018018281,-0.038969457,-0.023612503,-0.01606302,-0.010088609,0.02484812,-0.018479941,0.0040225442,0.0049730185,0.00076504715,0.022702763,-0.0010268519,-0.02205101,0.000100457524,-0.020951174,-0.011887721,-0.02363966,0.028812958,0.0022964145,-0.011120552,-0.027550183,-0.005197059,-0.009905303,-0.004599618,-0.022458356,0.008065456,0.009423276,0.029980684,0.0155877825,0.017230745,-0.013931241,0.011073029,0.0058895475,0.023245892,0.02196954,-0.016633304,-0.02370755,-0.0004667509,0.0072100284,-0.0058861533,-0.022431199,-0.000827422,-0.00020600687,-0.0052004536,-0.03348386,-0.007162505,-0.014080602,-0.0020995303,-0.0030024813,-0.0099731935,0.0091788685,-0.008852991,0.0065888255,-0.004433285,-0.023571769,0.019443994,0.014637308,-0.017760295,-0.026789805,-0.01765167,-0.012403693,0.021847336,0.013503527,-0.025649235,0.005634956,0.01773314,0.007922884,-0.007569851,0.0046098013,0.010156499,-0.008330231,-0.003486205,-0.045595624,0.02114127,-0.0012279792,-0.025133263,-0.008404911,-0.01879224,0.0117315715,-0.010930457,-0.0016192013,0.010170078,-0.012281489,0.0046912706,-0.03136566,0.018493518,-0.018276269,0.004969624,0.011690837,-0.020788236,0.007339021,0.0012839893,-0.011473586,0.012458006,0.010808254,0.023164423,-0.01076073,-0.03945827,-0.02840561,0.009219603,-0.009307861,0.008703631,0.021616507,-0.008133346,0.004664114,0.028134046,-0.009830622,-0.008296285,0.01310297,0.006459832,0.035086088,-0.02431857,-0.0059812004,-0.04016434,0.00031442038,-0.007861782,0.032180354,0.009294284,0.0025306386,0.00062459754,-0.017285058,-0.0019909048,0.021860914,0.0071353484,-0.0038765783,-0.033782583,-0.011779095,0.009172079,-0.011269912,-0.016497523,0.020014279,0.0019484728,0.029491868,-0.016375318,0.023354517,-0.008187659,0.0046946653,0.00083463546,0.00079559814,0.021073379,0.013815827,-0.02552703,-0.0005452499,-0.00795683,0.005947255,0.03712282,0.031501442,0.015709987,0.010502744,0.0024593528,-0.00719645,0.041087657,0.031501442,-0.0051121954,-0.026599709,-0.015438423,0.011690837,0.006195057,0.013639309,0.011011926,0.0025832541,0.0048134746,-0.010115765,0.003628776,-0.0024084346,0.005560276,0.0009156804,0.002730917,-0.0049594403,-0.0106724715,-0.0062561594,0.015872926,-0.0054686232,0.0010277006,-0.014026289,-0.021277051,-0.039594054,-0.0030822533,-0.027305776,-0.025893642,-0.024386462,0.005207243,-0.0018720954,-0.002663026,-0.0015996826,0.021222739,-0.010387329,0.025689969,-0.0151532795,-0.014705199,-0.032017414,0.018425629,0.009328228,-0.0038154763,0.009721997,-0.001162634,0.035900783,0.0048847604,0.01917243,-0.029220304,0.033538174,0.015601361,0.013089392,0.001570829,-0.00019635362,-0.011066239,-0.009660895,-0.029003052,-0.0032367052,0.004840631,-0.0041413535,0.06767379,-0.0017286756,0.009823834,0.03106694,0.008567849,0.026083738,0.02173871,0.0060966155,-0.0045351214,-0.017543044,0.013517106,-0.00742728,-0.02023153,-0.01697276,0.013191229,-0.00191283,0.0029413793,0.021535037,-0.016904868,-0.018697193,-0.0011566936,-0.0059031257,0.0015233052,0.011717994,0.010224391,0.00742728,0.014107758,0.006687267,-0.011690837,-0.03953974,0.0025119684,0.024047006,-0.04173941,-0.01076073,-0.005634956,0.017447997,-0.0074001234,-0.010611369,0.020095747,-0.009640528,0.0003818871,0.0015954394,-0.016619727,-0.017923234,-0.018126907,0.0067924983,0.00018235108,-0.013652888,-0.025133263]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"doc":null,"source":"async def answer(\n    context: str, question: str, answer_model: str = \"chatgpt\", **kwargs\n) -> str:\n    \n    prompt = F(\nf\"\"\"Background text: \"{context}\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"{question}\"\nAnswer: \"\"\"\n    ).strip()\n\n    prompt = prompt\n    answer = await recipe.agent(answer_model).complete(prompt=prompt, stop='\"')\n    return answer\n"}
{"context":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.","formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).","formatted":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details)."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The details of the hyper-parameters for our different models are given in Table 2 .","formatted":"The details of the hyper-parameters for our different models are given in Table 2 ."}]},"question":"How many parameters are used in the model?","answer_model":"chatgpt","top_n":3,"relevance_method":"vector","relevance_model":null}
[{"sentences":["We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) .","WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency.","It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling.","We set the attention length to 384 during training and 1600 during evaluation.","We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a).","As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture."],"sections":[{"title":"Main Results","number":"4.1"}],"section_type":"main"},{"sentences":["The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text.","We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled.","Consequently, this dataset mainly tests the ability of modeling only short-term dependency.","The comparison between Transformer-XL and the other methods is shown in Table 4 .","Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8.","Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences."],"sections":[{"title":"Main Results","number":"4.1"}],"section_type":"main"},{"sentences":["Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on.","As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do.","That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."],"sections":[{"title":"E Generated Text","number":null}],"section_type":"main"}]
0.770627365007215
0.7359686237098138
0.758352680308104
0.7636611680846485
0.7546686861623282
0.7600921420113453
0.7484448050478766
0.764718309420327
{"doc":"Generate an answer to a question given some context.","source":"    async def complete(\n        self,\n        *,\n        prompt: str,\n        stop: Stop = None,\n        verbose: bool = False,\n        default: str = \"\",\n        max_tokens: int = 256,\n    ) -> str:\n        \"\"\"Generate an answer to a question given some context.\"\"\"\n        if verbose:\n            self._print_markdown(prompt)\n        response = await self._complete(prompt, stop=stop, max_tokens=max_tokens)\n        completion = self._extract_completion(response)\n        if verbose:\n            self._print_markdown(completion)\n        return completion\n"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.","formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).","formatted":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details)."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The details of the hyper-parameters for our different models are given in Table 2 .","formatted":"The details of the hyper-parameters for our different models are given in Table 2 ."}]},"formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.\n\nOur models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).\n\nThe details of the hyper-parameters for our different models are given in Table 2 ."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"doc":"Send a completion request to the OpenAI API with the given prompt and parameters.","source":"    async def _complete(self, prompt, **kwargs) -> dict:\n        \"\"\"Send a completion request to the OpenAI API with the given prompt and parameters.\"\"\"\n        kwargs.update(\n            {\n                \"model\": self.model,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"n\": 1,\n            }\n        )\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = await openai_chatcomplete(messages, **kwargs)\n        if \"choices\" not in response:\n            raise ValueError(f\"No choices in response: {response}\")\n        return response\n"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.","formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).","formatted":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details)."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The details of the hyper-parameters for our different models are given in Table 2 .","formatted":"The details of the hyper-parameters for our different models are given in Table 2 ."}]},"formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.\n\nOur models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).\n\nThe details of the hyper-parameters for our different models are given in Table 2 ."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"context":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.","formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.","formatted":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set.","formatted":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."}]},"question":"How many parameters are used in the model?","answer_model":"chatgpt","top_n":3,"relevance_method":"vector","relevance_model":null}
{"doc":"Send a completion request to the OpenAI API and return the JSON response.","source":"@trace\nasync def openai_chatcomplete(\n    messages: list[dict[str, str]],\n    stop: Optional[str] = \"\\n\",\n    top_p: float = 1,\n    temperature: float = 0,\n    model: str = \"gpt-3.5-turbo\",\n    max_tokens: int = 256,\n    logit_bias: Optional[Mapping[str, Union[int, float]]] = None,\n    n: int = 1,\n    cache_id: int = 0,  # for repeated non-deterministic sampling using caching\n) -> dict:\n    \"\"\"Send a completion request to the OpenAI API and return the JSON response.\"\"\"\n    params = {\n        \"messages\": messages,\n        \"stop\": stop,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"n\": n,\n    }\n    if logit_bias:\n        params[\"logit_bias\"] = logit_bias  # type: ignore[assignment]\n    response = await _post(\"chat/completions\", json=params, cache_id=cache_id)\n    if isinstance(response, TooLongRequestError):\n        raise response\n    add_fields(total_tokens=extract_total_tokens(response))\n    return response\n"}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.","formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).","formatted":"Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details)."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The details of the hyper-parameters for our different models are given in Table 2 .","formatted":"The details of the hyper-parameters for our different models are given in Table 2 ."}]},"formatted":"As show in Table 8 , for a similar number of parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code. LLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and MBPP. LLaMA 65B also outperforms PaLM 62B, even when it is trained longer. The pass@1 results reported in this table were obtained by sampling with temperature 0.1. The pass@100 and pass@80 metrics were obtained with temperature 0.8. We use the same method as Chen et al. ( 2021) to obtain unbiased estimates of the pass@k.\n\nOur models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017) , with the following hyper-parameters: \u03b2 1 = 0.9, \u03b2 2 = 0.95. We use a cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We use 2, 000 warmup 0 200 400 600 800 1000 1200 1400 Billion of tokens steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).\n\nThe details of the hyper-parameters for our different models are given in Table 2 ."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.","formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.","formatted":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set.","formatted":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."}]},"formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.\n\nThe dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.\n\nDespite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.","formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.","formatted":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set.","formatted":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."}]},"formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.\n\nThe dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.\n\nDespite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.","formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.","formatted":"The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set.","formatted":"Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."}]},"formatted":"We apply Transformer-XL to a variety of datasets on both word-level and character-level language (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012) . WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018;Grave et al., 2016a). As shown in Table 1 , Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.\n\nThe dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4 . Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences.\n\nDespite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
0.7250826271031067
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0061607007,0.020718962,0.010916151,0.0025983562,-0.0063168397,0.022334661,0.0021927343,-0.0024133655,-0.015899021,-0.04046036,0.0059197037,0.024588494,-0.0038932913,-0.0025491384,-0.010651394,-0.013176772,0.012871282,0.015328774,0.0059400694,0.009049272,-0.0023946967,0.011262372,-0.0032534609,-0.007392841,-0.014296899,0.03929271,0.029924374,-0.015722515,0.015138691,-0.022226043,0.015790403,-0.004212358,-0.01615699,-0.028620953,-0.014717795,0.028349407,0.0108007435,0.0022657122,0.027453305,0.016767967,0.016863009,0.023162877,-0.004494087,0.004548396,0.006140335,0.016224876,0.015369506,-0.014975764,-0.023420846,0.009734926,0.0012915408,0.019089686,-0.02164222,-0.00425309,-0.022253199,0.0016445507,-0.0024201542,0.004694352,0.01001326,-0.029679982,0.018125698,0.009734926,-0.018546594,0.017976347,-0.026611513,-0.0041444716,0.006496739,0.00089101045,-0.0059536467,0.014663486,0.01409324,0.02299995,-0.008716628,-0.017555451,0.017120978,-0.023502309,-0.008662319,-0.012165262,-0.007942721,0.017188864,0.009422648,-0.010508832,-0.011459243,0.016700082,-0.0042089634,-0.021900188,0.020746117,0.017731957,-0.03432342,0.009476957,0.009334395,0.026394274,0.017107401,-0.006130152,0.0062727137,-0.005817874,-0.006079237,0.026285656,0.0151251145,0.0077186963,-0.010950094,-0.0032364894,-0.059197035,-0.017989924,-0.011140176,-0.007542191,0.011235218,0.004317582,0.00036764788,0.00039034744,-0.032829914,0.033888947,-0.0068056225,-0.033834636,-0.0072978,-0.0010123575,0.03492082,-0.007250279,-0.0038423764,-0.012491118,0.0212349,0.008411138,0.03133641,-0.02500939,0.02465638,-0.0072638565,-0.017338214,-0.0068361717,-0.023461578,-0.022687672,0.024466299,0.024629226,0.018804563,0.018397244,-0.024045402,0.033400163,-0.013278602,0.017093822,-0.023135722,-0.008798092,0.011914083,0.020691808,0.004463538,0.023475155,0.005719438,0.01071928,0.013916735,0.027561923,0.0030531958,-0.010006472,-0.012823761,-0.014826413,0.0291912,0.02938128,0.0042700614,0.011798675,-0.0054614698,-0.00039968183,-0.012803395,-0.0031346595,-0.009144313,0.024792153,-0.008485814,-0.00925972,0.022117425,0.013556936,0.024058979,-0.012144896,0.008451871,-0.0010191462,-0.02500939,0.03817937,-9.990719e-06,-0.008730205,0.0006911695,0.013156406,0.023420846,-0.027996397,-0.011988757,-0.007820526,-0.026937367,0.014609177,-0.009334395,0.003097322,-0.030521775,0.022986373,0.0042293295,-0.029734291,-0.0030786532,-0.005967224,0.016469266,0.016021216,-0.00044932382,-0.0007700876,-0.66822046,-0.02143856,0.007589712,-0.019401964,0.00704662,0.016129835,0.011208063,-0.0066596665,-0.03809791,0.033182926,-0.021560755,-0.01524731,0.0035945906,-0.029517055,-0.019239036,-0.039971575,0.00046077967,-0.02571541,-0.012246726,0.005475047,-0.011628959,0.0014663487,-0.0064526126,0.0168087,-0.0010454522,0.007820526,0.0043243705,-0.024914348,0.021805147,-0.0074743046,0.0059570414,0.023936784,0.005315514,0.00013874305,0.023040682,-0.037581973,-0.026122728,0.010169399,-0.0025813845,0.032531217,-0.0253624,-0.01746041,0.008865979,0.0047486615,-0.021696528,0.017351791,0.019497005,0.0062421644,0.010407002,0.010298383,-0.005773748,0.014812836,0.013027421,-0.011187697,0.018044233,-0.023882475,0.024316948,-0.013353276,0.011737578,-0.019266192,0.016374227,-0.0028121986,-0.029354127,-0.0020077436,0.0028172901,0.013903157,-0.0069719446,0.0016997085,-0.005339274,-0.03120064,0.022497589,0.0113031035,-0.00018042962,0.0129120145,0.016509999,-0.001193954,0.0418724,0.011690057,0.018695945,0.0038423764,0.016727235,-0.015288042,-0.01917115,0.0021027846,0.008445082,0.0012109256,0.008730205,-0.0020518696,0.003944206,-0.006035111,0.027358264,0.02818648,-0.0027595866,-0.025172317,-0.011900505,0.017392524,0.0030226468,0.009524478,-0.00079554506,-0.04382753,-0.004419412,0.0047418727,0.03133641,0.022212466,0.020895468,0.007983454,-0.017541874,-0.0030820474,0.016523577,-0.02500939,0.01701236,-0.0054580756,-0.0153151965,0.0035911964,-0.0052951477,-0.024289792,0.014541291,0.017881306,0.008716628,-0.0043786797,0.008981385,-0.005417343,0.01619772,-0.008988174,0.0008201539,0.006683427,-0.015478124,-0.013563724,0.008479025,-0.010644604,0.014989342,0.0063541774,0.01987717,-0.025525328,-0.0030820474,0.012423231,0.008275365,-0.007935933,0.01701236,-0.0015172635,-0.027317531,0.015450969,-0.0010004775,-0.020406684,-0.021560755,-0.013210715,0.006279502,0.009836755,0.008343252,-0.0068361717,0.0060656597,-0.02319003,-0.005773748,-0.00028363834,0.0011684966,0.003241581,-0.004921772,-0.033644553,0.0067547075,-0.02686948,0.011988757,0.03861385,-0.023610927,-0.019836439,-0.0041648373,-0.02460207,-0.007854469,0.034649275,-0.00415126,-0.013353276,0.007935933,-0.01604837,0.007386052,0.01600764,-0.01801708,0.0032568553,-0.009877487,-0.018981067,-0.0054988074,-0.021343518,0.012402865,0.0030684702,-0.015002918,0.0013772475,0.03538245,0.021004086,0.006265925,0.02118059,-0.028485179,0.027113872,0.00015200213,0.013719863,-0.02138425,0.03440488,0.020284489,0.013930311,0.031417876,-0.01328539,-0.02078685,0.017433256,0.027534768,0.025783297,0.022511167,-0.010963671,-0.00232681,-0.0109908255,0.0039815437,-0.010176187,0.021479292,0.020447416,0.027236067,0.011269161,-0.006781862,-0.012348556,0.0059943786,0.0025219838,-0.008383984,0.02223962,-0.011126599,0.0029242113,0.009232565,-0.0046977466,0.044832252,0.0097281365,-0.012056644,-0.012097376,-0.0017633521,0.024425566,0.03440488,-0.0023081412,-0.0035911964,-0.016985204,-0.01288486,0.012321401,0.004096951,0.0076372325,0.00027175818,-0.00182445,0.033888947,-0.008234633,0.015355929,0.013577302,-0.00037698227,-0.014867146,0.022008806,-0.00038780167,0.029544208,0.026475739,0.0042768503,0.0028308674,0.007487882,-0.00865553,-0.0030005837,0.02108555,-0.0021384251,-0.02073254,0.011608593,0.009476957,0.0053867945,0.030168764,0.029598517,0.014649909,0.017053092,-0.0054818355,0.03266699,-0.022864176,-0.028756725,-0.0006644392,-0.015736094,0.001793901,0.003087139,0.00860122,0.0074132066,-0.023135722,0.0114931865,0.002815593,0.034594964,0.014133971,0.0095109,0.026326388,-0.0256611,-0.017786266,0.0096941935,0.008648741,-0.012572582,-0.0050880937,-0.011459243,0.010522409,-0.020135138,0.03209674,0.0037269695,0.03386179,-0.017297482,-0.008071706,-0.009578787,0.0009970831,0.025756141,-0.01796277,-0.009952162,-0.008519757,-0.0038220105,0.0066291178,-0.013020633,-0.013679132,0.008214268,-0.0017955982,-0.0065612313,-0.03693026,0.018953914,-0.010271229,0.026190616,-0.02339369,0.0017514719,0.000745903,0.025185894,0.022022383,-0.005329091,0.020189447,0.028322252,0.020365953,0.00043998944,-0.027385417,-0.009022117,0.0038865027,0.007392841,0.026896635,0.0049387435,0.022131002,-0.010474889,-0.008112438,0.00034834264,0.002211403,0.019401964,0.0096602505,-0.014704218,-0.009565209,-0.0069176354,0.003492761,0.01595333,-0.010006472,0.0056515518,-0.038505226,-0.034486346,-0.019469852,-0.021873033,0.013862425,0.01716171,0.024289792,0.001512172,-0.0019992576,0.0022368606,0.009456591,0.00352331,0.003024344,0.01887245,0.02052888,0.022361817,0.0054920185,-0.013957466,0.019198304,0.014500558,-0.0039068684,-0.014527713,0.023760278,-0.0038321936,-0.01318356,-0.00183633,0.013108885,-0.011710423,-0.01967351,0.0028817824,-0.011404933,0.043393057,0.0058009024,0.009483745,0.0010081147,-0.030576084,-0.025226627,-0.004952321,-0.013027421,-0.013217503,-0.042496953,-0.015627474,-0.020148717,-0.021764414,-0.0184244,0.038966857,-0.006374543,0.017772688,0.003243278,-0.024588494,0.005159375,-0.032694142,-0.012871282,-0.0036828432,-0.027684119,-0.046189982,-0.016740814,0.0050745164,0.026706552,-0.0034757892,-0.013115673,-0.0042191464,0.015668206,-0.00734532,-0.0074810935,0.016021216,-0.011065501,-0.0070669856,0.019388387,-0.032341134,0.00076923904,0.0066189347,0.027005253,0.005661735,0.026788017,-7.165209e-05,-0.04165516,0.007175604,-0.0072706454,-0.022714825,0.020854736,-0.00450427,-0.019564891,0.0073181656,-0.010943306,-0.0029343944,-0.011961603,-0.021411406,0.0008218511,0.0076711755,-0.015708938,0.012314613,-0.01741968,-0.004921772,-0.011398145,0.0124096535,-0.011214851,0.021669373,0.0069583673,0.01822074,0.011452454,-0.0010556353,0.00044253518,0.002835959,-0.015966907,0.00510846,0.010176187,-0.022538321,0.0050439676,-0.002230072,0.0017234688,-0.014500558,0.015070805,-0.024683535,0.032476906,-0.014568445,0.00019750731,-0.02097693,-0.005118643,-0.018560171,0.008899922,-0.0028987539,-0.00644243,0.0009809601,0.010339116,-0.018560171,0.0071280836,-0.007827315,-0.015179424,-0.017433256,0.02335296,0.009130736,0.028213633,-0.009354761,0.012239938,-0.024181174,-0.031770885,-0.02279629,-0.017148132,-0.0051491917,-0.0030786532,0.0019653144,0.026557203,0.035735458,0.00010283745,0.034214802,0.009972529,-0.015980484,0.017501142,-0.007942721,0.008173536,-0.022402547,0.001574967,0.013930311,0.0025779903,0.02813217,0.0045721564,0.015546011,-3.3197432e-06,-0.018193584,-0.010108301,-0.014785682,-0.012735509,-0.0019262797,0.008764149,-0.009619518,-0.0025457442,-0.022103848,0.0055632996,-0.0184244,-0.017175287,0.00080530375,0.004263273,0.006371149,-0.0035300986,0.008417927,0.002910634,0.0038661368,-0.025226627,-0.03644148,-0.017338214,-0.010257652,0.00475545,-0.0075693456,0.024045402,0.016387803,0.012633679,-0.014867146,-0.014011775,-0.027765583,-0.013971044,0.004870857,-0.010827898,-0.0024337315,-0.02143856,-0.026896635,0.015383083,0.012613313,0.0047384785,-0.025538905,0.0041173166,0.0039713606,-0.017731957,0.009592364,0.014324053,0.024113288,0.01243002,0.02219889,0.019198304,-0.025430286,-0.00071280834,0.0022368606,0.0073181656,0.00055836653,0.02818648,0.022171734,-0.010447734,-0.020814003,-0.004059613,0.014758527,-0.016713658,-0.017949194,0.03133641,0.011852984,-0.02138425,-0.0065103164,-0.015288042,-0.026706552,-0.008682685,-0.00765081,0.0012228057,-0.0050337845,-0.0032704326,0.0048742513,-0.00860122,0.00860122,0.021615064,-0.002357359,-0.011866562,0.012559004,0.009823178,-0.006880298,-0.002179157,0.012124531,0.0228506,0.024384834,0.020501725,-0.017935615,-0.005278176,-0.0027205518,0.007623655,-0.014772104,0.028349407,-0.019198304,0.012898437,0.0058552115,-0.0035402814,0.0051050656,-0.009843544,-0.015478124,-0.008180324,-0.0078069484,0.006425458,0.03174373,-0.0021961285,-0.023027103,-0.0068633263,-0.007739062,-0.024303371,-0.014785682,0.0054886243,-0.011547496,-0.037907828,-0.0029004512,-0.016482845,0.011079079,0.023488732,0.0044262004,-0.010671759,0.013509415,0.015084382,-0.0272768,0.0020060462,0.00799703,-0.0007917264,-0.014527713,-0.0006194644,-0.024778577,-0.022375394,0.011628959,0.014120394,-0.054173436,-0.01308173,0.02576972,0.0041444716,0.016279185,0.0004037126,-0.008356829,0.01388958,-0.00040265187,0.009449802,0.0098503325,0.030087302,0.0064729787,0.008288943,0.00073572004,0.00354707,0.0057533816,-0.030711858,-0.0014722886,0.0042598783,-0.030358847,0.004931955,-0.007019465,0.008159959,-0.011907293,-0.0011693451,-0.02843087,-0.012789818,-0.012090587,0.048742514,0.022891331,-0.029326972,-0.029082581,-0.003879714,0.0039476007,0.015817557,-0.020420263,-0.034540657,0.0018074783,-0.017351791,-0.029408436,-0.022877753,-0.013943889,0.037310425,0.010495255,-0.0076779644,-0.025851183,-0.014839991,-0.013943889,-0.013631611,-0.023543041,-0.0047520557,-0.0096602505,0.023380114,-0.015831133,0.018953914,-0.008044551,0.0017277116,-0.021818724,0.008696262,-0.0057296213,-0.019157574,0.02560679,0.0015927872,-0.033888947,0.01851944,0.006225193,-0.016591463,-0.0028834795,0.00059443124,-0.020556035,-0.015288042,-0.004392257,-0.00029085125,0.013224292,0.02943559,-0.0025898702,-0.013903157,-0.012212783,0.025199473,-0.015763247,0.0055632996,-0.021710105,0.013319333,-0.007664387,-0.015450969,-0.026339965,-0.012239938,-0.003998515,0.0010123575,0.0033620794,0.012307825,-0.007644021,0.011859774,0.016170567,0.0036522942,0.004287033,0.0250637,0.0037507296,0.016265608,0.015342351,-0.01861448,-0.019999366,0.001897428,-0.022932064,0.0008524,-0.010563141,-0.0075965,-0.011248794,-0.021112705,0.008540123,0.019279769,-0.03019592,0.023515888,0.0040120925,-0.00073190144,0.018437976,0.012090587,0.0039577833,-0.011242006,-0.017949194,-0.010101513,-0.018790986,-0.01645569,0.04477794,0.015043651,0.017935615,0.0066155405,0.22038676,-0.017351791,0.0024999208,0.02818648,0.021357097,0.0010878813,0.012959534,0.027697695,-0.027086718,0.003577619,0.009897853,-0.013156406,-0.005013419,0.0037099977,0.00704662,0.0032874043,-0.036957417,-0.03739189,-0.030576084,0.028539488,0.012233149,0.01403893,0.01353657,-0.026543625,0.051593747,-0.0013840363,-0.011764732,-0.0006559534,-0.0014638029,-0.0043447367,-0.006578203,-0.009151101,0.029652826,0.03693026,-0.010535986,-0.017677648,-0.003470698,0.0049251663,0.018845296,-0.0032585524,-0.012518273,0.006978733,0.0046264655,-0.011391357,-0.009829966,-0.0036455055,-0.009687405,-0.010155822,0.0030158581,0.022361817,-0.016306339,-0.010244074,0.017894885,0.027032409,-0.0014714401,-0.015926175,0.005475047,-0.014636331,0.004891223,-0.006778468,-0.024384834,0.0073045883,-0.023882475,0.021343518,-0.032612678,0.0021146648,-0.032938533,-0.005067728,0.023570197,-0.019741397,-0.032042433,0.0033518963,-0.014296899,-0.0031635114,-0.027494036,-0.009707771,0.027969241,0.0209362,0.015790403,0.0134822605,-0.006948184,-0.0054920185,-0.011656114,-0.018682368,-0.0024693718,-0.007467516,0.015152269,-0.019551314,0.012674412,0.0026306023,0.0041274996,0.0016487936,-0.01796277,0.01338722,-0.01071928,0.01861448,0.0013271812,0.02223962,0.013597668,-0.025525328,-0.009239354,0.06120648,0.0077254847,0.00694479,0.0019093081,0.0058993376,-0.0071145063,0.016075525,0.020067252,-0.019103264,0.015342351,-0.05018171,0.0026781228,-0.0032076375,-0.012443597,-0.0061097858,-0.005067728,-0.0019839832,0.0022334661,-0.019401964,-0.0028410505,-0.024384834,-0.021221323,0.005339274,0.01137099,-0.019687088,-0.025430286,-0.001980589,-0.00860122,-0.0028817824,-0.014201858,-0.011153754,0.0022674093,0.01665935,-0.012389288,-0.025443863,-0.011499975,-0.026312811,0.006265925,0.009456591,0.0025084065,-0.010386636,0.0038186163,-0.022809867,0.0073792636,-0.013842059,0.016075525,0.012002335,0.004745267,-0.01604837,0.0013110582,0.0034096,-0.01358409,-0.01604837,0.021900188,-0.0060554766,-0.023719547,-0.015288042,-0.00561082,-0.003346805,-0.017053092,-0.027208913,0.020107985,0.0131360395,-0.02837656,-0.008485814,-0.17509289,0.011561072,0.019700665,-0.05178383,0.014120394,0.027480459,0.018845296,0.015858289,-0.034594964,-0.011506763,0.021479292,-0.022334661,-0.0019890747,-0.011914083,-0.00488104,-0.011208063,-0.013801327,0.0142425895,0.013998198,0.0145956,0.052707087,-0.01530162,-0.007929144,0.012002335,-0.017569028,-0.01881814,0.011520341,0.03326439,0.013346488,-0.051566593,-0.017229596,-0.012871282,0.03190666,0.02023018,-0.011805464,0.004568762,0.018723099,-0.026801594,-0.022579053,0.0077186963,-0.0051865294,0.013957466,-0.00890671,0.0054784412,-0.016224876,0.004100345,0.03644148,0.0019262797,0.0011566164,-0.011255584,0.014337631,-0.005682101,0.007250279,-0.0058484226,0.0020281095,0.009517688,0.017446833,-0.0061471234,-0.0057465932,-0.020800427,-0.020325221,-0.012538638,0.014120394,-0.0038593481,0.014622754,0.014229013,-0.014296899,0.020420263,-0.037174653,0.018492285,0.015030073,-0.018098544,-0.0040188814,-0.0019076109,5.627155e-05,0.026489316,0.0054784412,0.009578787,0.028811034,0.00020355346,-0.010671759,0.040894832,-0.020814003,0.022253199,-0.006075843,-0.010379847,0.0024676747,0.034051873,-0.012606525,-0.028295098,0.010135456,-0.040541824,0.020678231,-0.002231769,0.017786266,0.012402865,0.005780536,-0.018845296,0.008750571,-0.008132804,-0.020311644,-0.002533864,-0.009802812,0.021207746,0.018600903,-0.01157465,-0.014405517,0.014649909,0.037174653,0.003743941,-0.009123947,-0.015002918,0.0047384785,0.011214851,0.003139751,0.012749086,-0.010074358,-0.0024965263,-0.018913181,0.023339381,0.047004618,0.009667039,-0.015722515,0.014853568,-0.0131360395,-0.016537154,-0.095475584,-0.004348131,0.017731957,0.021207746,0.0073588975,0.04388184,-0.00013206048,0.012525061,-0.042279717,0.024262639,-0.043854684,-0.018886028,0.005777142,-0.00404943,-0.0030413156,-0.011540706,0.0009410767,-0.02541671,-0.022402547,0.028539488,0.003389234,-0.049041215,0.000293397,-0.010033626,0.018180007,-0.024384834,-0.021574333,0.010393425,-0.0010199948,-0.0007968179,0.017989924,-0.038912546,-0.023610927,-0.016102681,-0.010128668,0.024792153,-0.018451553,0.0040799794,0.004952321,0.0007556617,0.02274198,-0.0010250863,-0.022076692,0.00015539647,-0.0209362,-0.011886928,-0.023678815,0.028811034,0.002253832,-0.011092655,-0.027643386,-0.0051967124,-0.009877487,-0.004578945,-0.022470435,0.008058129,0.009422648,0.029978683,0.015464547,0.01721602,-0.013984621,0.01111981,0.0059197037,0.02329865,0.022008806,-0.016632196,-0.023705969,-0.0004832671,0.0072706454,-0.0058518173,-0.022361817,-0.0007968179,-0.00019920447,-0.0052442327,-0.03345447,-0.0071077175,-0.014106817,-0.00207563,-0.002993795,-0.009918219,0.009225776,-0.008899922,0.0065646255,-0.00450427,-0.023665236,0.019456273,0.014568445,-0.017772688,-0.02681517,-0.017677648,-0.012402865,0.021900188,0.013509415,-0.0256611,0.005570088,0.017731957,0.007915567,-0.0075150365,0.004568762,0.010176187,-0.008282154,-0.0035334928,-0.045619734,0.021126281,-0.0012024398,-0.025104431,-0.008417927,-0.018777408,0.011710423,-0.01082111,-0.0016038187,0.010155822,-0.012307825,0.0046671974,-0.03130926,0.018451553,-0.018275049,0.005023602,0.0116832685,-0.020841159,0.007352109,0.0012898437,-0.0114931865,0.012497907,0.010834686,0.023162877,-0.010692125,-0.03940133,-0.028403716,0.009130736,-0.00930724,0.008716628,0.021615064,-0.008166747,0.0046909577,0.028105015,-0.0098163895,-0.008336463,0.013129251,0.006517105,0.035138056,-0.024344102,-0.005933281,-0.040161658,0.00027727397,-0.0078612575,0.03209674,0.009300452,0.0025406526,0.0006551048,-0.01731106,-0.0020213209,0.021859456,0.00709414,-0.0038865027,-0.033807483,-0.011703635,0.009198622,-0.011201274,-0.016509999,0.020012943,0.0020179264,0.029462745,-0.016279185,0.023339381,-0.008187113,0.004714718,0.0008265182,0.00084051985,0.02108555,0.013733441,-0.025525328,-0.00050363305,-0.007922356,0.0059502525,0.0371475,0.031472187,0.01574967,0.010461311,0.002511801,-0.0072231246,0.04105776,0.03149934,-0.005098277,-0.026611513,-0.015410238,0.011737578,0.006181067,0.013665554,0.011024769,0.002576293,0.004819942,-0.010101513,0.0036421113,-0.00244052,0.0055632996,0.00092495367,0.0027137632,-0.0050507565,-0.010603873,-0.006310051,0.015844712,-0.0054920185,0.0010488465,-0.013957466,-0.021302788,-0.03950995,-0.0030820474,-0.027290376,-0.025878338,-0.02445272,0.00513222,-0.0018261471,-0.0026628484,-0.0016513393,0.021207746,-0.010400213,0.025701832,-0.015138691,-0.014677064,-0.032042433,0.0184244,0.0093140295,-0.0038084332,0.0096941935,-0.0011175816,0.035844076,0.0048742513,0.019116841,-0.029218353,0.033535935,0.015641052,0.013013843,0.0015537525,-0.00018987009,-0.011024769,-0.009680617,-0.029028272,-0.0032924956,0.00488104,-0.004147866,0.06772358,-0.0017446832,0.009775657,0.031064866,0.0085876435,0.026081996,0.021710105,0.0060928143,-0.0045619733,-0.017582607,0.013495838,-0.0074199955,-0.020216603,-0.016903741,0.013156406,-0.001960223,0.002961549,0.021492869,-0.016903741,-0.018695945,-0.0010547866,-0.005902732,0.0014807745,0.011764732,0.010291595,0.0074199955,0.0141747035,0.006639301,-0.011710423,-0.03945564,0.0024880406,0.024072556,-0.041790936,-0.010671759,-0.0056345803,0.017514719,-0.007392841,-0.0106106615,0.020135138,-0.009626307,0.00032394595,0.0016004244,-0.016577886,-0.017989924,-0.018125698,0.006832777,0.0001680191,-0.0136384,-0.025226627]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7791587922455706
0.7538141237119371
0.7394146288586467
0.6799359307778722
0.7488522442776079
0.7291512642122489
0.7320086639967716
0.7284631338073372
0.7452310987284336
0.7668202812728778
0.7595981400055435
0.7570533935508742
0.7589938032412494
0.7620751618201015
0.7385784694444447
0.7454334900889019
0.7645515655688967
0.7312528896022013
0.7508335375665866
0.7633333035031551
0.7184867664161794
0.7710034334754823
0.7536405256843443
0.7228457644301108
0.7476567809573532
0.7502994909719745
0.7537018569803503
0.729693256478108
0.7465730281787335
0.7663435547582123
0.7541447684425537
0.746730434439162
0.7473654801524557
0.7434337420110129
0.7311028159277815
0.7295897438279433
0.7284402039982013
0.7251227572809689
0.7490684491755966
0.7381290704817204
0.7378907033521741
0.7397738473039438
0.7405630002268324
0.7298510149668076
0.7453617095016835
0.7731245297020662
0.7454107394428767
0.7264204848935804
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006081489,0.020713056,0.010947359,0.0025670605,-0.006370113,0.022329353,0.0022088266,-0.0024651932,-0.01590491,-0.040421017,0.00594227,0.024611184,-0.0039354805,-0.002548385,-0.0106417565,-0.013113741,0.012869259,0.015361616,0.0059694345,0.009052624,-0.002393886,0.011273335,-0.0032325948,-0.0073887883,-0.014315777,0.0392801,0.029908294,-0.015687592,0.0151443,-0.022234276,0.015769087,-0.004217314,-0.01613581,-0.02865872,-0.014723247,0.028387073,0.010763997,0.0022597604,0.027477056,0.01677418,0.016896421,0.023157876,-0.0044957516,0.004584037,0.006146005,0.016230887,0.015388781,-0.0149405645,-0.02341594,0.0097589055,0.0012512723,0.019123923,-0.0216774,-0.004210523,-0.022329353,0.0016629867,-0.0024550064,0.0046926956,0.010016969,-0.029690977,0.01811883,0.009731741,-0.018539883,0.017996589,-0.026607787,-0.004156193,0.0065093325,0.0008896427,-0.005999995,0.0147096645,0.01409846,0.023049217,-0.008706274,-0.01753479,0.017100155,-0.02353818,-0.008672318,-0.012142604,-0.007993202,0.01718165,0.00943293,-0.010519516,-0.011470279,0.01666552,-0.0042207097,-0.0219083,0.02079455,0.017738525,-0.034336135,0.009500841,0.009337853,0.026417635,0.017100155,-0.0061324225,0.0063022017,-0.005820029,-0.006023764,0.026295394,0.01508997,0.007707973,-0.01095415,-0.0032105236,-0.059273295,-0.018023754,-0.01113072,-0.0075246114,0.011144303,0.004292017,0.00036226623,0.00037499968,-0.032814912,0.033928663,-0.0068420996,-0.033901498,-0.0072665475,-0.001033955,0.034906592,-0.0072258003,-0.0038200307,-0.01246858,0.021242766,0.008448211,0.03129369,-0.025005072,0.024665514,-0.007246174,-0.01735822,-0.006855682,-0.023388775,-0.022696076,0.02440745,0.024665514,0.01881153,0.018390477,-0.024040727,0.033412535,-0.013310685,0.017086573,-0.023198621,-0.008821724,0.011932078,0.020713056,0.0044889604,0.023524597,0.0057147657,0.010682504,0.013901516,0.02755855,0.0030916783,-0.009976222,-0.012835303,-0.014859071,0.029174848,0.029365001,0.004292017,0.011782672,-0.0054906574,-0.00034380276,-0.012794556,-0.0031188428,-0.009106954,0.024733426,-0.008516122,-0.009290314,0.022044124,0.013534794,0.024067892,-0.012162978,0.008488957,-0.0009617988,-0.02504582,0.038193516,1.4364907e-05,-0.008787769,0.000675721,0.013120532,0.023456687,-0.028006768,-0.011959243,-0.0077690934,-0.026960928,0.014614589,-0.009351435,0.0030814914,-0.030560246,0.02295414,0.0042139185,-0.029745305,-0.003027162,-0.0059592477,0.01648895,0.016054315,-0.00042996567,-0.00079074636,-0.66803336,-0.021405755,0.0076061054,-0.019368405,0.0070288563,0.016162975,0.011212215,-0.0067266496,-0.038112022,0.03327671,-0.021527994,-0.015225793,0.0035857356,-0.029582318,-0.01923258,-0.039986383,0.00048132386,-0.025738519,-0.012210516,0.005422746,-0.011606102,0.0014422738,-0.0064651896,0.01677418,-0.0010458395,0.007870961,0.004319181,-0.024964325,0.021826807,-0.0074227443,0.005901523,0.023904903,0.00535823,0.00015672737,0.023022052,-0.03762306,-0.02615957,0.010146001,-0.0026027143,0.032516103,-0.025317466,-0.017466879,0.008848889,0.0046689264,-0.021758895,0.017331054,0.019490646,0.0062139165,0.01042444,0.010288616,-0.005772491,0.014859071,0.013052621,-0.011178259,0.0180645,-0.02383699,0.024325956,-0.013365014,0.011721552,-0.01923258,0.016366709,-0.002784378,-0.029365001,-0.0019728336,0.002842103,0.013935472,-0.0069881096,0.0017241072,-0.0053208782,-0.031266525,0.022573834,0.011280126,-0.00021689285,0.0129711265,0.016502533,-0.0011629871,0.04194224,0.011667223,0.018675705,0.003949063,0.016747015,-0.015293705,-0.019137505,0.0020882834,0.008400672,0.0012308988,0.008760604,-0.002050932,0.0039422717,-0.006064511,0.027354816,0.02819692,-0.0026994883,-0.025140895,-0.011993199,0.017398966,0.0030050909,0.0095619615,-0.0007338704,-0.043843765,-0.004370115,0.0047640027,0.03134802,0.02226144,0.020889625,0.008020367,-0.0176027,-0.0030509313,0.01654328,-0.02499149,0.016991496,-0.005375208,-0.0153344525,0.0036536471,-0.0052699447,-0.024339538,0.014533095,0.017860767,0.008699483,-0.004346346,0.008998294,-0.005385394,0.016176557,-0.009005086,0.00083064445,0.0066112,-0.015415946,-0.01357554,0.008482166,-0.010580637,0.014994894,0.0063633225,0.01980304,-0.025534783,-0.0030747002,0.012461789,0.008251267,-0.007891335,0.016964331,-0.0015254656,-0.027341234,0.015429528,-0.0009949058,-0.02044141,-0.021595906,-0.013215609,0.006244477,0.009860773,0.008312386,-0.0068251216,0.0059864125,-0.023212204,-0.005738535,-0.0003049658,0.0012003386,0.0033072976,-0.004933782,-0.033684183,0.006814935,-0.026879434,0.0120203635,0.03857382,-0.023578927,-0.019830205,-0.0041799624,-0.024597602,-0.007877752,0.03460778,-0.0041426113,-0.013365014,0.007877752,-0.016013568,0.007381997,0.016013568,-0.017955841,0.0032461772,-0.00984719,-0.019001681,-0.005521218,-0.02132426,0.012400669,0.0030780958,-0.01503564,0.0013115439,0.03542272,0.021039031,0.0062478725,0.021174856,-0.028577225,0.02720541,0.00015640903,0.013718154,-0.021392172,0.03449912,0.020319168,0.013901516,0.03134802,-0.013276729,-0.020767385,0.01748046,0.027626462,0.025820011,0.02247876,-0.010933776,-0.0022886228,-0.010994897,0.003996601,-0.010173166,0.021405755,0.020427827,0.027218992,0.011320873,-0.00675721,-0.012353131,0.0060271597,0.0024957533,-0.0084142545,0.02226144,-0.011157885,0.0029286903,0.009195238,-0.004716465,0.044821694,0.009731741,-0.012095066,-0.012095066,-0.0018370104,0.024434615,0.03441763,-0.0023208808,-0.0035857356,-0.01701866,-0.012882842,0.012298801,0.0041256333,0.007660435,0.00030093352,-0.0018183346,0.033901498,-0.008196937,0.015361616,0.013555166,-0.00035335284,-0.014831906,0.022084871,-0.00040746995,0.029527988,0.026376888,0.0042614564,0.0028692675,0.0075042383,-0.008672318,-0.0030288599,0.021052614,-0.0021986398,-0.020672308,0.011646849,0.0095280055,0.0053786035,0.030207105,0.029609483,0.014601006,0.017100155,-0.005511031,0.032651924,-0.022859063,-0.028767377,-0.0006528008,-0.015755504,0.0018896419,0.003084887,0.008672318,0.0073684147,-0.0230628,0.0115314,0.0028285205,0.034634944,0.014112042,0.009473676,0.026376888,-0.025643442,-0.017779272,0.0097249495,0.008638362,-0.0125772385,-0.005015276,-0.0114295315,0.010526307,-0.020115433,0.03210863,0.0037928661,0.033847168,-0.01730389,-0.008115443,-0.009589126,0.0009770789,0.025792846,-0.017915094,-0.009915102,-0.0085432865,-0.0038200307,0.006617991,-0.012882842,-0.013718154,0.008196937,-0.0018285214,-0.006584035,-0.036862448,0.01893377,-0.010295407,0.0262139,-0.023388775,0.0017690987,0.00074490596,0.025195224,0.022071289,-0.0053514387,0.02022409,0.028305579,0.02038708,0.00040110323,-0.027395563,-0.008991503,0.0038947335,0.0074838647,0.026960928,0.0049643423,0.022152783,-0.01048556,-0.008122235,0.00030899805,0.0021443106,0.019477064,0.009663829,-0.014723247,-0.009568753,-0.0069881096,0.0035110328,0.015932076,-0.010023761,0.005646854,-0.03851949,-0.03441763,-0.019490646,-0.02185397,0.01386077,0.01718165,0.024325956,0.0014346337,-0.0019660424,0.0021969422,0.009405765,0.0035144282,0.003027162,0.018906604,0.020495739,0.022356518,0.005466888,-0.013935472,0.019246163,0.014478765,-0.0039456673,-0.014519513,0.023850573,-0.0038845467,-0.013195235,-0.0017894722,0.013079785,-0.011735135,-0.01969438,0.0029269925,-0.01142274,0.043381967,0.005789469,0.009446511,0.0009779278,-0.03058741,-0.025222389,-0.004998298,-0.013059412,-0.01322919,-0.042512696,-0.015565352,-0.02015618,-0.021758895,-0.018431224,0.038954128,-0.0063531357,0.017724942,0.0032359904,-0.024597602,0.0052088243,-0.03267909,-0.012876051,-0.0036842073,-0.027694374,-0.046207093,-0.016828509,0.0050899787,0.026757194,-0.0035042416,-0.013134114,-0.0042139185,0.01567401,-0.0073412503,-0.0075110295,0.015972821,-0.011076392,-0.0070696035,0.019381987,-0.032407444,0.00073514367,0.006604409,0.027042422,0.005663832,0.026743611,-9.741078e-05,-0.041752085,0.0071918443,-0.007293712,-0.022641746,0.02086246,-0.004468587,-0.019612886,0.007300503,-0.010920195,-0.0029524593,-0.011979616,-0.021419337,0.00081451546,0.0076128966,-0.01572834,0.012332757,-0.017494043,-0.0049100127,-0.01141595,0.012421042,-0.011252961,0.021636654,0.0069473623,0.018254653,0.011456696,-0.0010687597,0.000429329,0.0027979603,-0.015986403,0.0050899787,0.010213913,-0.022614582,0.0050458363,-0.0022037332,0.0017003381,-0.014560259,0.015049223,-0.024543272,0.032516103,-0.014587424,0.00015556013,-0.02097112,-0.0051103523,-0.018499136,0.008903218,-0.0028777565,-0.0064210473,0.0009898124,0.010376901,-0.018594211,0.0070696035,-0.007837005,-0.015185047,-0.017466879,0.023388775,0.009113745,0.02819692,-0.009344644,0.012237681,-0.02417655,-0.031755492,-0.022791153,-0.01718165,-0.0051884507,-0.0031307274,0.0019966026,0.02656704,0.035748698,6.3667176e-05,0.03420031,0.00996264,-0.015999986,0.017507626,-0.007972828,0.008210519,-0.022451594,0.0015458391,0.013935472,0.0025789451,0.028115425,0.0045670588,0.015592516,2.588137e-06,-0.018200325,-0.010098464,-0.014791159,-0.012747018,-0.0019405755,0.008747022,-0.009623081,-0.0026179943,-0.022112036,0.00556536,-0.018444806,-0.017222397,0.00081833545,0.00428183,0.0063565313,-0.0035449886,0.008400672,0.0029813217,0.0038641733,-0.025208807,-0.036427815,-0.017317472,-0.010213913,0.004753816,-0.0075653587,0.023972815,0.016380291,0.012617986,-0.0148726525,-0.0139898015,-0.02778945,-0.013976219,0.004859079,-0.010879448,-0.002449913,-0.021419337,-0.026865851,0.015388781,0.012631568,0.0047673983,-0.0255212,0.0041426113,0.0039796233,-0.017697778,0.009541588,0.014315777,0.024149384,0.012407459,0.022234276,0.019191835,-0.025426125,-0.0007194391,0.0022597604,0.0072597563,0.00056366675,0.02819692,0.022139201,-0.010397275,-0.02079455,-0.0040814904,0.014777577,-0.01671985,-0.017955841,0.031320855,0.011870958,-0.021365007,-0.006536497,-0.015293705,-0.026757194,-0.008658736,-0.007701182,0.0012139209,-0.0050594183,-0.0032767374,0.0048828484,-0.008570451,0.008604407,0.021650236,-0.0023565346,-0.011864167,0.012550075,0.009799652,-0.006937176,-0.0021578928,0.012135813,0.022872645,0.024434615,0.020522904,-0.017955841,-0.0053208782,-0.0027945647,0.007626479,-0.014736829,0.028359909,-0.019151088,0.012923588,0.005891336,-0.0035449886,0.005144308,-0.009860773,-0.015456693,-0.008162981,-0.007796258,0.0064040693,0.031755492,-0.002205431,-0.023035634,-0.0068624727,-0.007714764,-0.024285208,-0.014791159,0.0054431194,-0.011551773,-0.037894703,-0.002855685,-0.01648895,0.011076392,0.023524597,0.0044244444,-0.010648548,0.013589122,0.01508997,-0.027341234,0.0019830205,0.007966038,-0.0008115443,-0.014519513,-0.00061162934,-0.024814919,-0.022438012,0.011585728,0.014084877,-0.054220665,-0.013113741,0.025820011,0.004146007,0.016271632,0.00042720677,-0.008325969,0.013976219,-0.00041277555,0.009426138,0.009853981,0.030098446,0.006522915,0.008325969,0.0006960945,0.00358234,0.0057045794,-0.03064174,-0.0014821719,0.0042512696,-0.0302886,0.004933782,-0.0070152744,0.008162981,-0.011918495,-0.0012181654,-0.028468566,-0.012794556,-0.012088275,0.04870624,0.022913393,-0.029365001,-0.029093353,-0.0039185025,0.0039524585,0.015796252,-0.020495739,-0.034580614,0.0017996589,-0.017371802,-0.029392166,-0.0228319,-0.013949054,0.037324246,0.010471977,-0.007653644,-0.025942253,-0.014818324,-0.0139218895,-0.013650243,-0.023524597,-0.0047300467,-0.00967062,0.02336161,-0.015809834,0.018960934,-0.008033949,0.0016451599,-0.021826807,0.008699483,-0.0057045794,-0.019151088,0.025589112,0.0015976217,-0.033847168,0.018553466,0.00623429,-0.016624773,-0.0028981299,0.00059719814,-0.020604396,-0.015252958,-0.004400675,-0.00027971115,0.013235982,0.029473659,-0.0025942253,-0.013901516,-0.012183351,0.02510015,-0.015769087,0.005551778,-0.02173173,0.013317476,-0.007701182,-0.015443111,-0.026308976,-0.012196934,-0.0039796233,0.0009762301,0.0034006762,0.012319175,-0.007653644,0.011850584,0.016162975,0.0036332738,0.004308995,0.025086567,0.003663834,0.016258052,0.015293705,-0.018621376,-0.019952444,0.0018675706,-0.022994887,0.00077928626,-0.010533098,-0.0075314026,-0.01123938,-0.021120526,0.008550078,0.01928691,-0.03017994,0.02347027,0.0040509305,-0.0007207125,0.018431224,0.012095066,0.0039932053,-0.011246171,-0.017915094,-0.010098464,-0.01881153,-0.016502533,0.044767365,0.014994894,0.01794226,0.006648551,0.2204684,-0.017276725,0.002524616,0.02825125,0.021337843,0.0011434625,0.0130050825,0.027694374,-0.027056005,0.0035348018,0.009915102,-0.013120532,-0.0050424407,0.0037079765,0.0070967684,0.0032818308,-0.037079766,-0.037432905,-0.03058741,0.02855006,0.012196934,0.014057713,0.01351442,-0.02651271,0.051531367,-0.0013276729,-0.011741926,-0.00051952415,-0.0014966031,-0.004339555,-0.0065806396,-0.009154491,0.029663812,0.036998272,-0.010594219,-0.01771136,-0.003434632,0.004889639,0.018879442,-0.003271644,-0.012536492,0.007001692,0.004645157,-0.0114295315,-0.009799652,-0.0036502515,-0.009677411,-0.010179957,0.0030373489,0.022329353,-0.016380291,-0.010275033,0.017955841,0.027001675,-0.0014592517,-0.015932076,0.0054499106,-0.014641753,0.0048454967,-0.006797957,-0.02435312,0.007334459,-0.023823408,0.021419337,-0.03262476,0.002190151,-0.032950737,-0.005005089,0.02359251,-0.019721545,-0.032135796,0.003346347,-0.014302195,-0.0031358209,-0.02750422,-0.009731741,0.027952438,0.020998284,0.015796252,0.013439717,-0.0069881096,-0.0054634926,-0.011701179,-0.018634958,-0.0024533086,-0.007490656,0.0151443,-0.019599304,0.012679107,0.0026808127,0.004118842,0.0016663823,-0.017996589,0.013331058,-0.0106757125,0.018675705,0.0013828512,0.022275023,0.013494046,-0.025561947,-0.009290314,0.06122915,0.007680808,0.006910011,0.0019388779,0.0059117097,-0.0071171415,0.016067898,0.02010185,-0.019083176,0.0153344525,-0.050173134,0.002692697,-0.0032105236,-0.012441415,-0.0060746977,-0.00509677,-0.0019677402,0.0022682494,-0.01939557,-0.0028064493,-0.024421033,-0.021256348,0.0053208782,0.011368412,-0.019721545,-0.025439708,-0.0019728336,-0.00861799,-0.002825125,-0.014261448,-0.011205424,0.0022258046,0.01666552,-0.012421042,-0.025480453,-0.011551773,-0.026322559,0.0062410813,0.009453302,0.0025127314,-0.010410857,0.0038879423,-0.022804735,0.0073955795,-0.013847187,0.01602715,0.011932078,0.0047402335,-0.016067898,0.0012920193,0.003443121,-0.01362987,-0.016040733,0.02196263,-0.006016973,-0.023714751,-0.015293705,-0.005592525,-0.0033667204,-0.01707299,-0.027164662,0.020088268,0.013120532,-0.028387073,-0.008495748,-0.17515774,0.011517817,0.019680798,-0.05174868,0.014112042,0.02755855,0.018825112,0.015877746,-0.03460778,-0.011524608,0.021514414,-0.022356518,-0.0019966026,-0.011952451,-0.0048488923,-0.011259752,-0.013847187,0.014193537,0.014003384,0.014587424,0.05267228,-0.01532087,-0.007891335,0.012006781,-0.01753479,-0.018770782,0.0115314,0.033249546,0.013290311,-0.05155853,-0.017222397,-0.012903214,0.03186415,0.020264838,-0.011796255,0.0045602676,0.018730035,-0.02679794,-0.022573834,0.007735138,-0.0051918463,0.013935472,-0.00897113,0.0055076354,-0.016258052,0.0040509305,0.03645498,0.0018930375,0.0011443114,-0.011259752,0.014370107,-0.005728348,0.0072258003,-0.005843798,0.0020033938,0.009548379,0.017439714,-0.006129027,-0.005789469,-0.02079455,-0.020319168,-0.012570447,0.014139207,-0.0038709645,0.014601006,0.014193537,-0.014302195,0.02044141,-0.037188422,0.018485554,0.015022059,-0.018132413,-0.0040169745,-0.0019320867,5.058888e-05,0.026539875,0.0054736794,0.009480467,0.028821707,0.00022814072,-0.01066213,0.040909983,-0.020835297,0.022247858,-0.0060780933,-0.010404066,0.0024312374,0.034064487,-0.012679107,-0.028278414,0.01013921,-0.040529676,0.02074022,-0.0022886228,0.017806437,0.012400669,0.0057860734,-0.018838694,0.008733439,-0.008135816,-0.020237673,-0.0025042424,-0.009799652,0.021242766,0.018607793,-0.0115314,-0.014410853,0.014668918,0.037161257,0.0038336131,-0.009161282,-0.014994894,0.004797959,0.011273335,0.003052629,0.012685898,-0.010084881,-0.0024940555,-0.018974517,0.023266533,0.047049195,0.009663829,-0.01578267,0.014831906,-0.013079785,-0.016556863,-0.09534796,-0.0043022037,0.017752107,0.021229183,0.0074023707,0.043843765,-0.00016362465,0.012543283,-0.042268217,0.024217296,-0.043898094,-0.01886586,0.0057589086,-0.004084886,-0.0030780958,-0.011544982,0.00092614524,-0.025385378,-0.02242443,0.02855006,0.0034227476,-0.04900505,0.00028247005,-0.010010178,0.01817316,-0.024380285,-0.02155516,0.010390484,-0.0010016969,-0.000801782,0.017983006,-0.038954128,-0.02359251,-0.016108645,-0.010098464,0.024774173,-0.018471971,0.0040441393,0.0049779243,0.0007975375,0.022709658,-0.0009864168,-0.022071289,0.00013953724,-0.02097112,-0.0118573755,-0.023674004,0.028794542,0.0022920184,-0.011083183,-0.027626462,-0.00521222,-0.00990152,-0.004580641,-0.02247876,0.008047531,0.0093786,0.030044118,0.0154974405,0.017276725,-0.013901516,0.011144303,0.005874358,0.02330728,0.02201696,-0.016584028,-0.023660421,-0.0004762305,0.0072393827,-0.0058471938,-0.02242443,-0.00083319115,-0.00019949049,-0.00524278,-0.03349403,-0.0071443063,-0.014125625,-0.0020492342,-0.0029881129,-0.009881146,0.009195238,-0.008882845,0.006594222,-0.004455005,-0.023633257,0.019449899,0.014668918,-0.017752107,-0.026743611,-0.017697778,-0.012393878,0.02185397,0.013528002,-0.025684189,0.005582338,0.01771136,0.007932082,-0.0075042383,0.0045942236,0.010132419,-0.008319178,-0.0035042416,-0.045636635,0.02114769,-0.0012079786,-0.02510015,-0.008407463,-0.018770782,0.011714761,-0.0109066125,-0.0016366709,0.01013921,-0.012325966,0.0046485527,-0.031320855,0.018417642,-0.018308982,0.004971133,0.011687596,-0.020780968,0.0073140855,0.0012852281,-0.011470279,0.012421042,0.010831909,0.023212204,-0.010736833,-0.03938876,-0.028414238,0.009168074,-0.009371809,0.008706274,0.021650236,-0.008135816,0.0046859044,0.028115425,-0.009779278,-0.00838709,0.01310695,0.006536497,0.03512391,-0.024285208,-0.005935479,-0.0402037,0.00026803883,-0.007843796,0.03210863,0.009283523,0.0025178248,0.00060186704,-0.017317472,-0.0020288609,0.021921882,0.0071103503,-0.0038471953,-0.033792842,-0.011728344,0.009161282,-0.011280126,-0.01648895,0.020006774,0.0020203718,0.029555153,-0.016298797,0.02336161,-0.008142607,0.0047096736,0.00078904856,0.00076400617,0.021079779,0.013731737,-0.0255212,-0.00054244435,-0.007911708,0.005925292,0.037161257,0.03151101,0.01572834,0.0104787685,0.00247538,-0.0072325915,0.041100133,0.031538174,-0.0051375167,-0.026580622,-0.015443111,0.011701179,0.0062376857,0.013684199,0.010994897,0.0025670605,0.004825123,-0.010132419,0.0036264826,-0.0024516108,0.005551778,0.0009617988,0.0027300487,-0.005062814,-0.010621383,-0.006261455,0.015823416,-0.005429537,0.0010517818,-0.014016966,-0.021365007,-0.039578915,-0.0030611178,-0.027368398,-0.02587434,-0.02446178,0.0051816595,-0.0018285214,-0.0026808127,-0.001595075,0.021202018,-0.010336154,0.025684189,-0.015185047,-0.014696083,-0.032081466,0.018444806,0.0092971055,-0.0038132395,0.009704576,-0.0010959244,0.03583019,0.0048285187,0.019123923,-0.029256342,0.033575524,0.015606099,0.01304583,0.001562817,-0.00017370528,-0.011042436,-0.009690993,-0.02901186,-0.003280133,0.004886244,-0.004156193,0.067748666,-0.0017470274,0.009820025,0.031049209,0.008570451,0.026064495,0.02173173,0.006037346,-0.0045229164,-0.017575538,0.013521211,-0.007409162,-0.02022409,-0.016964331,0.013208818,-0.0019168066,0.002994904,0.021514414,-0.016896421,-0.01870287,-0.0010967733,-0.005843798,0.0015339545,0.01176909,0.01019354,0.007415953,0.014125625,0.0066383644,-0.0116943875,-0.03955175,0.0025432915,0.02405431,-0.04177925,-0.0107096685,-0.005663832,0.01748046,-0.007409162,-0.010594219,0.02015618,-0.009595918,0.00035441396,0.0016230886,-0.01661119,-0.017928677,-0.018159578,0.00675721,0.000180072,-0.013643452,-0.025208807]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.760276892115262
0.7247157468089849
0.7281786233878683
0.7360634899500083
0.7149694533245052
0.736344309877537
0.810789056108141
0.7532859406865187
0.7194342681547188
0.7316798027425797
0.7254642311871288
0.7413641638212848
0.730174401873083
0.7226716455609475
0.7459841571465439
0.7443371539396492
0.7195622690685939
0.7378351211199472
0.7089857676428677
0.7294484518847038
0.7668392125285949
0.7396695397488066
0.752183898319108
0.7769688859885692
0.748428479935405
0.7757176398653284
0.7550524522373192
0.7337467543295593
0.735766618283283
0.7601944527571474
0.7591701548016991
0.7637254266382146
0.7460800543259041
0.7304355010693953
0.7387065780682123
0.7525730771717186
0.7354877451689312
0.738152467347256
0.7206158716656262
0.7208544595039962
0.7364710211707568
0.8079324120272383
0.7549806308577048
0.7415540881582456
0.7205664704743341
0.7447484848346876
0.7533079155084264
0.7497672943336575
0.7597174360040214
0.792623594427274
0.7725377761592745
0.7882752862306578
0.7318122866360872
0.729070105424565
0.7871245593231561
0.7504923724308722
0.6994580072586438
0.7776254430344052
0.7310910922256568
0.7476774205964961
0.7877001632823418
0.7699990239710605
0.7522816395001691
0.7538085434182923
0.7529900817638764
0.7917575036413118
0.6740020416757232
0.7427693061390648
0.74499171636889
0.8049008019561517
0.7152070284998099
0.6826769499210027
0.7444701388297541
0.742300037735163
0.7244239575547723
0.7549612422050254
0.7539741457479335
0.7419345089622439
0.7458099138234793
0.7257993662429645
0.7365763682730221
0.7303929445743306
0.7595345806749434
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0061134654,0.020692773,0.010930238,0.0026222388,-0.0063307127,0.022335704,0.0022149007,-0.0024100835,-0.015831873,-0.040353626,0.00591998,0.024589641,-0.003893473,-0.002476276,-0.010692624,-0.013143442,0.012837938,0.015397379,0.0059573194,0.009036116,-0.0023829278,0.011296842,-0.0032298514,-0.0073524523,-0.014324722,0.039294545,0.029952927,-0.015736828,0.015112243,-0.02222708,0.015736828,-0.004198977,-0.016130587,-0.028622288,-0.0147049045,0.02835073,0.010774092,0.0022437538,0.027454587,0.01676875,0.016863797,0.023163958,-0.0044875075,0.0045655807,0.0061338325,0.016198477,0.015383801,-0.014922151,-0.02340836,0.009721802,0.0012338948,0.019117733,-0.021643229,-0.0041786097,-0.022349281,0.001609834,-0.0024779732,0.0046945713,0.010047672,-0.029681368,0.018058654,0.009769324,-0.018493148,0.01808581,-0.026626332,-0.0041921875,0.0064597027,0.00087153376,-0.0059369523,0.014745638,0.014066741,0.023041757,-0.008737401,-0.01755627,0.017176088,-0.02347625,-0.008703456,-0.012084363,-0.007922726,0.017176088,0.009457032,-0.010563634,-0.011473356,0.016660128,-0.0041887932,-0.021846898,0.020760663,0.01775994,-0.034297865,0.009490977,0.009348408,0.026395507,0.017176088,-0.0060998877,0.0063137403,-0.0058419066,-0.006069337,0.026273305,0.015044353,0.0077326344,-0.010970972,-0.003192512,-0.059145488,-0.017977187,-0.011154274,-0.0074882316,0.011201797,0.0043008113,0.0004147635,0.00037445402,-0.03280429,0.033890527,-0.006894197,-0.033863373,-0.0072234618,-0.0010056158,0.034949604,-0.007216673,-0.0038154,-0.012444178,0.021290204,0.00841832,0.031337876,-0.025024135,0.024630375,-0.0072845626,-0.017352602,-0.006771995,-0.02347625,-0.02268873,0.024426706,0.02465753,0.018791864,0.018370947,-0.024019368,0.033428878,-0.0132520655,0.017121777,-0.023109647,-0.008839236,0.011921427,0.020692773,0.0044773244,0.023462672,0.005736678,0.010740147,0.013890228,0.027603943,0.003065219,-0.0100137275,-0.012831149,-0.014786372,0.029246874,0.029436965,0.004355123,0.011799226,-0.0054481467,-0.00033626606,-0.01282436,-0.0031229253,-0.009124373,0.024766155,-0.008554099,-0.009307675,0.022077722,0.0135643575,0.024114413,-0.0121658305,0.0085065765,-0.0010471983,-0.02499698,0.038181156,1.8629882e-05,-0.008798502,0.00069714215,0.013116286,0.023367627,-0.02802486,-0.012016473,-0.007848047,-0.026938625,0.014637015,-0.009328042,0.0030737051,-0.03057751,0.022960288,0.004263472,-0.029735679,-0.003017696,-0.005940347,0.016483614,0.015994808,-0.00043788843,-0.0007315113,-0.6682517,-0.021453138,0.007603644,-0.019416448,0.0070333704,0.016130587,0.011215375,-0.006649794,-0.038045377,0.033184476,-0.021561762,-0.015207288,0.0036015473,-0.029491276,-0.019226357,-0.03997344,0.00048710845,-0.02571661,-0.012206564,0.0054006237,-0.011561613,0.0014545363,-0.0064019966,0.016714439,-0.001014102,0.007881992,0.0042804442,-0.024915513,0.021792587,-0.0074882316,0.0059573194,0.023965057,0.005387046,0.00015816174,0.022960288,-0.03761088,-0.026164683,0.010176662,-0.0025984775,0.032532733,-0.025350006,-0.01743407,0.008852814,0.004731911,-0.021779008,0.017379757,0.019497916,0.006266217,0.010359965,0.010312442,-0.005787595,0.014813527,0.013028029,-0.011133907,0.01801792,-0.023856433,0.024236616,-0.013367478,0.011731336,-0.019253513,0.016388569,-0.0027342567,-0.029409809,-0.0019738923,0.0028004493,0.01394454,-0.00697227,0.0017201548,-0.005387046,-0.031229252,0.02255295,0.011235742,-0.00019645576,0.012946562,0.016497191,-0.0012177711,0.041928664,0.01164308,0.01873755,0.003873106,0.016795907,-0.015302333,-0.019117733,0.0021588919,0.008438687,0.0011948583,0.008757768,-0.002041782,0.003971546,-0.005974292,0.027373118,0.02821495,-0.0027597153,-0.025173493,-0.011941794,0.017393336,0.0030041183,0.009511344,-0.0007404218,-0.043829575,-0.0043924623,0.0047794334,0.031337876,0.02222708,0.020882865,0.007963459,-0.017529115,-0.0030974667,0.016483614,-0.025010558,0.016972419,-0.0054277796,-0.015343067,0.003669437,-0.005281817,-0.024304505,0.014460501,0.017922875,0.008757768,-0.0043109944,0.009022539,-0.0053836512,0.016225632,-0.008968227,0.0008044927,0.0066531883,-0.015465269,-0.013537202,0.008492999,-0.010645102,0.014949307,0.006398602,0.019878097,-0.025526518,-0.0030397603,0.0124306,0.008268963,-0.007854836,0.016985998,-0.0015444902,-0.027318807,0.015506002,-0.0010480469,-0.020434793,-0.021561762,-0.013204542,0.006296768,0.009850793,0.008391164,-0.006829702,0.0060761264,-0.023204692,-0.0057536503,-0.00031314115,0.0011583676,0.0033028328,-0.0049152127,-0.03367328,0.006802546,-0.026870735,0.012016473,0.038642805,-0.023571296,-0.019850941,-0.0041853986,-0.024562486,-0.007888781,0.034623735,-0.0041005365,-0.013299588,0.007902359,-0.015967652,0.007386397,0.016035542,-0.017990764,0.0032145763,-0.009857581,-0.018981954,-0.0055126417,-0.021317359,0.01236271,0.0030686134,-0.015017197,0.0013654311,0.035275478,0.021005066,0.0062458506,0.021100111,-0.028567977,0.02716945,0.000198153,0.013768027,-0.021425983,0.0344608,0.020299014,0.013815549,0.03136503,-0.013258854,-0.020747086,0.01750196,0.027603943,0.025811655,0.02243075,-0.010930238,-0.0023421939,-0.011066018,0.0040088855,-0.01012914,0.021412404,0.020407638,0.027142294,0.0112832645,-0.0067821788,-0.012281243,0.0059946585,0.0024779732,-0.008431898,0.02224066,-0.011066018,0.0029277424,0.009226208,-0.004708149,0.044888657,0.009728591,-0.012057207,-0.012131886,-0.0018194434,0.024426706,0.034379333,-0.0023320105,-0.0036354922,-0.017108198,-0.012871883,0.012328765,0.0041582426,0.007603644,0.00030274552,-0.0018398103,0.033890527,-0.00816034,0.015410957,0.013605092,-0.00042176462,-0.014881417,0.022091301,-0.00039970048,0.029572744,0.026436241,0.004222738,0.0028649445,0.0075018094,-0.008662723,-0.003002421,0.021072956,-0.0021385248,-0.020706352,0.011615924,0.00947061,0.005410807,0.030170172,0.0295999,0.01466417,0.017094621,-0.0054685134,0.032587044,-0.022865243,-0.028758068,-0.00070944714,-0.015763983,0.0018177461,0.003092375,0.008628778,0.007372819,-0.023204692,0.011486934,0.0028598527,0.034650892,0.014175365,0.009490977,0.026327617,-0.02564872,-0.017800674,0.009742169,0.008655934,-0.012539224,-0.0050204415,-0.011480144,0.010516111,-0.020095345,0.03209824,0.0037203543,0.033836216,-0.017311867,-0.008072083,-0.009606389,0.0010285287,0.025743766,-0.017922875,-0.0099594155,-0.008601623,-0.0038425557,0.006622638,-0.013028029,-0.01367977,0.008201073,-0.0017787096,-0.0065785097,-0.03690483,0.018995533,-0.010292076,0.026205415,-0.02340836,0.0017922875,0.0007595158,0.025200648,0.022091301,-0.0053293398,0.02025828,0.028323574,0.020339748,0.00043619119,-0.027400274,-0.008954649,0.003873106,0.007420342,0.02696578,0.0049152127,0.02215919,-0.010441433,-0.008180706,0.00024631224,0.0022369649,0.019457182,0.009606389,-0.014745638,-0.009511344,-0.0069349306,0.0034759515,0.015872607,-0.010020517,0.0056314487,-0.03847987,-0.03440649,-0.019484337,-0.021887632,0.013863072,0.017135356,0.024358816,0.001508848,-0.0020010483,0.0021334332,0.009436665,0.00355063,0.0030635218,0.01887333,0.020475527,0.022335704,0.005505853,-0.014039585,0.019280668,0.014541969,-0.003907051,-0.014501235,0.023788543,-0.0038425557,-0.013157019,-0.001795682,0.013109497,-0.011731336,-0.019755896,0.0029362286,-0.011432622,0.043449394,0.0057536503,0.009429877,0.0010081617,-0.03057751,-0.025187071,-0.0049287905,-0.013028029,-0.013150231,-0.042526096,-0.015614626,-0.0201225,-0.021724697,-0.018438837,0.038887206,-0.006446125,0.017746361,0.0032587044,-0.024616797,0.005142643,-0.03264136,-0.012871883,-0.0036931986,-0.027766878,-0.046192136,-0.016809484,0.0051290654,0.026680643,-0.0035166852,-0.013150231,-0.0042057657,0.015696093,-0.00736603,-0.0074814423,0.015994808,-0.011066018,-0.007128416,0.019375714,-0.0323698,0.0007446649,0.006571721,0.027006514,0.005651816,0.026802845,-0.00013238487,-0.041738573,0.007209884,-0.007243829,-0.022634419,0.020896442,-0.0044807186,-0.019606538,0.0072506177,-0.010882716,-0.0029650817,-0.011935005,-0.02143956,0.000829527,0.007610433,-0.015682515,0.012267665,-0.01750196,-0.004908424,-0.011412255,0.012410233,-0.011222164,0.02157534,0.0069281417,0.018248746,0.011446199,-0.0010913266,0.0004162486,0.002841183,-0.016008386,0.005057781,0.010183452,-0.022566529,0.00506457,-0.0022216898,0.001715063,-0.014541969,0.015044353,-0.024576064,0.032478422,-0.014501235,0.0001597529,-0.020950755,-0.00516301,-0.018520305,0.008839236,-0.0028988894,-0.006446125,0.0010293772,0.010353176,-0.01854746,0.0070605264,-0.007875202,-0.015152976,-0.01750196,0.02335405,0.009158318,0.028242106,-0.009355198,0.012267665,-0.024182303,-0.031799525,-0.022743043,-0.0172304,-0.0051901657,-0.003051641,0.0019382503,0.026544863,0.03570997,3.6358106e-05,0.034189243,0.0099594155,-0.015954074,0.017515536,-0.008004193,0.008146761,-0.022349281,0.001589467,0.013998851,0.0026205415,0.02816064,0.004613104,0.015519581,2.7155871e-05,-0.018140122,-0.010115562,-0.01479995,-0.012756471,-0.0019110944,0.00868309,-0.009640334,-0.002561138,-0.02215919,0.005570348,-0.018425258,-0.017162511,0.0008537127,0.0042668665,0.0063103456,-0.0035676025,0.008425109,0.002973568,0.003828978,-0.025241382,-0.036416024,-0.017311867,-0.010230974,0.004708149,-0.007542543,0.02400579,0.016388569,0.012620691,-0.014867839,-0.01401243,-0.027794033,-0.013998851,0.0048642955,-0.010821614,-0.002484762,-0.021385249,-0.02684358,0.015397379,0.012627481,0.0047760387,-0.025472207,0.004131087,0.0039953077,-0.017746361,0.009531711,0.014379034,0.024141569,0.012389867,0.022213502,0.019212779,-0.025431473,-0.0007107201,0.0022284787,0.0073252963,0.00046461998,0.02821495,0.022091301,-0.01039391,-0.02085571,-0.00408017,0.01479995,-0.016741594,-0.01788214,0.031337876,0.011812804,-0.02137167,-0.006503831,-0.015288755,-0.0267078,-0.008662723,-0.0076919007,0.0012440784,-0.00506457,-0.0032654935,0.0048575066,-0.0086355675,0.008642356,0.021643229,-0.0024083864,-0.01183317,0.012573169,0.00986437,-0.0069043804,-0.0020926993,0.012091151,0.02282451,0.024426706,0.020529838,-0.01795003,-0.005254661,-0.0027240734,0.0076851114,-0.014772794,0.02835073,-0.019212779,0.012899038,0.0058724573,-0.0036083364,0.0051290654,-0.009891526,-0.015492424,-0.008207862,-0.007753001,0.006449519,0.031745214,-0.002230176,-0.023028178,-0.0068127294,-0.0077326344,-0.024263771,-0.014772794,0.0054617245,-0.01164308,-0.037909597,-0.002834394,-0.016415725,0.011045651,0.023516985,0.0044230125,-0.0106586795,0.013523624,0.015085086,-0.027345963,0.0020180207,0.007963459,-0.0008282541,-0.014541969,-0.0006194933,-0.024779733,-0.022403594,0.011595557,0.014121053,-0.05420312,-0.013095919,0.025838811,0.0041514537,0.0163071,0.00045486086,-0.008309697,0.013890228,-0.00038781977,0.009429877,0.009844003,0.030061549,0.0064427303,0.008241807,0.0007124173,0.0035879696,0.005702733,-0.030740445,-0.0015300636,0.0042532883,-0.030360265,0.004932185,-0.007013004,0.008235018,-0.011873905,-0.0011210283,-0.028486509,-0.012817571,-0.0121115185,0.048690476,0.022919556,-0.02932834,-0.02900247,-0.0039511793,0.0039511793,0.015777562,-0.020489104,-0.034569424,0.0017905902,-0.017406913,-0.029436965,-0.02282451,-0.01394454,0.037339322,0.010516111,-0.007624011,-0.02592028,-0.014840684,-0.013897017,-0.013666192,-0.023516985,-0.0047183326,-0.009694646,0.023313316,-0.015845452,0.018968375,-0.008065294,0.0016802695,-0.021765431,0.008710246,-0.0056857606,-0.019117733,0.025607986,0.0015733433,-0.033863373,0.018533882,0.0062186946,-0.016592238,-0.0029005865,0.0005448147,-0.020624883,-0.015315912,-0.004399251,-0.0002811057,0.013231698,0.029491276,-0.0025373767,-0.013958118,-0.012192986,0.025159914,-0.015709672,0.005563559,-0.021724697,0.01328601,-0.0077054785,-0.015478847,-0.026354773,-0.012220142,-0.0039240234,0.0010047673,0.0033843005,0.01230161,-0.0076511665,0.011826382,0.016157743,0.0036388868,0.004283839,0.025092024,0.0037033819,0.016212055,0.015315912,-0.018601771,-0.02005461,0.0018822413,-0.022973867,0.0007862473,-0.010543267,-0.0074950205,-0.011235742,-0.021127269,0.0085065765,0.019253513,-0.030143017,0.02348983,0.004005491,-0.00069841504,0.018384526,0.012091151,0.003907051,-0.01124932,-0.017909298,-0.010054462,-0.01881902,-0.016524348,0.04478003,0.015057931,0.017922875,0.0066124545,0.22028843,-0.017325446,0.0024813677,0.02821495,0.021344515,0.0011049045,0.013014452,0.027671833,-0.027115138,0.0035438412,0.009918682,-0.013095919,-0.005057781,0.003710171,0.0070944712,0.0033045302,-0.03695914,-0.037447948,-0.030604666,0.028486509,0.012220142,0.013985273,0.013523624,-0.026504131,0.05148753,-0.0013340322,-0.011751703,-0.0006076126,-0.0014757519,-0.004399251,-0.0065411706,-0.009185473,0.029681368,0.036931984,-0.010570423,-0.017732784,-0.003430126,0.0049253963,0.018886909,-0.0032756769,-0.012518857,0.006958692,0.004674204,-0.011452989,-0.009823636,-0.0036592537,-0.009694646,-0.010169874,0.0030329714,0.02236286,-0.016279945,-0.010251341,0.017977187,0.027074404,-0.0014307749,-0.015913341,0.0054753027,-0.014623437,0.0048914514,-0.0068093347,-0.02439955,0.007338874,-0.023883589,0.021317359,-0.03264136,0.0020994884,-0.03296723,-0.00499668,0.023557719,-0.01972874,-0.032071084,0.003360539,-0.014283989,-0.0031619617,-0.027522476,-0.009708224,0.027943391,0.02091002,0.015736828,0.013510046,-0.006951903,-0.005437963,-0.011683813,-0.018642506,-0.0024610008,-0.0074678645,0.015166554,-0.019552227,0.012756471,0.0026341195,0.0041548484,0.0017133658,-0.017990764,0.013347111,-0.010699414,0.018696817,0.0013213028,0.02224066,0.013584725,-0.025553675,-0.009253363,0.061209332,0.0076783225,0.0069383252,0.001963709,0.005940347,-0.007162361,0.01604912,0.02005461,-0.019090578,0.015315912,-0.05018405,0.002657881,-0.0032247596,-0.0124781225,-0.006096493,-0.005091726,-0.0019484338,0.0022878821,-0.019348558,-0.0027953575,-0.024413127,-0.021195158,0.0053293398,0.01137831,-0.019701585,-0.02545863,-0.0019026082,-0.008601623,-0.0028309997,-0.0141889425,-0.0110999625,0.0022776986,0.016660128,-0.012403444,-0.025512941,-0.011561613,-0.026286883,0.006222089,0.009450243,0.0025051292,-0.010421066,0.0038832896,-0.022797354,0.007393186,-0.013835916,0.016021963,0.011962161,0.004708149,-0.01604912,0.0012568077,0.003401273,-0.013632247,-0.016021963,0.021928366,-0.006004842,-0.02366634,-0.015275178,-0.00565521,-0.003331686,-0.017081043,-0.02715587,0.020095345,0.013157019,-0.02835073,-0.008526944,-0.17510106,0.011507301,0.019674428,-0.051731933,0.014093897,0.027481742,0.018832596,0.015804717,-0.034569424,-0.011486934,0.02143956,-0.022267815,-0.0019654061,-0.011921427,-0.0048303506,-0.01124932,-0.013801971,0.0141889425,0.013998851,0.014609858,0.052628078,-0.015275178,-0.007915936,0.0120368395,-0.017529115,-0.018791864,0.011514089,0.03326594,0.013394633,-0.051541843,-0.017243978,-0.012878672,0.03196246,0.020231124,-0.011792437,0.0045418194,0.01873755,-0.026789267,-0.022580108,0.0077394233,-0.005234294,0.01401243,-0.008934282,0.0055805314,-0.016225632,0.004131087,0.036470335,0.0019552228,0.0010989641,-0.011215375,0.014379034,-0.0057230997,0.0072370395,-0.0058826404,0.0019891676,0.009497766,0.017474802,-0.006157594,-0.0057230997,-0.020814976,-0.020366903,-0.0125528015,0.014175365,-0.0038663172,0.014623437,0.01420252,-0.014229677,0.020434793,-0.037149232,0.018493148,0.015044353,-0.018058654,-0.004032647,-0.0019161862,3.6729376e-05,0.026517708,0.005437963,0.009511344,0.028785223,0.00017110321,-0.0105840005,0.040896744,-0.020801397,0.02229497,-0.0061134654,-0.010421066,0.0024372395,0.034053463,-0.0127361035,-0.028323574,0.010095195,-0.040489405,0.02071993,-0.0022624235,0.01782783,0.012396655,0.005814751,-0.018886909,0.008778135,-0.008072083,-0.020285435,-0.0025204043,-0.009810058,0.021195158,0.018574616,-0.011493723,-0.0143383,0.014677748,0.037149232,0.0037305378,-0.009158318,-0.014990041,0.0047726445,0.011208585,0.0030804942,0.012749681,-0.010047672,-0.0024593035,-0.018968375,0.023245426,0.047006812,0.009653912,-0.015696093,0.014867839,-0.013143442,-0.016592238,-0.09548004,-0.004348334,0.01775994,0.02123589,0.0074135526,0.043856733,-0.00015954074,0.012546012,-0.042336002,0.024223037,-0.043911044,-0.01887333,0.005801173,-0.00408017,-0.003078797,-0.011588768,0.0008876575,-0.025404317,-0.02243075,0.028567977,0.0033775114,-0.04901635,0.00024440285,-0.010040883,0.018140122,-0.024358816,-0.021588918,0.010359965,-0.0010124048,-0.00082613254,0.018031498,-0.038914364,-0.023584874,-0.016130587,-0.010156296,0.02479331,-0.018465992,0.0040020966,0.004952552,0.000780307,0.022783777,-0.0010022214,-0.022118457,8.343005e-05,-0.0209236,-0.0118535375,-0.023693498,0.028785223,0.002230176,-0.011140696,-0.027631098,-0.0052241106,-0.009898315,-0.004609709,-0.022512218,0.00809245,0.009443454,0.029980082,0.015601048,0.017284712,-0.013958118,0.011079595,0.0058962186,0.023245426,0.022050567,-0.01663297,-0.023652764,-0.0004972919,0.0072506177,-0.005886035,-0.02236286,-0.00084268063,-0.00020303257,-0.0051901657,-0.033510346,-0.007121627,-0.014093897,-0.002084213,-0.0030075128,-0.009925471,0.009212629,-0.008900337,0.006592088,-0.004446774,-0.023598451,0.019457182,0.014623437,-0.017732784,-0.02677569,-0.01762416,-0.012383077,0.021914788,0.013462523,-0.025635142,0.005611082,0.01769205,0.007977037,-0.007542543,0.0045621865,0.010190241,-0.008302907,-0.00355063,-0.045594707,0.021168001,-0.0012466242,-0.025159914,-0.008431898,-0.018846175,0.011744915,-0.010855559,-0.0015886185,0.010197029,-0.01236271,0.0046300762,-0.03131072,0.018533882,-0.018303057,0.005057781,0.011649868,-0.02085571,0.0073252963,0.0012415325,-0.011452989,0.012484912,0.010835192,0.02315038,-0.010706202,-0.039430324,-0.028459353,0.009144739,-0.009300886,0.008723823,0.021561762,-0.008099238,0.004721727,0.028106326,-0.0098304255,-0.008357219,0.013082341,0.0064664916,0.0351397,-0.024277348,-0.005943741,-0.040082067,0.00029192562,-0.007854836,0.032125395,0.0092601525,0.0025662298,0.0006432547,-0.01729829,-0.0019959565,0.021874053,0.00710126,-0.0038832896,-0.03380906,-0.011738125,0.009171896,-0.0112832645,-0.016537925,0.020013876,0.0020180207,0.029409809,-0.016388569,0.023340471,-0.008201073,0.0047183326,0.0008460751,0.0007675777,0.021086534,0.013808761,-0.025526518,-0.0005690004,-0.007983826,0.00595053,0.037122075,0.03150081,0.015763983,0.010475378,0.002483065,-0.0072709844,0.041032523,0.031473655,-0.005149432,-0.026626332,-0.015451691,0.011744915,0.006174566,0.013605092,0.011072806,0.0026205415,0.004789617,-0.010197029,0.0036558593,-0.0024253586,0.00555677,0.0009012355,0.0026765505,-0.0049898913,-0.010617945,-0.0062560337,0.015913341,-0.005444752,0.0010107076,-0.013971696,-0.021263048,-0.039511792,-0.0030414576,-0.027278073,-0.025825234,-0.024345238,0.0051324596,-0.0018296269,-0.0026476975,-0.0016717833,0.021222314,-0.010434643,0.025607986,-0.015139398,-0.01466417,-0.032016773,0.018370947,0.009361987,-0.0038561337,0.009653912,-0.0011116934,0.035872906,0.0048914514,0.019158468,-0.029246874,0.033564657,0.015614626,0.013068763,0.0015767377,-0.00018977287,-0.0110252835,-0.009694646,-0.028975315,-0.0032722824,0.0048507173,-0.0041582426,0.06761812,-0.0016853613,0.009837214,0.031066317,0.008554099,0.026069636,0.021683963,0.006069337,-0.0045689754,-0.01755627,0.013510046,-0.0074135526,-0.020231124,-0.016931687,0.013170597,-0.0019688006,0.0029311369,0.02150745,-0.016918108,-0.018696817,-0.0011099962,-0.005865668,0.0014536878,0.011697392,0.010332809,0.0074610757,0.014134631,0.006683739,-0.011670236,-0.039511792,0.0024491202,0.024019368,-0.04176573,-0.010672257,-0.005645027,0.017515536,-0.0074067637,-0.010685835,0.020163234,-0.009626756,0.00035811806,0.0016021964,-0.016660128,-0.017977187,-0.018099388,0.0068161236,0.00020069887,-0.013713715,-0.025227804]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.7366392048830853
0.7357915620715477
0.7206379161277926
0.7166847273531329
0.7498482800362257
0.7752621142697748
0.6782068277048293
0.6892060656160305
0.689615207899111
0.7280383729378012
0.7514641842882065
0.6974612667646097
0.6847412347542368
0.7219974812064656
0.7346033829954361
0.7965168448783085
0.7424549918950103
0.6861764311253561
0.6858095720452543
0.7554845419797879
0.7639943579973663
0.7360025113675297
0.6939093517462863
0.6843731786431655
0.745018982875796
0.7134073315096966
0.7244437759969901
0.7139367451234614
0.6862398648893643
0.7137120693795634
0.6804477828647028
0.7322263683057536
0.7298100163717006
0.7790953794411556
0.7123931644487653
0.770603983703393
0.7123335806273833
0.7403702451735601
0.7418717832437649
0.683959805253076
0.7524144456207519
0.7706638903092372
0.6713609642619882
0.7197096425436997
0.7591477369615875
0.7270258201077947
0.7655407020830418
0.7287989883120781
0.6893677178498612
0.7509086399170884
0.6962919715363939
0.7341562807619019
0.7229994369587494
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0061325156,0.02071337,0.010940734,0.0025840777,-0.006346441,0.022343274,0.0022037667,-0.0024210874,-0.015891569,-0.040448796,0.00594236,0.024611559,-0.0039049794,-0.002489,-0.010682666,-0.013134314,0.012869454,0.015348268,0.005932173,0.0090391785,-0.0023837353,0.011266715,-0.0032496222,-0.007409274,-0.014302412,0.039307863,0.029935913,-0.015714996,0.01514453,-0.02222103,0.015769325,-0.0042105867,-0.01610889,-0.028686319,-0.01472347,0.028387504,0.010750579,0.0022445144,0.02746389,0.0168016,0.016855929,0.023158226,-0.0044924244,0.0045841066,0.0061528897,0.016203968,0.015348268,-0.014967957,-0.023402711,0.009752262,0.001253838,0.019110631,-0.021636982,-0.004244543,-0.022288945,0.0016502783,-0.0024499502,0.004706349,0.010023912,-0.029691428,0.018105522,0.009752262,-0.018526582,0.018010445,-0.026635356,-0.0041562566,0.006526409,0.0008981453,-0.0059389644,0.014709888,0.014071509,0.023035984,-0.00872678,-0.01754864,0.017086832,-0.023511373,-0.0086317025,-0.012129206,-0.0079865325,0.01718191,0.009378742,-0.010499301,-0.011463662,0.01670652,-0.00417663,-0.021867886,0.020808447,0.01776596,-0.034282323,0.009500985,0.009297247,0.026418036,0.017100414,-0.006149494,0.006285319,-0.005816722,-0.006057812,0.026268627,0.015130946,0.0076877163,-0.010954317,-0.0031732204,-0.059274193,-0.01798328,-0.011144472,-0.0075179343,0.011144472,0.0043158513,0.00036672852,0.00038349445,-0.03281541,0.03390201,-0.0068354122,-0.03392918,-0.0072802403,-0.0010246327,0.03490712,-0.007198745,-0.0037997148,-0.01247556,0.02129742,0.008461921,0.031348497,-0.024978286,0.024638724,-0.0072734486,-0.017385649,-0.0068489946,-0.023375547,-0.022669256,0.024448568,0.02467947,0.018798232,0.018390756,-0.024041092,0.033440206,-0.013270139,0.017141161,-0.023144644,-0.008815067,0.011911886,0.02071337,0.0044720504,0.023524955,0.0057250396,0.010689457,0.0139085185,0.02755897,0.0030951207,-0.010003539,-0.012855872,-0.014859296,0.02917529,0.029365446,0.00430906,0.011803225,-0.005449993,-0.00033192325,-0.012815124,-0.0031222857,-0.0091410475,0.024747383,-0.008516251,-0.009310829,0.022085207,0.013562163,0.024054674,-0.012122415,0.008482295,-0.0010161436,-0.02499187,0.038194094,-2.8199682e-05,-0.0087675275,0.0007088387,0.01311394,0.02344346,-0.028034357,-0.011986589,-0.007823542,-0.026920589,0.014574063,-0.009378742,0.0030815382,-0.03050638,0.022995235,0.0042105867,-0.029745758,-0.0030153233,-0.006010273,0.016502783,0.016040977,-0.00044270584,-0.00076868676,-0.66804355,-0.021433244,0.0075926385,-0.019409446,0.0070697106,0.016163219,0.011232759,-0.0066927955,-0.03805827,0.033222888,-0.021555487,-0.015212442,0.0035925813,-0.029555602,-0.01921929,-0.03998699,0.00046477746,-0.025766073,-0.012271822,0.005426224,-0.011619861,0.0014813455,-0.0064686835,0.016760852,-0.0010798117,0.007850707,0.0042988732,-0.024937538,0.021813555,-0.007422857,0.0059389644,0.023918848,0.005368498,0.00013349087,0.023076732,-0.03762363,-0.02611922,0.010200486,-0.0025670996,0.03254376,-0.025345014,-0.017412813,0.008849023,0.004709745,-0.02173206,0.017331317,0.01949094,0.00623778,0.010417806,0.010288772,-0.005758996,0.014818548,0.013039236,-0.011171637,0.018078357,-0.023850936,0.02429916,-0.013358425,0.011728521,-0.019232873,0.016380541,-0.0027606508,-0.029365446,-0.002015309,0.002806492,0.013901727,-0.006974633,0.0017147954,-0.0053379373,-0.031239836,0.022560595,0.011327837,-0.00021424331,0.012930576,0.0164892,-0.0011579114,0.041942876,0.011680982,0.018662406,0.0039321445,0.01670652,-0.01525319,-0.019137796,0.0020866173,0.0084279645,0.0012241262,0.008787902,-0.00206115,0.0039694966,-0.0060544163,0.027314482,0.028170183,-0.0027606508,-0.025141276,-0.011966215,0.017399231,0.0030255103,0.009555316,-0.0007215723,-0.04384443,-0.004427907,0.0047980314,0.03137566,0.022248197,0.020917108,0.007972949,-0.017575804,-0.0030390928,0.016516365,-0.02499187,0.016991755,-0.005463576,-0.015334684,0.0036706808,-0.0053141676,-0.024312742,0.014519732,0.017847454,0.0087335715,-0.0043226425,0.009012014,-0.0053820806,0.016176803,-0.00899164,0.00082301686,0.0066350694,-0.01547051,-0.013562163,0.008502669,-0.010574006,0.014995121,0.0063837925,0.019830504,-0.025562335,-0.0030577688,0.012434813,0.008271765,-0.00791862,0.016991755,-0.0015017194,-0.027355231,0.0154433455,-0.0010144458,-0.020414554,-0.021609817,-0.013229392,0.0062751323,0.009881296,0.008332887,-0.006832016,0.005979712,-0.023212556,-0.005731831,-0.00027313633,0.0011961123,0.0033090457,-0.0049746046,-0.033766188,0.00674373,-0.026879841,0.011973007,0.038683068,-0.023538537,-0.019789757,-0.0041936086,-0.024584392,-0.007864289,0.03463547,-0.004190213,-0.013344843,0.007843915,-0.01605456,0.0073956917,0.01600023,-0.017956115,0.0032632046,-0.009847339,-0.018974805,-0.005517906,-0.021338167,0.012380483,0.0030798404,-0.015035869,0.0013667429,0.035368927,0.021025768,0.006268341,0.021175176,-0.02857766,0.027178658,0.000148559,0.013677616,-0.021433244,0.03447248,0.020346642,0.013901727,0.031402826,-0.013283722,-0.020781282,0.01745356,0.027640464,0.025861152,0.022519847,-0.010940734,-0.0023073335,-0.011001855,0.0039932663,-0.01016653,0.021433244,0.020441718,0.027219405,0.011314253,-0.006760708,-0.012305779,0.00602046,0.002517863,-0.008387217,0.022248197,-0.011144472,0.0029609932,0.00920896,-0.004747097,0.044849537,0.009731888,-0.012074876,-0.012108833,-0.0017894993,0.024448568,0.034390986,-0.002331103,-0.0035891857,-0.016991755,-0.012876245,0.012346527,0.004084948,0.00765376,0.00028586996,-0.0018251535,0.033874847,-0.008231018,0.01536185,0.01359612,-0.0003665163,-0.014845714,0.022139536,-0.00036842632,0.029555602,0.026377289,0.0042411475,0.0029049653,0.0074907695,-0.00861812,-0.00301023,0.021052932,-0.0021799973,-0.020699788,0.011626652,0.009487403,0.005378685,0.030207563,0.029582767,0.014628393,0.017113997,-0.005487345,0.03265242,-0.022886576,-0.02879498,-0.0006761557,-0.015701413,0.0018506207,0.003101912,0.008645285,0.007361735,-0.023117479,0.0115112,0.0027827225,0.034608305,0.014125839,0.009460238,0.026377289,-0.025643831,-0.017779542,0.009725097,0.0085977465,-0.012563847,-0.005069682,-0.011443288,0.010499301,-0.020115739,0.03210912,0.0037657586,0.033793353,-0.017344901,-0.008129149,-0.009589272,0.0009957697,0.025779657,-0.017928949,-0.009928835,-0.008536625,-0.0037929236,0.006628278,-0.012964532,-0.013684407,0.008210644,-0.0018472251,-0.0065535745,-0.03689017,0.018974805,-0.010268399,0.02617355,-0.023416294,0.0017928949,0.0007275147,0.025195608,0.022058042,-0.005303981,0.02018365,0.028306007,0.020373806,0.0003902857,-0.027395979,-0.00899164,0.0038947926,0.0074839783,0.026947755,0.0049372525,0.022112371,-0.010465345,-0.008108775,0.00030581933,0.002210558,0.019477358,0.009616436,-0.01472347,-0.009555316,-0.0069304896,0.0034567558,0.015918734,-0.010017121,0.0055960054,-0.038520075,-0.03441815,-0.01949094,-0.021895051,0.013867771,0.017168328,0.024380654,0.0014966258,-0.001977957,0.0021969755,0.009480611,0.003521273,0.0029966473,0.018906891,0.020509632,0.022343274,0.0054771584,-0.013935683,0.019246455,0.0144518195,-0.0039423313,-0.014519732,0.023796605,-0.0038846058,-0.0131954355,-0.0018251535,0.013086775,-0.011708147,-0.019681096,0.0029406194,-0.011382166,0.04335546,0.005758996,0.009439863,0.000994072,-0.03061504,-0.025195608,-0.0049712085,-0.013046027,-0.013242974,-0.042486176,-0.015552006,-0.020170068,-0.021813555,-0.018390756,0.038900387,-0.006400771,0.017779542,0.00319699,-0.024611559,0.0051953206,-0.03270675,-0.01289662,-0.0036570982,-0.027708376,-0.04618063,-0.0168016,0.005052704,0.026730433,-0.0035280641,-0.013141105,-0.0042309603,0.01568783,-0.007354944,-0.007504352,0.015973065,-0.011062977,-0.0070561282,0.019395864,-0.0324351,0.00076614,0.006628278,0.026988503,0.0056808963,0.026771182,-7.677317e-05,-0.041725554,0.0071715796,-0.0072734486,-0.022655673,0.020808447,-0.00444149,-0.019613184,0.007348153,-0.010927152,-0.0029762734,-0.011945842,-0.021378914,0.00081877236,0.00765376,-0.015701413,0.012319362,-0.017467143,-0.0048693395,-0.0114568705,0.012387274,-0.011198802,0.021636982,0.006899929,0.018241348,0.011497618,-0.0010721716,0.00041384294,0.00282347,-0.015973065,0.0051104296,0.010248025,-0.02258776,0.005083265,-0.0022241406,0.0017258312,-0.014560481,0.015076617,-0.024638724,0.03254376,-0.014628393,0.00013635594,-0.020998603,-0.0051341993,-0.018512998,0.008889771,-0.0028778,-0.0064788703,0.0010152947,0.010370268,-0.018580912,0.007103667,-0.007843915,-0.015185277,-0.017467143,0.023375547,0.009120674,0.028197348,-0.0093447855,0.012217493,-0.024217665,-0.031755973,-0.02280508,-0.01718191,-0.0051953206,-0.0030951207,0.0019558854,0.026567444,0.035722073,8.303387e-05,0.03420083,0.009956,-0.01600023,0.017494308,-0.007932202,0.008197062,-0.022411186,0.0015008705,0.013976431,0.0025484236,0.028115852,0.004533172,0.015592753,-3.1621847e-05,-0.018187018,-0.010146156,-0.014764219,-0.01278796,-0.0019270225,0.0087335715,-0.009596063,-0.0025399346,-0.022085207,0.005551862,-0.018431503,-0.01718191,0.0008349016,0.00430906,0.006353232,-0.0034975035,0.008421173,0.002998345,0.0038472537,-0.025222773,-0.036482695,-0.017331317,-0.010268399,0.0047674705,-0.007579056,0.024013927,0.016353374,0.012584221,-0.014859296,-0.014003596,-0.027803455,-0.013949266,0.004848966,-0.0108796125,-0.0024024113,-0.021473993,-0.026879841,0.01536185,0.012584221,0.0047606793,-0.025521588,0.0041834214,0.003996662,-0.01772521,0.009568898,0.014343159,0.024163334,0.01241444,0.022207448,0.019205708,-0.025399346,-0.000682947,0.0022445144,0.0072394926,0.00056791987,0.028170183,0.022139536,-0.0104381805,-0.020794865,-0.004078157,0.014791383,-0.016733686,-0.01798328,0.03132133,0.011850764,-0.021365331,-0.0065433877,-0.01525319,-0.0267576,-0.0086656585,-0.007680925,0.0012716651,-0.005059495,-0.0032903696,0.0049236696,-0.008584164,0.008604538,0.021636982,-0.0023752463,-0.011857555,0.012536682,0.009786218,-0.006879555,-0.0021426452,0.012108833,0.022886576,0.02440782,0.020536797,-0.017969698,-0.0052971896,-0.0027844203,0.0076469686,-0.014764219,0.028360339,-0.019205708,0.012910202,0.0058676563,-0.0035348553,0.005144386,-0.009833757,-0.015484093,-0.008197062,-0.007728464,0.0063973754,0.031783137,-0.0022088601,-0.023035984,-0.0068184338,-0.00770809,-0.02429916,-0.014777801,0.0054567843,-0.011565531,-0.03789528,-0.0028930805,-0.016516365,0.011090142,0.023511373,0.004410929,-0.01064871,0.013562163,0.015103782,-0.027341649,0.0019966331,0.007932202,-0.0008090099,-0.01450615,-0.0005959338,-0.024815297,-0.022438351,0.011585904,0.0141122565,-0.054194324,-0.013120731,0.025833987,0.0041562566,0.01627188,0.000392408,-0.008346469,0.013922101,-0.00046180628,0.0094466545,0.009881296,0.030098904,0.006492453,0.008319304,0.00071902556,0.0035823942,0.005721644,-0.030642204,-0.0014550294,0.004244543,-0.030316224,0.004920274,-0.0070832935,0.008183479,-0.011871138,-0.0011850764,-0.028496163,-0.012781167,-0.012088458,0.04870698,0.022886576,-0.029365446,-0.029093795,-0.003918562,0.0039253533,0.01578291,-0.0204553,-0.034553975,0.0018166644,-0.017385649,-0.029392611,-0.022832246,-0.013949266,0.037351977,0.010506093,-0.00765376,-0.025901899,-0.014764219,-0.0139085185,-0.0136368675,-0.023538537,-0.004730119,-0.009670767,0.023375547,-0.015810074,0.018974805,-0.008068027,0.0016477316,-0.021799972,0.0086317025,-0.0057420176,-0.019083465,0.025562335,0.0015857612,-0.033847682,0.018485833,0.006214011,-0.016611444,-0.0028862893,0.0005636753,-0.020591127,-0.015266772,-0.0044075334,-0.0002680429,0.013270139,0.029501272,-0.0025399346,-0.0139085185,-0.012210702,0.025141276,-0.01574216,0.005551862,-0.021718478,0.013270139,-0.007680925,-0.015429762,-0.02633654,-0.012197118,-0.0039932663,0.0009787916,0.0033820518,0.012339735,-0.007660551,0.011891511,0.016190385,0.003602768,0.0043328293,0.0250462,0.003704637,0.016312627,0.015321102,-0.018621659,-0.019952748,0.0018845771,-0.022940906,0.0007678378,-0.010553632,-0.007558682,-0.0112870885,-0.021093681,0.008536625,0.01927362,-0.030180398,0.023470625,0.004105322,-0.0007457662,0.018431503,0.012129206,0.00401364,-0.011259924,-0.017928949,-0.010091825,-0.018825397,-0.016475618,0.044795208,0.015049452,0.017928949,0.006628278,0.22047175,-0.017331317,0.0024940935,0.028224513,0.021338167,0.0011265017,0.013012071,0.027681211,-0.027069997,0.0035688118,0.00990167,-0.013134314,-0.005042517,0.0037114285,0.0070629194,0.003241133,-0.03708033,-0.037406307,-0.030642204,0.02857766,0.01225824,0.014030761,0.013528207,-0.026526695,0.051532146,-0.0013158083,-0.011762477,-0.00060357404,-0.0015000214,-0.0043124557,-0.0065807393,-0.009134256,0.029637096,0.037025996,-0.010594379,-0.017684463,-0.0034499646,0.0048693395,0.018906891,-0.0032547156,-0.012461978,0.0070085893,0.0046282494,-0.011429706,-0.009860923,-0.0036638896,-0.009691141,-0.010193694,0.0030339994,0.022356857,-0.016366959,-0.010261607,0.017888201,0.027015667,-0.001439749,-0.015905151,0.005487345,-0.014601228,0.004838779,-0.00679806,-0.024367072,0.0073209875,-0.023837354,0.021392496,-0.03270675,0.0021749039,-0.03292407,-0.004988187,0.023565702,-0.019721845,-0.032136284,0.0033548868,-0.01428883,-0.0031647312,-0.027504638,-0.009752262,0.027980028,0.020971438,0.015823657,0.0135010425,-0.0069678416,-0.005487345,-0.011680982,-0.018635241,-0.0024652306,-0.0075179343,0.01514453,-0.01953169,0.012679298,0.002674062,0.0041494654,0.0016833857,-0.017996863,0.013365217,-0.010723413,0.018662406,0.0013497646,0.02226178,0.013507834,-0.025562335,-0.009256499,0.06123008,0.0076877163,0.0069542592,0.0019456985,0.0059016123,-0.007096876,0.016081724,0.02007499,-0.019110631,0.01530752,-0.05020106,0.0026502926,-0.0032275505,-0.012455187,-0.006105351,-0.005073078,-0.001979655,0.0022818663,-0.019409446,-0.0028132831,-0.024367072,-0.021188758,0.0053277505,0.01134821,-0.019694678,-0.02542651,-0.0020102155,-0.008652076,-0.002915152,-0.014193752,-0.0111852195,0.0022326296,0.016692938,-0.012394065,-0.025467258,-0.011524783,-0.026241463,0.0062479675,0.0094466545,0.0024720218,-0.01038385,0.0039049794,-0.022845829,0.0073821093,-0.013847397,0.016027395,0.012027337,0.0047233272,-0.016013812,0.0012886432,0.003436382,-0.01359612,-0.01600023,0.021962963,-0.0060476246,-0.023742275,-0.015239608,-0.0056265662,-0.0033582824,-0.01707325,-0.027178658,0.020047825,0.013147896,-0.028387504,-0.0084958775,-0.17516041,0.011504409,0.019694678,-0.051749468,0.0141122565,0.027450308,0.018852562,0.015864404,-0.03458114,-0.011531575,0.021541905,-0.022302527,-0.0019813527,-0.011952633,-0.004825196,-0.011266715,-0.0138406055,0.014234499,0.014030761,0.014587645,0.05267308,-0.01530752,-0.007938993,0.011979798,-0.017562222,-0.018771067,0.011524783,0.033250052,0.0132973045,-0.05158648,-0.01723624,-0.012876245,0.031891797,0.02023798,-0.011816808,0.0045942934,0.018703153,-0.026784765,-0.022560595,0.0077488376,-0.005191925,0.013935683,-0.008916936,0.0055246972,-0.016258297,0.004037409,0.03645553,0.0019015552,0.001133293,-0.01129388,0.014383907,-0.005758996,0.0072123273,-0.005871052,0.00196777,0.009541732,0.017494308,-0.00612912,-0.0057963477,-0.02082203,-0.020319477,-0.012570638,0.014180169,-0.0038880014,0.014628393,0.014234499,-0.01428883,0.020428136,-0.037243318,0.018445086,0.014967957,-0.01809194,-0.0040238267,-0.0019236269,6.637404e-05,0.026567444,0.005504323,0.00952815,0.028767815,0.00023281318,-0.010655501,0.0409106,-0.020849194,0.02226178,-0.0060408334,-0.01038385,0.0024261808,0.03403784,-0.01268609,-0.028306007,0.010125781,-0.04053029,0.020754118,-0.0022513056,0.017793125,0.012407648,0.0057827653,-0.018852562,0.0087335715,-0.008108775,-0.020251563,-0.0025399346,-0.009786218,0.021188758,0.018594494,-0.011531575,-0.014424655,0.014669141,0.037161823,0.0037555716,-0.009120674,-0.015022286,0.0047980314,0.011246341,0.0030577688,0.012706464,-0.010098617,-0.002424483,-0.018920476,0.0233348,0.04704991,0.009623228,-0.015755743,0.014832131,-0.01311394,-0.016557112,-0.095349416,-0.004326038,0.017752377,0.021229506,0.0073413616,0.043871596,-0.00015503193,0.012529891,-0.042296022,0.024258412,-0.04389876,-0.018825397,0.005813326,-0.00406797,-0.0030781426,-0.011565531,0.0009304038,-0.025399346,-0.022438351,0.02857766,0.0034244973,-0.04897863,0.00028735556,-0.010023912,0.018119104,-0.024367072,-0.021555487,0.010404224,-0.0009881296,-0.0008090099,0.018051192,-0.038954716,-0.023565702,-0.016095307,-0.010085034,0.02478813,-0.018485833,0.004057783,0.0049610217,0.00078184484,0.022669256,-0.0009991655,-0.022058042,0.000104256585,-0.02098502,-0.01188472,-0.023674363,0.028876474,0.0023158228,-0.011110515,-0.027613299,-0.005202112,-0.00990167,-0.0046316455,-0.022451935,0.008040862,0.009405907,0.030044572,0.015497675,0.01723624,-0.013976431,0.011117307,0.005908404,0.023253305,0.022030875,-0.016625026,-0.023687946,-0.00047326655,0.007198745,-0.0058574695,-0.022411186,-0.0008442396,-0.00021424331,-0.0051953206,-0.033440206,-0.0071172495,-0.014125839,-0.002117178,-0.0030000429,-0.009881296,0.009195378,-0.0089033535,0.0065535745,-0.0044380943,-0.023647198,0.019450193,0.01461481,-0.017738795,-0.026825512,-0.017711628,-0.012380483,0.021867886,0.013521416,-0.025670996,0.005589214,0.017698046,0.007938993,-0.007511143,0.00460448,0.010146156,-0.0082921395,-0.0035348553,-0.04561016,0.021148011,-0.0012071481,-0.025114112,-0.008441547,-0.018825397,0.0116674,-0.0108796125,-0.0016553717,0.010132573,-0.012265031,0.0046418323,-0.031294167,0.018445086,-0.01825493,0.004988187,0.011674191,-0.020835612,0.007361735,0.0012835498,-0.011463662,0.012482352,0.0108117,0.023212556,-0.010770952,-0.039470855,-0.028441833,0.00915463,-0.009324412,0.0086996155,0.0216234,-0.008142731,0.004699558,0.028088687,-0.00979301,-0.008360052,0.013073192,0.0064958488,0.035124443,-0.024326324,-0.005932173,-0.040177148,0.00027568307,-0.007884663,0.032081954,0.009256499,0.0025110717,0.00058489805,-0.017317735,-0.0020034243,0.021908633,0.007076502,-0.0038472537,-0.033793353,-0.011701356,0.009161421,-0.0112191755,-0.016529948,0.020007078,0.0020051221,0.029501272,-0.016312627,0.023389129,-0.008169896,0.0047131404,0.0008056143,0.00074958627,0.021107264,0.013725154,-0.02557592,-0.00051868317,-0.007945784,0.0059457556,0.03713466,0.031457156,0.015728578,0.010506093,0.0024618348,-0.0072191185,0.041127924,0.03153865,-0.005137595,-0.026635356,-0.015389015,0.011660608,0.0062106154,0.0136708245,0.011015438,0.0026044515,0.0048455703,-0.010112199,0.0036129549,-0.0024346698,0.005534884,0.0009363462,0.002767442,-0.005083265,-0.010628335,-0.006285319,0.015864404,-0.005449993,0.0010314239,-0.013990014,-0.02135175,-0.039579514,-0.0030951207,-0.0273009,-0.025888316,-0.024434986,0.005198716,-0.0018217579,-0.0026808532,-0.0016035883,0.021175176,-0.010397432,0.02569816,-0.015130946,-0.01472347,-0.03205479,0.01841792,0.009263291,-0.00382688,0.00974547,-0.0011120703,0.035885066,0.0048897136,0.019083465,-0.02928395,0.033521704,0.015633501,0.013012071,0.0015246398,-0.00016511272,-0.011042603,-0.009725097,-0.0290123,-0.0032920674,0.004896505,-0.0042105867,0.067749694,-0.0017419604,0.009772636,0.031076847,0.008556998,0.026092054,0.02173206,0.0060781855,-0.004560337,-0.017589387,0.01348746,-0.0074364394,-0.02023798,-0.01696459,0.013202227,-0.0019236269,0.0030034387,0.021501157,-0.016910259,-0.018662406,-0.0011035812,-0.0058370954,0.0015246398,0.011708147,0.010241234,0.007409274,0.014139421,0.006641861,-0.011735313,-0.03949802,0.0024822087,0.02408184,-0.041725554,-0.010709831,-0.0056333574,0.017480725,-0.007429648,-0.0106419185,0.020115739,-0.009596063,0.00030199924,0.0015950992,-0.016584279,-0.017928949,-0.018119104,0.0067641037,0.00016277822,-0.013616494,-0.025195608]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
[{"sentences":["We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ).","The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings.","The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings.","These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions.","We use Byte-Level BPE tokenization as implemented in HuggingFace.","GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) .","We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing.","As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J.","Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs.","Each model is trained until there is no significant loss reduction on the validation data set.","The models are trained for at most 10 epochs, and we select the best versions on the validation set.","We use beam search for inference using a beam size of 5."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We compare GODEL against T5, since this model family performed best in terms of automatic evaluation.","Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe.","Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks.","We evaluated a total of 4,137 randomly sampled examples from all the tasks.","Table 8 shows the winning percentage of the total.","Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting.","We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks.","It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."],"sections":[{"title":"Human Evaluation Results","number":"5.3"}],"section_type":"main"}]
0.752046110075299
0.7409671371451524
0.7435877879600412
0.7350674496377254
0.7202420969416076
0.7482431980377393
0.7897987091320888
0.7382314267181326
0.7203037339563257
0.7244907337795972
0.774961198171173
0.7368437100542677
0.722712586242885
0.7463526105206816
0.7504448799649321
0.7322890674129694
0.7011357005510875
0.7930731689574381
0.6927712855739557
0.7410039296332298
0.7569462252814383
0.7699843031297051
0.7731837262226046
0.733014617371963
0.7542105404847426
0.7184246777008642
0.7556466771004455
0.7424615612501245
0.7387113958034234
0.751825505773931
0.7621625787732611
0.7202820806235611
0.7402205385026797
0.7619894066727451
0.7229276125139806
0.7583287642200444
0.7738989033534764
0.7410275701731606
0.7590438482709716
0.7395129505920399
0.7331874347096926
0.7419188606314997
0.7549171464041347
0.7002115220451858
0.7383718976753332
0.7463775660098768
0.7370884809309509
0.7380253953845373
0.7035723142796148
0.7621026143797995
0.7053931331536997
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006034376,0.02071644,0.010969124,0.0026167063,-0.0063500097,0.022345519,0.0022569515,-0.002470768,-0.015897084,-0.04040113,0.005912195,0.024639802,-0.003994635,-0.0024792529,-0.010622946,-0.013059774,0.012822201,0.015340483,0.005908801,0.0090481695,-0.0023740416,0.011240637,-0.003203853,-0.00733085,-0.014281582,0.039260775,0.02992073,-0.015666299,0.015150424,-0.022209762,0.015747752,-0.0041948752,-0.016127871,-0.028698921,-0.014675276,0.028373105,0.01073155,0.002197558,0.027463537,0.016806653,0.01684738,0.023160057,-0.0045105093,0.004595357,0.0062006777,0.016155021,0.015408361,-0.0149467895,-0.02333654,0.0096930135,0.0012828988,0.019087363,-0.02168031,-0.0042220266,-0.022359094,0.0016248354,-0.0024809497,0.0046869926,0.009944162,-0.029622065,0.018164217,0.0096930135,-0.018598638,0.017974159,-0.026635423,-0.004208451,0.0064755846,0.000928235,-0.005918983,0.014702427,0.014050797,0.0230243,-0.008722355,-0.017526163,0.01706459,-0.023553751,-0.008593386,-0.0120416,-0.008036784,0.017146043,0.0094215,-0.010541491,-0.011484999,0.016698048,-0.0041813,-0.021938248,0.020825045,0.017770525,-0.034292087,0.009584408,0.009346834,0.026431788,0.017118894,-0.006153163,0.0062821317,-0.005800196,-0.006064921,0.026255304,0.015109697,0.0077177566,-0.010914821,-0.003203853,-0.05924413,-0.018028462,-0.011084517,-0.0075209094,0.011165971,0.0043000868,0.00039157263,0.00036717887,-0.032771617,0.033939123,-0.006838733,-0.033939123,-0.007229033,-0.0010733247,0.03494372,-0.0071679424,-0.0037943937,-0.012516748,0.021313768,0.008430478,0.031305447,-0.024965618,0.024666954,-0.0072833356,-0.017349679,-0.0068760663,-0.023404418,-0.022739211,0.024327563,0.024734832,0.018815849,0.018381428,-0.0240289,0.0334504,-0.013317712,0.017146043,-0.02314648,-0.0088241715,0.011899056,0.02071644,0.0045037214,0.023485873,0.005800196,0.010745126,0.0139286155,0.027558567,0.003096945,-0.009964527,-0.012788261,-0.014770306,0.029160494,0.029432006,0.004249178,0.011804027,-0.005454017,-0.00033684578,-0.012774685,-0.0031190054,-0.009251805,0.024734832,-0.008552658,-0.0092585925,0.022033278,0.013582436,0.024096776,-0.012129842,0.00851872,-0.0009859315,-0.024979195,0.03809327,3.3965636e-05,-0.008864898,0.00076150906,0.013100501,0.023417994,-0.02804729,-0.011987298,-0.0077652712,-0.026906935,0.014580247,-0.009346834,0.003095248,-0.030518059,0.023010725,0.0042322087,-0.02973067,-0.0030307635,-0.0059800735,0.01653514,0.01600569,-0.00038881507,-0.0007924785,-0.6681391,-0.021381646,0.0076023634,-0.019413177,0.007072913,0.016114295,0.011186334,-0.006730128,-0.03806612,0.033233188,-0.021517403,-0.015204727,0.0036145165,-0.029567763,-0.019236695,-0.03996671,0.0004269966,-0.025725855,-0.012265598,0.0053521995,-0.011593604,0.0014016858,-0.006458615,0.016752351,-0.0010342947,0.007867089,0.0043068747,-0.024952043,0.021748189,-0.0073647895,0.0059325583,0.023920294,0.0053454116,0.0001531503,0.023051452,-0.037658848,-0.026133124,0.010195312,-0.002628585,0.032581557,-0.025359312,-0.017431132,0.00883096,0.00471075,-0.021802492,0.017336104,0.019521782,0.0061735264,0.010453249,0.010310705,-0.005806984,0.014783882,0.013005472,-0.011179547,0.01812349,-0.023838839,0.024341138,-0.013324499,0.011641119,-0.019290997,0.016358657,-0.0027270084,-0.0293234,-0.0020007112,0.002781311,0.013942191,-0.0069371564,0.0017682282,-0.005375957,-0.031305447,0.022535577,0.011261,-0.00025284645,0.012998683,0.016521564,-0.0011759906,0.04205736,0.011688634,0.018666517,0.003977665,0.016698048,-0.015245453,-0.01912809,0.0020414381,0.008342236,0.0012973229,0.008763081,-0.0020618017,0.003940332,-0.0060513453,0.027273478,0.028210199,-0.0027202207,-0.025169253,-0.011994085,0.017431132,0.0030087032,0.009564045,-0.0007687211,-0.043822195,-0.0044358433,0.0047345073,0.031332597,0.022236913,0.020920075,0.007982481,-0.01756689,-0.0030358545,0.016507989,-0.024979195,0.016983137,-0.005460805,-0.015394785,0.0036382738,-0.0052130492,-0.024300411,0.014471642,0.01787913,0.008695203,-0.0043442077,0.008993867,-0.005379351,0.0162229,-0.009054958,0.0008315085,0.0066283103,-0.015421937,-0.0135688605,0.00857981,-0.010541491,0.015028243,0.0063907364,0.019793296,-0.025535796,-0.0030426423,0.012442082,0.008253994,-0.007826362,0.016955985,-0.0014865336,-0.027341357,0.015421937,-0.0010147798,-0.020472078,-0.021598857,-0.013222682,0.006251586,0.009849133,0.0083150845,-0.0067776428,0.005925771,-0.023200784,-0.005759469,-0.00032263377,0.0011972025,0.003332822,-0.004955112,-0.033721913,0.0067776428,-0.026839057,0.011987298,0.038609143,-0.023540175,-0.019820448,-0.004238996,-0.024612652,-0.007853513,0.034617905,-0.004188088,-0.013378802,0.007819573,-0.015992114,0.0073647895,0.016032841,-0.017947007,0.0032360952,-0.00982877,-0.019005908,-0.005518501,-0.02134092,0.012367416,0.0031156114,-0.015028243,0.0013422923,0.03537814,0.02102868,0.006265162,0.021164436,-0.028536014,0.0272056,0.00015368058,0.0137046175,-0.021422373,0.03453645,0.020295596,0.013786071,0.0313869,-0.013243046,-0.020784318,0.017431132,0.027694324,0.025848035,0.02249485,-0.010955549,-0.002282406,-0.011023426,0.003994635,-0.010134222,0.021408798,0.020404201,0.027164873,0.011335666,-0.0067504914,-0.012347053,0.005990255,0.0025335555,-0.008396539,0.022223337,-0.011159183,0.0029527035,0.0091839265,-0.004683599,0.044881094,0.009747316,-0.012129842,-0.01213663,-0.0018496822,0.024422592,0.034400694,-0.0023299207,-0.0036077288,-0.017010288,-0.012842564,0.012313114,0.0041168155,0.0076906052,0.00027193723,-0.0017936826,0.033803366,-0.008213268,0.015408361,0.013562073,-0.00034299726,-0.014838184,0.022114731,-0.0004163906,0.029540611,0.026323183,0.0042661475,0.0028797344,0.0075141215,-0.008606961,-0.0030426423,0.021015104,-0.002245073,-0.020648561,0.011661482,0.009401137,0.005454017,0.030219395,0.029622065,0.014607398,0.017118894,-0.00549135,0.032608707,-0.022847816,-0.028780375,-0.00066605525,-0.015639147,0.0018988939,0.0030731875,0.008729142,0.0073512136,-0.023065027,0.011518938,0.0027507658,0.034617905,0.014091523,0.00945544,0.026391061,-0.025657976,-0.017756948,0.009686225,0.008613749,-0.0125371115,-0.0049958387,-0.011410333,0.0105007645,-0.020119112,0.032038532,0.0037570607,0.033803366,-0.017322527,-0.008138602,-0.009611559,0.00094775,0.025820885,-0.017851979,-0.009910224,-0.008613749,-0.0038588783,0.0065943715,-0.012896867,-0.013724981,0.008213268,-0.0018378034,-0.006533281,-0.036871463,0.018883727,-0.010290341,0.026214577,-0.02336369,0.0017987734,0.0006987217,0.025142102,0.022114731,-0.005291109,0.02030917,0.028345956,0.020390624,0.00039263323,-0.027409235,-0.00895314,0.0038894233,0.0075141215,0.02701554,0.0050128084,0.022169035,-0.010548279,-0.008125026,0.00028339168,0.002167013,0.019494632,0.009720164,-0.0146617,-0.009550469,-0.0070118224,0.0034719722,0.015964963,-0.00998489,0.0055829855,-0.038581993,-0.034373544,-0.01956251,-0.02189752,0.013887889,0.017200347,0.024354713,0.0014475036,-0.001968469,0.0021907703,0.0094215,0.0034889418,0.0030273697,0.018883727,0.020472078,0.022359094,0.005494744,-0.013982918,0.01925027,0.014458066,-0.0039539076,-0.014498793,0.023757385,-0.003821545,-0.013175167,-0.0018293187,0.013059774,-0.011681845,-0.019684691,0.0029577946,-0.011383181,0.04336062,0.0057764384,0.009401137,0.00096471957,-0.03057236,-0.025196403,-0.005016202,-0.0130461985,-0.013222682,-0.042491782,-0.0155305425,-0.020078385,-0.021829642,-0.018422155,0.038853507,-0.0063907364,0.017729796,0.0031716109,-0.0245855,0.0051892917,-0.032690164,-0.012883291,-0.0036857887,-0.02773505,-0.046211507,-0.016874531,0.0050569293,0.026771178,-0.0035703955,-0.013127652,-0.0041813,0.01569345,-0.0072901235,-0.0075276974,0.015992114,-0.011016639,-0.007079701,0.019413177,-0.032364346,0.0007271457,0.0065875836,0.027056267,0.0056338944,0.026757604,-9.5294694e-05,-0.041785847,0.007188306,-0.0072969114,-0.022657758,0.020825045,-0.0044596004,-0.01958966,0.0072969114,-0.010941973,-0.0029645823,-0.011939783,-0.021395223,0.0008535689,0.007582,-0.015707025,0.012333477,-0.01747186,-0.0048804455,-0.011505363,0.012380992,-0.011247425,0.021612434,0.0069778836,0.018245673,0.011505363,-0.0010919912,0.00038436055,0.0028050684,-0.01600569,0.0050806864,0.010222464,-0.022535577,0.0050705047,-0.0022314973,0.0017113802,-0.014607398,0.01506897,-0.024571924,0.032581557,-0.014580247,0.00019355906,-0.02093365,-0.005148565,-0.01850361,0.0088988375,-0.0028729467,-0.006407706,0.0009918709,0.010371796,-0.018639365,0.0070118224,-0.007826362,-0.015191151,-0.017512588,0.02336369,0.00920429,0.028155895,-0.009340046,0.01229275,-0.024232533,-0.031712715,-0.022752788,-0.017200347,-0.005192686,-0.0030799753,0.0019379238,0.026499666,0.035703957,-1.7513117e-05,0.034183484,0.009957738,-0.01597854,0.017512588,-0.007941755,0.008131813,-0.022413395,0.0015052,0.0139286155,0.0025708885,0.028074441,0.0045682057,0.015625572,-1.6691154e-05,-0.018245673,-0.010073131,-0.014811032,-0.012815412,-0.0019769538,0.0086409,-0.009604772,-0.0025912519,-0.022128308,0.005613531,-0.018490033,-0.01715962,0.0008209025,0.0042729354,0.006346616,-0.0034889418,0.008349024,0.0030358545,0.003913181,-0.025196403,-0.036545645,-0.017322527,-0.010181736,0.004754871,-0.0076023634,0.02398817,0.01631793,0.012571051,-0.014865335,-0.013969342,-0.02783008,-0.013874313,0.0048091738,-0.010785853,-0.002436829,-0.021408798,-0.026825482,0.015354059,0.01260499,0.0046869926,-0.025522219,0.004140573,0.004059119,-0.017770525,0.009475803,0.01432231,0.02415108,0.012482809,0.022236913,0.019223118,-0.025386464,-0.00066775223,0.0022230125,0.00726976,0.00064399483,0.028183047,0.022087581,-0.010385372,-0.020784318,-0.004059119,0.014770306,-0.016806653,-0.017919857,0.031359747,0.011885481,-0.021273041,-0.0064857663,-0.015218303,-0.026689725,-0.008647689,-0.0076838173,0.0012498081,-0.0050399597,-0.0032564588,0.004860082,-0.0086001735,0.0086001735,0.02168031,-0.002318042,-0.011912632,0.012557475,0.009754104,-0.006920187,-0.0021330738,0.012170569,0.02290212,0.024409017,0.020567108,-0.017947007,-0.00533523,-0.0028033715,0.0076973927,-0.014783882,0.028318804,-0.019168817,0.012930806,0.0058544986,-0.0035228808,0.0051451707,-0.009808406,-0.015489815,-0.008186116,-0.0077788467,0.0064382516,0.03176702,-0.0022298002,-0.023051452,-0.0068014,-0.0077177566,-0.02427326,-0.014824608,0.005409896,-0.011546089,-0.037930362,-0.0028424014,-0.016480837,0.011111668,0.023499448,0.0044222674,-0.010602581,0.013582436,0.015136848,-0.027368508,0.0019599842,0.007914604,-0.00086629606,-0.01453952,-0.0004980566,-0.02480271,-0.022454124,0.011552877,0.01413225,-0.054221142,-0.013080138,0.025888762,0.004211845,0.016236477,0.00037672426,-0.008369387,0.013982918,-0.00047811738,0.009469015,0.009869496,0.030083638,0.0064993417,0.008308297,0.00064950995,0.0036043348,0.0057391054,-0.03054521,-0.0015111394,0.004276329,-0.030300848,0.004934748,-0.0070321863,0.008097875,-0.011817602,-0.0011437484,-0.028508862,-0.012706807,-0.012034813,0.048682276,0.022820666,-0.0293234,-0.029133342,-0.0038928173,0.0039267563,0.015788479,-0.020472078,-0.034590755,0.001890409,-0.017417558,-0.029377704,-0.02280709,-0.013942191,0.037333034,0.010527915,-0.0075887875,-0.025929488,-0.014783882,-0.013799647,-0.013718193,-0.023540175,-0.004720932,-0.009659074,0.02330939,-0.015761329,0.01891088,-0.0080164205,0.0016511383,-0.02184322,0.008566234,-0.0057085603,-0.019114513,0.025603674,0.0015773207,-0.03391197,0.018517185,0.006214253,-0.016616594,-0.0028542802,0.0005544804,-0.020689288,-0.015245453,-0.0044562067,-0.00028657346,0.013256621,0.029540611,-0.002545434,-0.0139286155,-0.012170569,0.025142102,-0.015720602,0.005562622,-0.021761766,0.013331288,-0.007731332,-0.015449088,-0.026404636,-0.012224872,-0.0039606956,0.0009435076,0.0034159727,0.01229275,-0.007656666,0.011899056,0.016182173,0.0035805774,0.0043034805,0.025060648,0.0036145165,0.016250052,0.015313332,-0.01862579,-0.020010507,0.0018496822,-0.022956422,0.00075726665,-0.010582218,-0.007500546,-0.011267789,-0.021110134,0.008606961,0.01925027,-0.03011079,0.023485873,0.00402518,-0.00073732744,0.018435732,0.012061964,0.003957302,-0.011288152,-0.017919857,-0.010086707,-0.018856576,-0.016453687,0.04479964,0.015041819,0.01790628,0.006652068,0.22035994,-0.017241074,0.0024962225,0.028210199,0.021368071,0.0011259303,0.012985108,0.027694324,-0.027069844,0.0034550026,0.009950951,-0.013161591,-0.005104444,0.0037163338,0.007147579,0.0032394892,-0.037142973,-0.03744164,-0.030626664,0.028536014,0.012211296,0.013969342,0.013521346,-0.026499666,0.051533163,-0.001256596,-0.011756511,-0.00054514717,-0.0014729579,-0.004367965,-0.006604553,-0.009177139,0.029594915,0.03706152,-0.010616157,-0.017743373,-0.0034380332,0.0048906277,0.018924454,-0.0032954887,-0.012462446,0.00695752,0.004720932,-0.011437484,-0.00973374,-0.0036484557,-0.009686225,-0.010181736,0.0030579148,0.022359094,-0.016331505,-0.010249615,0.018014885,0.027056267,-0.0015102909,-0.01588351,0.005447229,-0.014675276,0.00486687,-0.0067810365,-0.024341138,0.0073647895,-0.023825264,0.0214631,-0.032608707,0.002160225,-0.032907374,-0.004972081,0.023567326,-0.019698266,-0.032092832,0.00336167,-0.014281582,-0.0031427627,-0.027544992,-0.009726953,0.02802014,0.020974377,0.015734177,0.013473831,-0.0069405506,-0.00547438,-0.011688634,-0.018666517,-0.0024690712,-0.0075276974,0.015177575,-0.01965754,0.012781473,0.0026930694,0.0041846936,0.0016935621,-0.017960584,0.013310923,-0.010636521,0.018652942,0.0013898071,0.022264063,0.013521346,-0.02549507,-0.009272168,0.06130763,0.0076906052,0.006991459,0.0019209543,0.0058511044,-0.0071068523,0.016032841,0.020105535,-0.019073786,0.015231878,-0.050175596,0.002623494,-0.0032089439,-0.012509961,-0.0060988604,-0.0050365655,-0.0019243482,0.0023350115,-0.01937245,-0.0028033715,-0.024409017,-0.021273041,0.005348806,0.011301728,-0.019711843,-0.02549507,-0.001890409,-0.008695203,-0.00281525,-0.014227279,-0.011220274,0.0021941643,0.016698048,-0.012401355,-0.025454342,-0.01157324,-0.026228154,0.006221041,0.0094825905,0.0024911317,-0.010473613,0.0038385147,-0.022820666,0.0073580015,-0.013894676,0.016046416,0.01198051,0.004754871,-0.015992114,0.0012947775,0.0034108819,-0.013657102,-0.01593781,0.021951824,-0.006034376,-0.023716658,-0.015245453,-0.005637288,-0.003332822,-0.017051015,-0.02717845,0.01999693,0.013114077,-0.028400257,-0.008511932,-0.17507157,0.011491787,0.019738993,-0.051750373,0.0140779475,0.027599294,0.018856576,0.01581563,-0.034617905,-0.011518938,0.021490252,-0.022318367,-0.002010893,-0.011919419,-0.004832931,-0.011328879,-0.013792859,0.014281582,0.013996494,0.014620974,0.05267352,-0.01528618,-0.007921391,0.01198051,-0.017485436,-0.018707244,0.011546089,0.033233188,0.013351651,-0.051560313,-0.017213922,-0.012842564,0.03187562,0.020282019,-0.011871905,0.004595357,0.018734396,-0.026771178,-0.022535577,0.0076770294,-0.005172322,0.013955767,-0.008939565,0.0054574106,-0.016290778,0.004014998,0.03640989,0.0018327126,0.0011445968,-0.011301728,0.0143630365,-0.005718742,0.007242609,-0.005830741,0.002015984,0.0095436815,0.01747186,-0.006119224,-0.0058477107,-0.020784318,-0.020336322,-0.012618566,0.014172978,-0.0038826356,0.01463455,0.014227279,-0.014281582,0.020472078,-0.037197277,0.018395005,0.015055395,-0.018177794,-0.0040353616,-0.0019158635,3.065127e-05,0.026608272,0.0054777744,0.0094825905,0.028753225,0.00019430148,-0.010602581,0.040944155,-0.020852197,0.022264063,-0.006071709,-0.010378583,0.0024622832,0.033993423,-0.012706807,-0.028291652,0.010134222,-0.040536888,0.020797893,-0.00236386,0.017770525,0.012380992,0.005756075,-0.018897302,0.008756294,-0.008104662,-0.020200565,-0.0025216767,-0.009774467,0.021245891,0.018612215,-0.011525726,-0.01444449,0.0146617,0.037115823,0.003784212,-0.009190714,-0.015014668,0.0048125675,0.011247425,0.0031122174,0.0127611095,-0.010039193,-0.0024062837,-0.019046634,0.023282237,0.047107503,0.0095572565,-0.015747752,0.014878911,-0.013080138,-0.016630169,-0.09540966,-0.004276329,0.0177841,0.021273041,0.007378365,0.043822195,-0.00012578687,0.0126117775,-0.042274572,0.024259685,-0.043876495,-0.018802274,0.0058171656,-0.004093058,-0.0031105205,-0.011539302,0.0009765982,-0.025345737,-0.022413395,0.028563164,0.0034685782,-0.049008094,0.0003273004,-0.01004598,0.018109916,-0.02436829,-0.02155813,0.010371796,-0.0010164767,-0.00086841726,0.018028462,-0.038989265,-0.023608053,-0.016059993,-0.0100663435,0.02482986,-0.01850361,0.0040285736,0.004978869,0.0007742362,0.022671333,-0.0010444765,-0.02206043,0.00012271114,-0.021015104,-0.011905844,-0.023675932,0.028780375,0.0024062837,-0.011104881,-0.027558567,-0.0051757162,-0.009896648,-0.004561418,-0.022359094,0.008036784,0.009448651,0.030056486,0.015544117,0.0172818,-0.0139286155,0.011132032,0.0059020133,0.023268662,0.022046853,-0.01660302,-0.023675932,-0.00046920837,0.007229033,-0.0058511044,-0.022481274,-0.0008739324,-0.00017924099,-0.005172322,-0.0334504,-0.0071272156,-0.014091523,-0.002056711,-0.0030104,-0.009862709,0.009156775,-0.0088988375,0.0065536443,-0.004395116,-0.023675932,0.019426754,0.0146617,-0.017811252,-0.026784755,-0.017743373,-0.012428506,0.021856794,0.01354171,-0.025657976,0.0056271064,0.017743373,0.007941755,-0.0075209094,0.00463269,0.010161373,-0.0083626,-0.0034855478,-0.04561418,0.02112371,-0.0012404749,-0.02511495,-0.008416902,-0.018775122,0.011715785,-0.010908034,-0.0015934418,0.010147798,-0.012224872,0.0046869926,-0.031305447,0.01840858,-0.0182864,0.0049585057,0.011613968,-0.020784318,0.007378365,0.0012328386,-0.011464636,0.012476021,0.010813004,0.023173632,-0.01073155,-0.039477985,-0.028427409,0.0091839265,-0.009373985,0.008688415,0.021639584,-0.0081657525,0.004690387,0.028074441,-0.009740528,-0.00832866,0.013066562,0.0064687966,0.03510663,-0.02427326,-0.005952922,-0.04018392,0.00031457323,-0.007839937,0.03217429,0.009278956,0.002436829,0.000598177,-0.017308952,-0.0019939234,0.021924673,0.0071272156,-0.003828333,-0.033803366,-0.011756511,0.009061745,-0.011281364,-0.016494414,0.020064808,0.0019413178,0.02948631,-0.01631793,0.023377268,-0.008199692,0.004714144,0.00077678164,0.0007504788,0.021069407,0.013765708,-0.025603674,-0.00052096555,-0.007894239,0.0058986195,0.037197277,0.031495504,0.015652724,0.010521128,0.0024979194,-0.007242609,0.041107066,0.031495504,-0.0051383832,-0.026635423,-0.015394785,0.011742936,0.006207465,0.013589225,0.011050578,0.0026167063,0.004870264,-0.010161373,0.0036280921,-0.0024809497,0.005562622,0.0009511439,0.0027134328,-0.0050195963,-0.010677247,-0.0062719495,0.015856357,-0.0054268655,0.001063143,-0.0140779475,-0.021381646,-0.039613742,-0.0031240962,-0.027327782,-0.02583446,-0.024409017,0.005192686,-0.0018276217,-0.0026506453,-0.0015824116,0.02121874,-0.010344644,0.025671551,-0.015191151,-0.014702427,-0.03198423,0.018422155,0.0092585925,-0.0038385147,0.009726953,-0.0012328386,0.035948317,0.004860082,0.019114513,-0.029187646,0.033586156,0.015666299,0.013005472,0.0015705329,-0.00020310443,-0.011070942,-0.009652286,-0.029024737,-0.003246277,0.0048872335,-0.0041202093,0.06760673,-0.0017563496,0.009815194,0.031061085,0.008613749,0.026065245,0.021761766,0.0060547395,-0.0045240847,-0.017594041,0.013521346,-0.007419092,-0.020282019,-0.017078167,0.013236257,-0.0018768335,0.002974764,0.021544555,-0.016860956,-0.018707244,-0.0011488392,-0.0058409227,0.0015306544,0.011715785,0.010215675,0.007412304,0.01413225,0.006689401,-0.011702209,-0.039586592,0.002474162,0.0240289,-0.041704394,-0.010738338,-0.0056338944,0.017539738,-0.0074462434,-0.010602581,0.02009196,-0.0095572565,0.0003610274,0.0016307748,-0.016616594,-0.01790628,-0.018177794,0.006767461,0.00018284701,-0.013663891,-0.025182828]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.006133381,0.020704828,0.010922645,0.0026101659,-0.0063166698,0.02233406,0.0021875838,-0.0023980264,-0.01591217,-0.04040496,0.0059263324,0.024614986,-0.003913552,-0.0025490697,-0.010637529,-0.013156051,0.01285057,0.015382669,0.005977246,0.009049028,-0.0023963293,0.011275645,-0.0032618588,-0.0073586996,-0.014282937,0.039291654,0.029896414,-0.015749246,0.015151861,-0.022225445,0.015762823,-0.004205456,-0.016156554,-0.028647335,-0.014690245,0.028348643,0.0107597215,0.0022418916,0.027452564,0.016808247,0.016889708,0.023162253,-0.004500754,0.004612764,0.0061096214,0.016210862,0.015382669,-0.01493463,-0.023433791,0.00970751,0.0013084773,0.019102748,-0.021655213,-0.004191879,-0.02229333,0.0015927443,-0.0024421513,0.0046772547,0.010026568,-0.02965203,0.018111633,0.009761817,-0.018559672,0.01798944,-0.026570063,-0.004161331,0.006530506,0.0009062605,-0.0059840344,0.014717399,0.014038552,0.023026483,-0.008723182,-0.017541401,0.017120516,-0.023447368,-0.00868245,-0.0121309925,-0.007915353,0.017188402,0.009469913,-0.010528914,-0.011404626,0.016686054,-0.0042258212,-0.021913175,0.020745559,0.017731478,-0.034268185,0.00948349,0.009374875,0.026407141,0.017120516,-0.006150352,0.0063064867,-0.0058279,-0.006075679,0.026244218,0.015056822,0.007732065,-0.010936222,-0.0032262194,-0.05919544,-0.01798944,-0.011146665,-0.0075080455,0.011187395,0.0042903116,0.00042258215,0.00038206347,-0.032801878,0.033833724,-0.00684617,-0.033860877,-0.007263661,-0.0009996019,0.03491988,-0.007209353,-0.003842273,-0.012490781,0.021315789,0.008431277,0.031308416,-0.024995139,0.02464214,-0.007236507,-0.017337747,-0.0068258047,-0.023420215,-0.022700638,0.02446564,0.024655716,0.01879048,0.018410325,-0.0240176,0.033372108,-0.013305398,0.017052632,-0.0231351,-0.008831797,0.011879819,0.020704828,0.004453235,0.023501676,0.0056955246,0.010698626,0.013895994,0.027629064,0.0030785704,-0.009985836,-0.012857359,-0.014812437,0.029217567,0.029461952,0.0043106773,0.011798358,-0.005485082,-0.00038142706,-0.012823416,-0.0031125126,-0.009116912,0.024764331,-0.008512739,-0.009279836,0.022103252,0.013556571,0.024071908,-0.012144569,0.008472008,-0.0010276043,-0.025022293,0.03823265,-1.186656e-05,-0.008750335,0.000692848,0.013189994,0.023406638,-0.027995642,-0.011940915,-0.007861046,-0.026950218,0.014581629,-0.009327355,0.003088753,-0.030520953,0.022917869,0.0042733406,-0.029760644,-0.0030140798,-0.0059500923,0.0164824,0.016047938,-0.00046119155,-0.0007382459,-0.66820246,-0.021410828,0.007603084,-0.019333556,0.00704643,0.0161294,0.011221338,-0.0066493046,-0.03801542,0.033209186,-0.021587329,-0.015233322,0.0036046766,-0.029543413,-0.019224942,-0.039943345,0.00047349566,-0.025755448,-0.01223282,0.005454534,-0.011594703,0.0014544292,-0.0064286795,0.01675394,-0.0010377871,0.0078338925,0.0042937063,-0.024927255,0.021763828,-0.007440161,0.005960275,0.023936138,0.0053391303,0.00011466147,0.02299933,-0.037608113,-0.026122024,0.010155548,-0.002552464,0.032503184,-0.025334563,-0.017405633,0.008811432,0.00469762,-0.02175025,0.017337747,0.019523634,0.0062487847,0.010440663,0.010345626,-0.005817717,0.014812437,0.013088167,-0.011187395,0.018043747,-0.023854677,0.024275562,-0.013325763,0.011737261,-0.019252095,0.016346632,-0.0027646036,-0.02938049,-0.0019907183,0.0027798777,0.013929936,-0.0069717565,0.0017157852,-0.0053391303,-0.031226953,0.022537714,0.011282434,-0.00018074297,0.012952397,0.016495977,-0.0012176814,0.041952733,0.011676165,0.018681863,0.0038796095,0.01675394,-0.015301207,-0.019184211,0.0021281848,0.008472008,0.0012261671,0.008743547,-0.0020467232,0.003961071,-0.0060281595,0.027316796,0.02818572,-0.0027476323,-0.02517164,-0.011934127,0.0173649,0.0030242626,0.009510644,-0.0007840681,-0.04382635,-0.004443052,0.004717985,0.03133557,0.022225445,0.020894906,0.007990027,-0.017568555,-0.0030870559,0.016509553,-0.025022293,0.017025478,-0.0054443516,-0.015301207,0.0036386189,-0.0053187646,-0.024316292,0.014554475,0.017894402,0.00872997,-0.004303889,0.009001508,-0.005407015,0.016170131,-0.008967566,0.0007692183,0.0066798525,-0.01546413,-0.013563359,0.008458431,-0.010610376,0.014948207,0.006350612,0.019849481,-0.02552464,-0.003048022,0.012409319,0.008268354,-0.007908565,0.016998325,-0.0015486192,-0.02734395,0.015450553,-0.0009928135,-0.020419711,-0.021587329,-0.013230724,0.006296304,0.00983649,0.008349815,-0.006829199,0.0060519194,-0.023257291,-0.0057532266,-0.0002849035,0.0011582824,0.0033331378,-0.004955582,-0.033643648,0.0067782854,-0.026882334,0.0120088,0.038612805,-0.023610292,-0.019849481,-0.004178302,-0.02458783,-0.007874623,0.034621187,-0.004164725,-0.013312186,0.007915353,-0.015966477,0.0073722764,0.01599363,-0.01798944,0.0032448878,-0.009850067,-0.018912671,-0.0054918705,-0.021342944,0.0124161085,0.0030531136,-0.014988937,0.0014001215,0.035354342,0.020949213,0.006275939,0.02118002,-0.028538719,0.027085988,0.00017331807,0.013699128,-0.021397252,0.034458265,0.020311097,0.013875629,0.03141703,-0.013278243,-0.020772712,0.017432787,0.027588334,0.025823332,0.022442676,-0.010949799,-0.0023097761,-0.010990529,0.004018773,-0.01014876,0.021410828,0.020460444,0.027194602,0.011302799,-0.0067647086,-0.01232107,0.0060247653,0.0024998533,-0.008390547,0.022252599,-0.011112723,0.002871522,0.0092051625,-0.00469762,0.044912506,0.009755028,-0.012042742,-0.012069896,-0.0018108238,0.024424909,0.034376804,-0.0022860167,-0.0035910995,-0.017025478,-0.012911666,0.012287127,0.0041579367,0.007657392,0.00028702492,-0.0018447662,0.03388803,-0.008207258,0.015382669,0.013556571,-0.00039097332,-0.014826015,0.022062521,-0.00036000094,0.029570566,0.026434295,0.0042393985,0.0028545507,0.007480892,-0.008648508,-0.002988623,0.021017097,-0.0021807954,-0.020772712,0.011642222,0.009463125,0.0053968322,0.030167952,0.02959772,0.0146223605,0.017066209,-0.005468111,0.032638956,-0.022890715,-0.028783105,-0.00065254146,-0.015749246,0.0017921556,0.0030989356,0.008607778,0.0073451223,-0.023148676,0.011452146,0.0028528536,0.034539726,0.014106437,0.009503855,0.026352832,-0.025606101,-0.017785786,0.00970751,0.008621355,-0.01258582,-0.00505062,-0.011452146,0.01050176,-0.020134596,0.03212303,0.0037200805,0.033860877,-0.017297016,-0.008071489,-0.009585317,0.000964811,0.025769025,-0.01794871,-0.009911164,-0.008580623,-0.0037947537,0.006584814,-0.013013493,-0.013597301,0.008207258,-0.0018108238,-0.0066051795,-0.03695642,0.018980557,-0.010257375,0.026203487,-0.023406638,0.0017463334,0.00075055,0.025185216,0.022076098,-0.005308582,0.020243213,0.028321488,0.020365404,0.00042003646,-0.027398257,-0.009015085,0.0038897921,0.0073586996,0.026950218,0.0049420046,0.022143982,-0.010447453,-0.008146161,0.0003362413,0.0021689155,0.01945575,0.0096396245,-0.014730976,-0.009551374,-0.0069106603,0.0034892727,0.015925746,-0.010006201,0.0056513995,-0.03850419,-0.034458265,-0.019442173,-0.021818137,0.013862052,0.017201979,0.024302715,0.0014790375,-0.00197205,0.0021926751,0.00939524,0.0035978882,0.003048022,0.01887194,0.02051475,0.022347637,0.0054748994,-0.013997821,0.019252095,0.014527322,-0.0039101574,-0.014540899,0.023732483,-0.0038490614,-0.013183205,-0.0018277951,0.013108532,-0.0116965305,-0.019700134,0.0028766133,-0.0114114145,0.043391887,0.005763409,0.009476702,0.0010080875,-0.03057526,-0.025185216,-0.0049521876,-0.01302707,-0.013203571,-0.042522963,-0.015586322,-0.020107443,-0.02175025,-0.018451056,0.038965806,-0.00638116,0.017745055,0.003249979,-0.024574254,0.0051592356,-0.032693263,-0.012891301,-0.0036793498,-0.027683374,-0.04621589,-0.016808247,0.005060803,0.02671941,-0.0034926669,-0.013135686,-0.004205456,0.015722092,-0.007338334,-0.007440161,0.015966477,-0.0110108955,-0.0070396415,0.019387865,-0.03239457,0.0007403673,0.006594997,0.026977371,0.005644611,0.026787294,-7.20214e-05,-0.041708346,0.007209353,-0.0072908145,-0.022700638,0.02086775,-0.004456629,-0.01959152,0.0072772377,-0.010895492,-0.0029343152,-0.011995223,-0.021437982,0.00082522316,0.007643815,-0.015708515,0.012355012,-0.017473517,-0.0049080625,-0.011418203,0.012429685,-0.01125528,0.021614483,0.0069785453,0.018220248,0.011452146,-0.0010759722,0.00043615908,0.0028307913,-0.015939323,0.00507438,0.010135183,-0.02255129,0.005047226,-0.0022130406,0.0017149367,-0.014527322,0.015070399,-0.024628563,0.03247603,-0.014568052,0.0001911378,-0.020935636,-0.0051422645,-0.018505365,0.008899681,-0.0028834017,-0.0064286795,0.0010208159,0.010352414,-0.018586826,0.007080372,-0.007854258,-0.015192592,-0.017446363,0.02335233,0.009137278,0.028212873,-0.009320566,0.012266762,-0.024180524,-0.03174288,-0.022809252,-0.01714767,-0.005132082,-0.0030819646,0.0019228336,0.02654291,0.03568019,5.7012523e-05,0.03421388,0.009958683,-0.015966477,0.017487094,-0.00792893,0.008152951,-0.0224291,0.0015554078,0.013943513,0.0025762236,0.028131412,0.004555062,0.015586322,-7.5442154e-06,-0.01820667,-0.01010124,-0.014812437,-0.012687647,-0.0018837999,0.008723182,-0.009605682,-0.00253719,-0.022116829,0.005546178,-0.018437479,-0.017188402,0.00080485776,0.0042903116,0.0063302466,-0.0035367918,0.008410912,0.0029360123,0.003859244,-0.02521237,-0.036386188,-0.017297016,-0.010243799,0.0047247736,-0.007541988,0.023990447,0.016400939,0.012572243,-0.014866745,-0.013984244,-0.027751258,-0.013943513,0.004918245,-0.010827607,-0.0024302716,-0.021397252,-0.02685518,0.015355514,0.012612973,0.004738351,-0.025511064,0.0041206,0.003961071,-0.017731478,0.009551374,0.014323668,0.024112638,0.012375377,0.022171136,0.019184211,-0.025443178,-0.0006970908,0.0022232232,0.0073179686,0.00054053176,0.02818572,0.022143982,-0.010399933,-0.020840596,-0.0040968405,0.014812437,-0.016686054,-0.017921556,0.031389877,0.011845876,-0.021356521,-0.0065033524,-0.015301207,-0.026705833,-0.008702816,-0.007684546,0.0012422897,-0.005033649,-0.0032618588,0.00487412,-0.008594201,0.008600989,0.021641636,-0.0023420216,-0.011859453,0.012545089,0.009822913,-0.006873324,-0.0021638242,0.012110627,0.022863561,0.024411332,0.02047402,-0.01794871,-0.005308582,-0.002756118,0.0076641804,-0.014771706,0.028321488,-0.019211365,0.012891301,0.005885602,-0.0035367918,0.005108322,-0.0098568555,-0.015504861,-0.008180104,-0.0077524306,0.006425285,0.03174288,-0.0022724397,-0.02304006,-0.0068427757,-0.007738854,-0.024275562,-0.014785283,0.005485082,-0.011547185,-0.0378525,-0.0029054643,-0.016468823,0.011092356,0.023488099,0.004439658,-0.010685049,0.013509052,0.015138283,-0.027289642,0.0020518145,0.007942508,-0.0008044335,-0.014554475,-0.00062708475,-0.024832217,-0.02241552,0.0116286455,0.014120013,-0.054253437,-0.0131017435,0.025823332,0.00414436,0.016292322,0.00043488623,-0.008275143,0.013929936,-0.00040730808,0.009497067,0.009884009,0.030086491,0.006472804,0.0083022965,0.000735276,0.0035503688,0.005726073,-0.030711029,-0.0014883716,0.004222427,-0.030303722,0.004931822,-0.007032853,0.008152951,-0.011879819,-0.001145554,-0.028430104,-0.01276232,-0.01209705,0.04868689,0.022917869,-0.029299028,-0.029054644,-0.0039339173,0.003964465,0.0157764,-0.020460444,-0.03456688,0.0018159152,-0.017432787,-0.02938049,-0.022836406,-0.013957091,0.037336573,0.01050176,-0.0076641804,-0.025850486,-0.014812437,-0.013902782,-0.013651609,-0.023528831,-0.004741745,-0.009693933,0.023338754,-0.015830707,0.019021288,-0.008078277,0.0016928742,-0.021818137,0.008668873,-0.0057226787,-0.019116325,0.025673985,0.0015808645,-0.03388803,0.018518941,0.0062827272,-0.01657744,-0.0028766133,0.0005825354,-0.020555481,-0.015274053,-0.0044328696,-0.00024014206,0.013278243,0.029461952,-0.0025813149,-0.013916359,-0.012212454,0.025158063,-0.015708515,0.005583515,-0.021736674,0.013332551,-0.007677757,-0.015436976,-0.026325678,-0.012226031,-0.003988225,0.0010250587,0.0033857485,0.012300704,-0.0076302383,0.011845876,0.016156554,0.0036284362,0.004283523,0.025063025,0.0037098979,0.016265169,0.015314784,-0.018559672,-0.019958096,0.0018651315,-0.022931445,0.0007917051,-0.010549279,-0.0075691417,-0.011282434,-0.021112137,0.008512739,0.019292826,-0.030167952,0.023488099,0.004018773,-0.0007085464,0.018437479,0.012069896,0.0039033692,-0.01120776,-0.01794871,-0.01010124,-0.01883121,-0.016468823,0.044776734,0.014975361,0.017921556,0.006554266,0.22048944,-0.017351324,0.002479488,0.028212873,0.021356521,0.0010929434,0.012979551,0.027642641,-0.027099565,0.0035435804,0.009877221,-0.013108532,-0.005013284,0.0037234747,0.0071143145,0.003336532,-0.036983576,-0.037418038,-0.03057526,0.028538719,0.012219243,0.013984244,0.0135362055,-0.026515756,0.051565204,-0.0013729676,-0.011744049,-0.0006317518,-0.0014714005,-0.004348014,-0.0065610544,-0.009232316,0.029679183,0.03695642,-0.010556068,-0.017731478,-0.0034553302,0.0049080625,0.018885517,-0.0032822243,-0.012524724,0.0069853337,0.004616158,-0.011397838,-0.009877221,-0.0036352247,-0.009673567,-0.010203067,0.0030276568,0.022401944,-0.016346632,-0.010277741,0.017975863,0.027018104,-0.0014408523,-0.015966477,0.0055054477,-0.014649514,0.004860543,-0.0067952564,-0.024343446,0.007324757,-0.023868253,0.021356521,-0.032693263,0.0021112135,-0.0329648,-0.005013284,0.023555985,-0.019727288,-0.032068722,0.0033585946,-0.014310091,-0.0031736088,-0.027493296,-0.00970751,0.02804995,0.020976366,0.015803553,0.013495474,-0.0069785453,-0.0055020535,-0.011682954,-0.018668287,-0.0024540313,-0.0074469494,0.015165438,-0.019523634,0.012687647,0.0026305313,0.004130783,0.0016886315,-0.01798944,0.013352917,-0.010746145,0.01865471,0.0013313883,0.022252599,0.013549782,-0.02556537,-0.009266259,0.06120483,0.0076913345,0.0069378144,0.001900771,0.005933121,-0.0071007377,0.016047938,0.020066712,-0.019102748,0.015355514,-0.050180357,0.0026407142,-0.003207551,-0.012463627,-0.006075679,-0.005077774,-0.0019347135,0.0022995935,-0.01941502,-0.0027646036,-0.024411332,-0.021207174,0.005390044,0.011309587,-0.019713711,-0.025470331,-0.0019533816,-0.00864172,-0.0028511565,-0.014215052,-0.011146665,0.002231709,0.0166589,-0.012456839,-0.025470331,-0.011533607,-0.026257794,0.0062589673,0.009456336,0.0024896706,-0.010399933,0.0038626383,-0.022809252,0.007379065,-0.013834897,0.016061515,0.011954492,0.0047688987,-0.016034361,0.0013110229,0.0034095082,-0.013624456,-0.016047938,0.02188602,-0.006017977,-0.023718907,-0.01528763,-0.005641217,-0.0033314407,-0.017079785,-0.027181026,0.020080289,0.013067801,-0.02840295,-0.0085195275,-0.17508817,0.011526818,0.019700134,-0.051782433,0.014106437,0.027466143,0.018858364,0.015830707,-0.03456688,-0.011533607,0.021478713,-0.02229333,-0.0019652615,-0.011954492,-0.004833389,-0.011200972,-0.013794167,0.014201475,0.013984244,0.014581629,0.05267851,-0.015328361,-0.007942508,0.012002012,-0.017554978,-0.01879048,0.011553973,0.033263493,0.013359705,-0.051592357,-0.01724271,-0.012870936,0.031932954,0.020243213,-0.011805146,0.0045856102,0.01874975,-0.026787294,-0.022578444,0.007738854,-0.005193178,0.013957091,-0.008967566,0.0055020535,-0.016238015,0.004107023,0.036440495,0.0019398048,0.0011523424,-0.0112620685,0.014377975,-0.0057091014,0.0072297184,-0.0058652363,0.00197205,0.009503855,0.017473517,-0.0061876886,-0.0057769865,-0.020799866,-0.02033825,-0.012551878,0.014106437,-0.0038490614,0.014635937,0.014215052,-0.014255783,0.020446865,-0.03717365,0.018464632,0.015029668,-0.018098056,-0.0040255617,-0.0019465932,7.907505e-05,0.026502179,0.0054613226,0.009551374,0.028783105,0.00023971777,-0.010678261,0.040866576,-0.020799866,0.022252599,-0.0060654962,-0.010393145,0.00243706,0.034023803,-0.012646916,-0.028294334,0.01010124,-0.04051358,0.020718405,-0.0022639541,0.017826516,0.012395742,0.0057939575,-0.01887194,0.008696028,-0.008119008,-0.020311097,-0.002566041,-0.009782182,0.021207174,0.018559672,-0.011540396,-0.014364398,0.014649514,0.037119344,0.003740446,-0.009144067,-0.015029668,0.0047621103,0.011200972,0.0030955414,0.012728377,-0.01006051,-0.0024998533,-0.018939827,0.023298023,0.04700335,0.009666778,-0.015735669,0.014853168,-0.013088167,-0.016550286,-0.09541871,-0.004324254,0.017772209,0.021207174,0.007365488,0.043853503,-0.00017162095,0.012531512,-0.04227858,0.024207678,-0.04390781,-0.018858364,0.0057939575,-0.004059504,-0.0030921472,-0.011553973,0.00091220037,-0.025416024,-0.022442676,0.028565874,0.0033891427,-0.049067046,0.00029932903,-0.010046933,0.018193094,-0.024384176,-0.021600906,0.010399933,-0.0010326958,-0.00080485776,0.017975863,-0.0389115,-0.023610292,-0.016115824,-0.010108029,0.024805062,-0.018437479,0.0040527154,0.0049386104,0.00077176397,0.022727791,-0.0010309985,-0.022089675,0.00017098454,-0.020949213,-0.011900185,-0.023637446,0.028783105,0.0022809252,-0.011092356,-0.027615488,-0.0052169375,-0.009917952,-0.004568639,-0.022496983,0.008044335,0.009449547,0.030005028,0.015545592,0.01724271,-0.013997821,0.011139876,0.0059297266,0.023257291,0.021994637,-0.01661817,-0.023651022,-0.0005036195,0.007243295,-0.0058686305,-0.022401944,-0.0008277688,-0.00017650017,-0.005206755,-0.03345357,-0.0071278913,-0.014079283,-0.002043329,-0.0029767433,-0.0099247405,0.0092051625,-0.008899681,0.0065780254,-0.004493966,-0.023596715,0.01945575,0.014568052,-0.017799363,-0.02676014,-0.017663594,-0.0123821655,0.021913175,0.013509052,-0.025673985,0.005600486,0.017717902,0.007949296,-0.0075487765,0.004612764,0.0101419715,-0.008295508,-0.003540186,-0.04559135,0.021152867,-0.0012270156,-0.025117332,-0.008431277,-0.018763326,0.011777992,-0.010841183,-0.0016343237,0.010162337,-0.012334647,0.0046331296,-0.031308416,0.018437479,-0.018315287,0.0050540143,0.011662588,-0.02082702,0.0073315455,0.0012677464,-0.011465723,0.012456839,0.010841183,0.02317583,-0.010685049,-0.039454576,-0.028457258,0.009130489,-0.009273048,0.0087367585,0.021600906,-0.008119008,0.0046670716,0.028077103,-0.0098568555,-0.008356604,0.013067801,0.0064863814,0.03513711,-0.024275562,-0.005963669,-0.040160578,0.0002768422,-0.007874623,0.03212303,0.009273048,0.002552464,0.000612235,-0.017378477,-0.002014478,0.021872444,0.007107526,-0.0038931863,-0.033833724,-0.011730473,0.009184797,-0.011241703,-0.016495977,0.020025982,0.002033146,0.02951626,-0.016319478,0.023393061,-0.008186893,0.0046942257,0.000790008,0.00080188777,0.021098558,0.013773802,-0.025497487,-0.00053077337,-0.007956085,0.005946698,0.037119344,0.03149849,0.015749246,0.01050176,0.0024625168,-0.0072568725,0.041110963,0.03149849,-0.005115111,-0.026637949,-0.015396246,0.0116965305,0.0061910828,0.013651609,0.011038049,0.0026203487,0.004833389,-0.010162337,0.0036318304,-0.002408209,0.005546178,0.0009147461,0.0027289642,-0.005020072,-0.010623952,-0.006309881,0.01587144,-0.0054443516,0.0010420298,-0.014011398,-0.021302212,-0.03948173,-0.0030819646,-0.027289642,-0.025850486,-0.024411332,0.0051558414,-0.0018651315,-0.0026712622,-0.0016190496,0.021247905,-0.010379568,0.025687562,-0.015124707,-0.014676668,-0.032014415,0.018396748,0.009327355,-0.0037947537,0.009700721,-0.0010912463,0.035870265,0.0048503606,0.019157058,-0.029244721,0.033535033,0.015654208,0.013040647,0.001529951,-0.0002291108,-0.011024472,-0.00965999,-0.029000336,-0.0032856185,0.0049046683,-0.0041579367,0.06766745,-0.0017132396,0.009775394,0.03106403,0.008600989,0.026040563,0.02170952,0.0060688904,-0.0045279083,-0.017541401,0.013495474,-0.0074062184,-0.020229634,-0.016916862,0.013169628,-0.0019041653,0.002944498,0.021478713,-0.016916862,-0.018668287,-0.0010394842,-0.005882208,0.0014671576,0.011771204,0.010284529,0.0074333725,0.014120013,0.006673064,-0.011723684,-0.03948173,0.00244894,0.02405833,-0.04178981,-0.010637529,-0.005668371,0.017514247,-0.0074197957,-0.010623952,0.020175327,-0.009653201,0.00033114996,0.0015630447,-0.016604593,-0.018016594,-0.01812521,0.0068190163,0.00019294099,-0.013651609,-0.02521237]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
0.6810617208191129
0.7288283049119854
0.7054572988151243
0.7503858761855651
0.6798748461012141
0.7353386764054303
0.6970167073760634
0.7093054191432305
0.7165472353100925
0.7108246367705486
0.69861867855662
0.6837530505693109
0.6713619412663939
0.6959023568199981
0.6930528685973071
0.7586989508797307
0.722258898711729
0.7102168735337814
0.7091902104996465
0.6645316006013193
0.6909945660772171
0.7468895635302879
0.6811540952744487
0.7797899768615341
0.7124060853665964
0.770469713362641
0.6747255329465729
0.7152150261632971
0.7142129518020808
0.7088538664686613
0.7650224378826143
0.7279783318765018
0.7451584086795425
0.6710990654068956
0.7170051761711373
0.7041168029639088
0.7006885749047365
0.6904645071669244
0.6710669874284394
0.772894615143386
0.6799962114811595
0.7697838424781854
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0060758824,0.020705521,0.0109094335,0.002617042,-0.0063304584,0.022307653,0.0022029318,-0.0024320502,-0.01592628,-0.040406317,0.005912954,0.02461581,-0.0039170776,-0.002499937,-0.010698984,-0.0131429145,0.012871367,0.015396762,0.0059401086,0.009083275,-0.002409987,0.011282812,-0.0032212362,-0.007358946,-0.014378457,0.039320126,0.029951725,-0.015749773,0.015220256,-0.022253344,0.015749773,-0.004225963,-0.016143518,-0.02862114,-0.0146771595,0.028322438,0.010794026,0.0022674243,0.027494216,0.016727345,0.016876698,0.023149451,-0.0045042997,0.0046129185,0.006198079,0.016170673,0.015342452,-0.014948708,-0.023407422,0.009701046,0.0012771233,0.01910339,-0.021628784,-0.0041614706,-0.0222805,0.0016182553,-0.002460902,0.0047113546,0.0100201145,-0.029653022,0.01811224,0.009782511,-0.018519562,0.01805793,-0.026665997,-0.004158076,0.0064662327,0.000868953,-0.005980841,0.014745046,0.014093332,0.023040833,-0.0087302625,-0.017555567,0.017134668,-0.023516042,-0.008703108,-0.012063512,-0.007929197,0.0171754,0.009388766,-0.010563211,-0.011466106,0.01665946,-0.004185231,-0.021873178,0.020773409,0.017772805,-0.03429649,0.009531328,0.009300513,0.02638087,0.017161822,-0.0061437697,0.006306698,-0.0058314893,-0.006069094,0.026258675,0.015070904,0.0077323243,-0.010943377,-0.003192384,-0.059197426,-0.017962888,-0.011147038,-0.0074811424,0.011160616,0.0043379767,0.00039225933,0.00034431415,-0.032802977,0.033862013,-0.006859977,-0.033916324,-0.007223172,-0.0010123643,0.03497536,-0.0072435383,-0.0038593735,-0.012450468,0.021194307,0.008404405,0.031309463,-0.025023133,0.024602233,-0.007318214,-0.017365484,-0.0068430053,-0.023502463,-0.022701398,0.024466459,0.024629388,0.018831842,0.018383788,-0.024018405,0.033400383,-0.013237956,0.017107513,-0.023149451,-0.008832093,0.011975259,0.020691944,0.0044873278,0.023488887,0.0057160817,0.010692196,0.013930404,0.027575681,0.003054913,-0.0100201145,-0.0128645785,-0.014785779,0.0292457,0.029435784,0.00429385,0.011825908,-0.0054682945,-0.00039819942,-0.012789902,-0.0031872925,-0.0091307955,0.02472443,-0.008506236,-0.009286935,0.022049684,0.013550237,0.02409987,-0.012172131,0.008465503,-0.0010454591,-0.024995977,0.03820678,-3.073476e-05,-0.008777783,0.00068693113,0.013129338,0.023393845,-0.027969426,-0.011995625,-0.007786634,-0.026923968,0.01462285,-0.009334456,0.0030990394,-0.030521976,0.022959368,0.0042599067,-0.029734487,-0.0030006035,-0.0059435028,0.016482953,0.01598059,-0.00045908554,-0.00072426896,-0.6682249,-0.021465855,0.007535452,-0.019442825,0.0070263,0.016143518,0.011235291,-0.0066495272,-0.03804385,0.03315599,-0.021574475,-0.015206678,0.00359122,-0.029517248,-0.019225586,-0.040053304,0.00042111127,-0.025688425,-0.012192497,0.005427562,-0.011574726,0.0014672069,-0.0064560496,0.016740924,-0.0010361247,0.00785452,0.004263301,-0.024914512,0.021791713,-0.007453988,0.005953686,0.023977673,0.005397013,0.00018679521,0.02305441,-0.03758222,-0.026163632,0.0102305645,-0.0025830986,0.03253143,-0.025308257,-0.01743337,0.00884567,0.004724932,-0.021764558,0.017324751,0.019469978,0.0062999097,0.0103867045,0.010332394,-0.005797546,0.014772202,0.013034295,-0.011174193,0.018044353,-0.023855476,0.024222067,-0.013366941,0.011717289,-0.01925274,0.016387912,-0.0027477245,-0.02940863,-0.0020145455,0.0028224003,0.013971136,-0.006985568,0.0017090541,-0.005339309,-0.031228,0.022511315,0.011276023,-0.00024099869,0.01290531,0.01649653,-0.0011846274,0.041954137,0.011676556,0.018723223,0.0038899225,0.016768077,-0.015328875,-0.019157698,0.0021401362,0.008465503,0.001195659,0.008716685,-0.0020433974,0.0039476263,-0.0059978124,0.027399175,0.02821382,-0.0027545132,-0.025172483,-0.011934527,0.017406216,0.0030175752,0.0094973855,-0.0007628797,-0.043854974,-0.0043854974,0.0047486927,0.031309463,0.02222619,0.02092276,0.007983506,-0.01754199,-0.0030939481,0.016482953,-0.024995977,0.016958162,-0.0054207733,-0.01530172,0.0036625012,-0.0052917884,-0.024289953,0.014514231,0.017895002,0.008737051,-0.0042972444,0.008981444,-0.005407196,0.016224982,-0.008940713,0.00080912764,0.006642739,-0.015451071,-0.013529871,0.008499447,-0.010583577,0.014948708,0.0064356835,0.019850146,-0.02549834,-0.0030667933,0.012396158,0.008288997,-0.007847732,0.016998895,-0.0015240621,-0.027331289,0.015505381,-0.0010666738,-0.020433974,-0.021520166,-0.013204013,0.006313487,0.009830032,0.0083636725,-0.0068430053,0.0060555167,-0.023176607,-0.0057126875,-0.00028533736,0.0011430467,0.0033349467,-0.004898044,-0.033699084,0.006819245,-0.026828924,0.01202278,0.0386141,-0.023516042,-0.019863723,-0.004178442,-0.024602233,-0.00790883,0.03462235,-0.004107161,-0.013278688,0.007888464,-0.016021322,0.007372523,0.016007744,-0.017990043,0.0032059615,-0.009877552,-0.01894046,-0.0055327867,-0.021357236,0.012328271,0.0030990394,-0.01498944,0.0014137458,0.035274062,0.020990647,0.006238811,0.021167153,-0.028566832,0.027181936,0.00013142491,0.013686011,-0.021384392,0.03445942,0.020311777,0.01389646,0.031363774,-0.013292266,-0.020773409,0.017474102,0.027616413,0.02576989,0.022443427,-0.010970532,-0.0023454942,-0.01103163,0.0040256963,-0.010121945,0.021425122,0.020406818,0.027222669,0.011303178,-0.0068362164,-0.012301116,0.0059570805,0.0024761767,-0.008424771,0.022239767,-0.011092729,0.0029496884,0.009259781,-0.0046977773,0.044886854,0.0097417785,-0.012063512,-0.012083878,-0.0017939128,0.024425726,0.034377955,-0.0023539802,-0.003611586,-0.017093936,-0.01290531,0.012348637,0.0041444986,0.007603339,0.00031164355,-0.0018278563,0.033889167,-0.008214322,0.015396762,0.013611334,-0.0003740147,-0.014853666,0.022076838,-0.00039798729,0.029544404,0.026435181,0.0042089913,0.002878407,0.0075490293,-0.008642009,-0.0030277583,0.021072112,-0.0021333476,-0.020705521,0.011601881,0.009449864,0.005427562,0.030223273,0.029598713,0.014609273,0.017107513,-0.0054581114,0.032612894,-0.022891482,-0.02878407,-0.00066698936,-0.015736196,0.0018431308,0.0030854621,0.008614855,0.0073385797,-0.02316303,0.011527205,0.0028750126,0.03459519,0.014161219,0.0094973855,0.026353717,-0.025620537,-0.017786382,0.0097417785,0.008675953,-0.012572664,-0.0050406065,-0.011459318,0.010536056,-0.020094538,0.032096952,0.003689656,0.033807702,-0.017338328,-0.00806497,-0.009599215,0.000990301,0.02575631,-0.017895002,-0.009918285,-0.008580911,-0.003815247,0.0066087954,-0.013027507,-0.013611334,0.008214322,-0.0018329478,-0.0065816403,-0.036903348,0.019021925,-0.010271297,0.02617721,-0.023380268,0.0018227648,0.00074972655,0.025240371,0.022103993,-0.0052951826,0.020257467,0.028349593,0.020352509,0.0004493268,-0.027385598,-0.008954289,0.0039001056,0.0073928894,0.026964698,0.004925199,0.022103993,-0.0104749575,-0.008160012,0.00024396875,0.0021689883,0.019429246,0.00962637,-0.014704315,-0.009510962,-0.0069041033,0.0034181082,0.015967011,-0.010026904,0.005593885,-0.03850548,-0.03440511,-0.019469978,-0.021873178,0.013814995,0.017161822,0.024371417,0.0015003018,-0.0019958764,0.0021537137,0.009449864,0.003560671,0.003034547,0.018858997,0.020515438,0.02237554,0.005505632,-0.014011867,0.019266319,0.014527809,-0.003944232,-0.014514231,0.023774011,-0.003855979,-0.013183647,-0.0018142789,0.0131429145,-0.011744443,-0.019700795,0.0029208362,-0.011411797,0.043420497,0.005766997,0.009436287,0.00095720607,-0.03054913,-0.025226792,-0.0049829027,-0.013054661,-0.013210801,-0.04247008,-0.015627578,-0.020121694,-0.021710249,-0.018438097,0.038858492,-0.0064424723,0.017772805,0.0032449965,-0.024588656,0.0051763803,-0.032667205,-0.012823846,-0.0037100222,-0.02773861,-0.04610882,-0.016768077,0.0050983103,0.02669315,-0.0034995726,-0.013170069,-0.0042022024,0.015654732,-0.007365735,-0.0074607763,0.015994167,-0.01103163,-0.0070873983,0.019334204,-0.03242281,0.00078961015,0.0065952176,0.027019009,0.0056278287,0.02680177,-0.00012718196,-0.041709747,0.007141708,-0.007270693,-0.022660665,0.02086845,-0.0045314543,-0.019578598,0.007318214,-0.010882279,-0.0029327164,-0.011927738,-0.021397969,0.00081549207,0.00765086,-0.015641155,0.012240018,-0.01748768,-0.004915016,-0.011418586,0.012396158,-0.011221713,0.021628784,0.0069176806,0.018261591,0.011438952,-0.0010768569,0.00045356972,0.0028648295,-0.016021322,0.005067761,0.010135522,-0.022565624,0.005054184,-0.0021995374,0.0016844451,-0.014541386,0.014975863,-0.024588656,0.03253143,-0.014554963,0.00017109634,-0.020963492,-0.0051390426,-0.018546717,0.008879614,-0.0028444636,-0.006452655,0.0010064241,0.01035955,-0.018519562,0.0071077645,-0.0078409435,-0.0151659455,-0.01748768,0.02331238,0.009144373,0.028268129,-0.009348034,0.01228075,-0.024181334,-0.03179825,-0.022796439,-0.017202554,-0.0051899576,-0.0030854621,0.0019262923,0.0265438,0.03565423,5.8340353e-05,0.03424218,0.009925073,-0.015967011,0.017514834,-0.007956351,0.00817359,-0.022348385,0.0015308509,0.013971136,0.0025932817,0.028186664,0.004599341,0.015573268,-1.4028203e-05,-0.018193705,-0.010074425,-0.014799356,-0.012722015,-0.001902532,0.008709896,-0.009646736,-0.0025440636,-0.022158302,0.005590491,-0.018383788,-0.0171754,0.00078961015,0.0043176105,0.006316881,-0.0035742482,0.008417983,0.0029344137,0.003893317,-0.025253948,-0.036414564,-0.017297596,-0.010237353,0.0047147493,-0.007603339,0.024018405,0.016401488,0.012606608,-0.014840089,-0.014011867,-0.027833652,-0.013984713,0.0048539173,-0.010760083,-0.0024710852,-0.021397969,-0.02685608,0.015423916,0.01259303,0.0047215377,-0.025471186,0.0041377097,0.0039645983,-0.017772805,0.009477019,0.014378457,0.024154179,0.012348637,0.022199035,0.019225586,-0.025376145,-0.00069371983,0.0021825656,0.0073317913,0.00053588266,0.028159508,0.022090415,-0.010441014,-0.020827718,-0.0040834006,0.014812934,-0.016740924,-0.017922156,0.031363774,0.011853063,-0.021370813,-0.006486599,-0.015260988,-0.026720306,-0.008675953,-0.0076848036,0.0012083878,-0.0051186765,-0.0032585738,0.0048912554,-0.008608066,0.0085877,0.021601629,-0.002372649,-0.011900583,0.01259303,0.009857186,-0.0069312584,-0.002146925,0.012151765,0.022837171,0.024425726,0.020501861,-0.017908579,-0.005247662,-0.0027154782,0.007671226,-0.014785779,0.028349593,-0.019225586,0.012912099,0.0058586444,-0.003564065,0.0051152823,-0.009891129,-0.015478226,-0.008180378,-0.0077323243,0.0064424723,0.031771094,-0.0022402694,-0.02305441,-0.0067751184,-0.007698381,-0.02424922,-0.014785779,0.0054615056,-0.0116086695,-0.037853766,-0.002829189,-0.016428644,0.011045207,0.023502463,0.0043956805,-0.01066504,0.013482349,0.015125214,-0.027304133,0.001961933,0.007969929,-0.0008070062,-0.014595696,-0.000602921,-0.024792315,-0.022402696,0.011635824,0.014093332,-0.054200947,-0.013088605,0.025797043,0.0041512875,0.01629287,0.00042874855,-0.00837725,0.013889671,-0.00038207628,0.009443075,0.009843608,0.030087499,0.0064356835,0.0082414765,0.0007327548,0.0035776426,0.005750025,-0.030739214,-0.001536791,0.0042429348,-0.030386202,0.00491841,-0.007067032,0.008214322,-0.011873429,-0.001127772,-0.028403902,-0.012817057,-0.012077089,0.048688523,0.022932213,-0.029381474,-0.029001307,-0.0039102887,0.0039645983,0.015790505,-0.020433974,-0.034540884,0.0017888213,-0.017392637,-0.02940863,-0.022823595,-0.013971136,0.03731067,0.01051569,-0.007603339,-0.02591924,-0.014812934,-0.013903248,-0.01363849,-0.023502463,-0.0046875942,-0.009714623,0.02336669,-0.015858393,0.018954039,-0.008058182,0.0016861423,-0.021764558,0.008682742,-0.005736448,-0.01910339,0.025579805,0.0015944949,-0.03383486,0.018519562,0.0061946847,-0.016550839,-0.0028495549,0.00060334534,-0.020596903,-0.015328875,-0.00437192,-0.0002465145,0.013251534,0.029435784,-0.0025610353,-0.013957558,-0.012273962,0.025199639,-0.01572262,0.0055226036,-0.021723825,0.013305844,-0.0077255354,-0.015464649,-0.026367294,-0.012212863,-0.0039204718,0.0010191529,0.003396045,0.012328271,-0.007671226,0.011839485,0.016102785,0.0036319522,0.0042599067,0.025077442,0.0036964447,0.016238559,0.01530172,-0.018587448,-0.01998592,0.0018991376,-0.022972945,0.00075821247,-0.010542844,-0.0075490293,-0.011214925,-0.021044957,0.008540179,0.019266319,-0.030168964,0.023448154,0.0039781756,-0.000734452,0.018397365,0.012138188,0.003940838,-0.011208137,-0.017908579,-0.010081213,-0.018777533,-0.016482953,0.044805393,0.01498944,0.017935734,0.0065850345,0.22049683,-0.017365484,0.0025016344,0.02821382,0.021357236,0.0011048602,0.012993563,0.027670722,-0.027141204,0.0035538822,0.009952228,-0.013081816,-0.005003269,0.00374736,0.0070806094,0.0033145805,-0.037066277,-0.03741929,-0.030630594,0.028539676,0.012260384,0.013957558,0.013557025,-0.026503067,0.051485468,-0.0013407674,-0.011785176,-0.0006211657,-0.0015087876,-0.0043888916,-0.006561274,-0.0091307955,0.029653022,0.03695766,-0.010563211,-0.017732073,-0.0034299884,0.004911621,0.018899728,-0.0032823344,-0.012477622,0.0069414414,0.0046366793,-0.011438952,-0.009823242,-0.0036183747,-0.0097417785,-0.010135522,0.00304473,0.022321232,-0.016306447,-0.0102645075,0.017949311,0.02700543,-0.0014315662,-0.01587197,0.0055259983,-0.014636428,0.004921804,-0.0067785126,-0.024344262,0.007311425,-0.023896208,0.021357236,-0.03258574,0.0020926155,-0.03288444,-0.00499648,0.023543196,-0.019741526,-0.0320698,0.0033383411,-0.014269838,-0.0031754123,-0.02748064,-0.009728201,0.027969426,0.020949915,0.015736196,0.0135095045,-0.006965202,-0.0054682945,-0.011662979,-0.018601026,-0.0024761767,-0.007426833,0.015179523,-0.019632908,0.012769536,0.0026475913,0.004175048,0.001737906,-0.017990043,0.0132990545,-0.010712561,0.018655336,0.0013509504,0.022239767,0.013618124,-0.025525495,-0.009205471,0.06120688,0.007698381,0.0069041033,0.001972116,0.005885799,-0.0071620736,0.016048476,0.020067384,-0.019089812,0.015274565,-0.050182037,0.0026594715,-0.0031737152,-0.012477622,-0.00608946,-0.0051220707,-0.0019789047,0.0022894875,-0.01936136,-0.002771485,-0.02441215,-0.02123504,0.005322337,0.011384643,-0.019727949,-0.02549834,-0.001912715,-0.008642009,-0.0027935482,-0.014188373,-0.011113095,0.0022487554,0.016700191,-0.012369003,-0.025471186,-0.011547571,-0.026285829,0.006225234,0.009429499,0.0025033315,-0.010420647,0.0038627677,-0.022796439,0.0073250025,-0.013842151,0.016102785,0.011975259,0.004755481,-0.016048476,0.001269486,0.0033790732,-0.013624912,-0.016021322,0.02195464,-0.006001207,-0.023651816,-0.015260988,-0.0055870963,-0.0033621015,-0.017107513,-0.027181936,0.020067384,0.013163281,-0.028376747,-0.0084858695,-0.17509404,0.011547571,0.01967364,-0.051757015,0.014120487,0.02748064,0.018872574,0.01581766,-0.03462235,-0.011493262,0.021465855,-0.022294076,-0.001972116,-0.011948104,-0.00484034,-0.011221713,-0.0137538975,0.01425626,0.013957558,0.014609273,0.052653123,-0.01530172,-0.007956351,0.0120092025,-0.017569143,-0.01879111,0.011561149,0.033237454,0.0133873075,-0.051566932,-0.017188977,-0.012851001,0.03190687,0.020271044,-0.011846274,0.0045518205,0.018723223,-0.026747461,-0.022552047,0.007786634,-0.0052272957,0.0139982905,-0.008954289,0.0055090263,-0.016238559,0.004096978,0.036414564,0.0019076235,0.0011192862,-0.011228503,0.01436488,-0.0056651663,0.0072435383,-0.0058484613,0.002009454,0.009544906,0.01748768,-0.006157347,-0.0057805744,-0.020800563,-0.020325355,-0.012559087,0.014147642,-0.0038661622,0.014650005,0.014201951,-0.014229106,0.020406818,-0.037202053,0.018438097,0.0150437495,-0.018071508,-0.004049457,-0.0018906518,9.9549856e-05,0.026475912,0.0054547167,0.009538117,0.028702606,0.00017258136,-0.010590365,0.040840793,-0.020800563,0.022307653,-0.0061132205,-0.010379915,0.0024133814,0.034079254,-0.012728804,-0.028268129,0.010088001,-0.040514935,0.0207191,-0.0022436639,0.017827114,0.0123757925,0.0057839686,-0.018913306,0.0087302625,-0.008105703,-0.02024389,-0.0024829654,-0.009823242,0.021194307,0.018587448,-0.011540783,-0.014392035,0.014663583,0.037120588,0.0037303881,-0.009171528,-0.014975863,0.0047690584,0.011269235,0.0031058283,0.012755959,-0.010054058,-0.0024269586,-0.018926883,0.023285225,0.047004927,0.0095856385,-0.01570904,0.014894398,-0.013088605,-0.01660515,-0.09531329,-0.004341371,0.017718496,0.021221463,0.007358946,0.04393644,-0.0002131014,0.012518355,-0.042334307,0.024235643,-0.04390928,-0.018872574,0.0058484613,-0.004127527,-0.00304473,-0.011540783,0.0008961078,-0.025335412,-0.02242985,0.028566832,0.0033824677,-0.049041536,0.00030485485,-0.0100201145,0.018152973,-0.024384994,-0.021520166,0.010352761,-0.0010318818,-0.0008231293,0.01805793,-0.038912803,-0.023651816,-0.016116364,-0.010155888,0.024846626,-0.018451674,0.0040426683,0.004938776,0.0007900344,0.022728553,-0.0010089698,-0.022090415,0.0001688688,-0.020909183,-0.011927738,-0.023692548,0.02878407,0.00226403,-0.011147038,-0.027643569,-0.0051763803,-0.009877552,-0.004616313,-0.022511315,0.008098914,0.009409132,0.02997888,0.015546113,0.017256863,-0.013984713,0.0111198835,0.0059401086,0.023244495,0.022022529,-0.01665946,-0.023706125,-0.00048411885,0.0072639044,-0.0058722217,-0.022416273,-0.00086046715,-0.00019400819,-0.005240873,-0.033427536,-0.0071484963,-0.01410691,-0.0020705522,-0.0030090893,-0.009945439,0.009191894,-0.00889998,0.0065952176,-0.0044703563,-0.023611084,0.019429246,0.014595696,-0.017759228,-0.026761038,-0.017664187,-0.012416524,0.021886755,0.013495927,-0.02560696,0.0056040683,0.017664187,0.0079970835,-0.007555818,0.004595947,0.010196621,-0.008288997,-0.0035504878,-0.045592878,0.02118073,-0.0012236624,-0.025199639,-0.008417983,-0.018804686,0.011703711,-0.010855124,-0.0015919491,0.010155888,-0.012321482,0.004657045,-0.031363774,0.018519562,-0.0183159,0.0050473954,0.011669768,-0.020827718,0.0073317913,0.0012559086,-0.011479684,0.012457256,0.010807604,0.023190184,-0.010732927,-0.039455898,-0.028403902,0.009151162,-0.009273359,0.008703108,0.02154732,-0.008139646,0.004731721,0.028078046,-0.009850398,-0.008322941,0.013054661,0.006452655,0.035138287,-0.024344262,-0.0059435028,-0.040161923,0.00030039978,-0.0078409435,0.032096952,0.009307302,0.0025627327,0.0006385617,-0.017284019,-0.0019738132,0.02180529,0.007114553,-0.003893317,-0.033807702,-0.011744443,0.009144373,-0.011221713,-0.016550839,0.020013075,0.002019637,0.02940863,-0.016333602,0.023380268,-0.008187167,0.004704566,0.0008473141,0.00075736386,0.02103138,0.013814995,-0.02555265,-0.0005138194,-0.00796314,0.0059604747,0.03714774,0.031499546,0.015749773,0.010481746,0.0025033315,-0.0072163832,0.041085187,0.031499546,-0.0051356484,-0.026611688,-0.015451071,0.011744443,0.00616753,0.01363849,0.011004476,0.0026221336,0.0047996077,-0.010142311,0.0036319522,-0.0024388388,0.0055395756,0.0009020479,0.0026374082,-0.0050100572,-0.010644674,-0.0062489943,0.015858393,-0.0054649,0.000990301,-0.013984713,-0.021248616,-0.03951021,-0.00304473,-0.027290557,-0.025851352,-0.024398573,0.0051186765,-0.0018668913,-0.0026764432,-0.0016946282,0.02123504,-0.010413859,0.02566127,-0.015125214,-0.014663583,-0.0320698,0.018370211,0.009361612,-0.0038593735,0.009694258,-0.0011218319,0.035925776,0.0048844665,0.019171277,-0.029272856,0.033536155,0.015627578,0.0131089715,0.0015401853,-0.00023505859,-0.011024842,-0.009694258,-0.029001307,-0.0032551796,0.004823368,-0.0041818363,0.06766972,-0.0016920824,0.009830032,0.031065071,0.008540179,0.026027858,0.021710249,0.006082671,-0.0045416374,-0.017569143,0.0135095045,-0.0074200444,-0.020230314,-0.016931007,0.0131429145,-0.0019364754,0.0029225335,0.02149301,-0.016958162,-0.0187368,-0.0010853427,-0.0058891936,0.0014706012,0.011696923,0.010278085,0.0074132555,0.014161219,0.0066495272,-0.011696923,-0.03948305,0.0024388388,0.02404556,-0.041709747,-0.010698984,-0.005651589,0.017501257,-0.007406467,-0.01067183,0.02014885,-0.009639948,0.00034049552,0.0015877062,-0.016632304,-0.017976467,-0.018085085,0.0068260334,0.00018870452,-0.013726743,-0.02518606]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"context":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."}]},"question":"How many parameters are used in the model?","answer_model":"chatgpt","top_n":3,"relevance_method":"vector","relevance_model":null}
0.7325024185364819
0.711812566571579
0.690368333390956
0.6757619927657039
0.698617628819236
0.6932869320338836
0.7025273065303196
0.7904425948291975
0.6752872766599619
0.6942993289667566
0.7007710144800765
0.6638511043362394
0.7303793369524236
0.7567976368776136
0.6848077402112724
0.7093019505669392
0.6910974949040251
0.7550542933617603
0.7050688599707657
0.676402779175108
0.6898073720760688
0.7700124542243039
0.7046585860312382
0.7447318745472687
0.6940807340532332
0.6626570953441064
0.7389349481573533
0.7272547443771054
0.6941805533985369
0.6758009385829887
0.691410773299741
0.7315455085581143
0.6975988266641516
0.7374516950769843
0.6879784809152596
0.6890494841544731
0.6998656127304126
0.6872845627618935
0.6946309619579243
0.6949493716193237
0.6799653796285622
0.7136147642476195
0.6913925665458697
0.6910898640605629
0.6789448601056742
0.7750885835736889
0.6956117217939303
0.7928302701170225
0.7202142073148572
0.7738260016214817
0.6856685809891874
0.6938460795258273
0.7067073824542744
0.7047299508106206
0.6932034487910467
0.7109675045003955
0.7357237703382717
0.731378708712893
0.7395645734943067
0.7005531679995186
0.6954700769340236
0.7475658355641005
0.7296114694539106
0.7053066252228088
0.7136564708517301
0.7231699720853941
0.7677551998638205
0.6836846555523155
0.6991261608456484
0.7121348554949491
0.7012959331411696
0.734746641991048
0.6902007781491266
0.6935842354202737
0.6746033865160848
0.6685336075737706
0.7074485540713573
0.7367144640939643
0.7117599926665047
0.7207243214879869
0.7532254900862251
0.6821984266240271
0.7365336412466039
0.6927294974650433
0.7317376337915816
0.7752238459816901
0.655296080297426
0.7377289199093906
0.7170534230309902
0.6818414976196959
0.6936187492125875
0.6726282880122224
0.7366601461713065
0.6945172137815173
0.7608954624709383
0.7508676088720964
0.7694490355838188
0.6955905261448799
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."}]},"formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.\n\nGODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
0.6947807432647038
0.6990539442087055
0.7341427936063176
0.7072847666487507
0.7495738272192982
0.7361484826257692
0.7044441930349663
0.7380702362288236
0.7012275223779988
0.7330932411683801
0.7023424686654312
0.7072422721647442
0.6976957833210022
0.6684862813218827
0.7545175323196076
0.687926266476745
0.7688414321519543
0.7483703357905921
0.6861230275086158
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.0061255354,0.02071273,0.01096756,0.0026111621,-0.006315685,0.022301838,0.0021952097,-0.0024057326,-0.015850332,-0.040420383,0.0059625497,0.024637962,-0.0038742991,-0.0025602293,-0.010675545,-0.013120326,0.012848684,0.01538854,0.005945572,0.0090524815,-0.002373475,0.011307113,-0.0032563128,-0.007361508,-0.014342717,0.039360978,0.029907824,-0.015741674,0.015157644,-0.022233928,0.015796004,-0.004244412,-0.016149139,-0.028631104,-0.014695852,0.028332299,0.010763829,0.0022902847,0.027490208,0.016760334,0.01685541,0.02315751,-0.004502472,0.0046145245,0.0061696772,0.016230632,0.015374958,-0.014967494,-0.023429153,0.009738379,0.0013106744,0.019096458,-0.021636315,-0.0041731056,-0.022274673,0.001623912,-0.0024311992,0.0046790396,0.010023603,-0.02969051,0.018118545,0.009758752,-0.01853959,0.017996306,-0.026593788,-0.0041629192,0.00650923,0.000859069,-0.0059863185,0.014695852,0.014071074,0.022994526,-0.008746884,-0.017534515,0.017140632,-0.023524228,-0.008645019,-0.012135622,-0.007959121,0.01718138,0.009419199,-0.010546515,-0.011436144,0.016678842,-0.0042342255,-0.021921538,0.02076706,0.017724665,-0.03430843,0.009453154,0.009317333,0.026390055,0.01711347,-0.006149304,0.006291916,-0.005836915,-0.006067811,0.026308563,0.015116897,0.0077553894,-0.0109811425,-0.0032155665,-0.05916371,-0.018009888,-0.0111373365,-0.0075380756,0.011184874,0.004302136,0.00039027366,0.00036714162,-0.032841563,0.033873804,-0.00682841,-0.033900965,-0.0073139705,-0.0009914946,0.034987535,-0.007232478,-0.0038675081,-0.012509131,0.021242432,0.008386958,0.0312932,-0.025031844,0.024665127,-0.007232478,-0.017357947,-0.0068453876,-0.023429153,-0.02269572,0.024515724,0.024678709,0.018811233,0.01840377,-0.024040349,0.033357684,-0.0132561475,0.017086305,-0.023116766,-0.008841959,0.011925099,0.02071273,0.004482099,0.023497064,0.0057146763,0.0107095,0.013901298,0.027558118,0.0030644655,-0.010010021,-0.012855475,-0.014790927,0.029201554,0.029391704,0.004285158,0.011809652,-0.0054735937,-0.0003883637,-0.012814729,-0.0031442603,-0.009140765,0.02474662,-0.0085159885,-0.009310542,0.022098105,0.0136024915,0.024053931,-0.012176369,0.00846166,-0.0010424276,-0.02500468,0.038192917,-3.0888045e-06,-0.00878763,0.0007236722,0.0131610725,0.023442736,-0.028033491,-0.011952264,-0.007836882,-0.026960505,0.014587195,-0.0093377065,0.0031052118,-0.030532602,0.023008108,0.0042478074,-0.02974484,-0.0030508833,-0.005969341,0.01647511,0.0160269,-0.00047028085,-0.0007678141,-0.6680229,-0.021446165,0.0075448668,-0.019436011,0.007049119,0.016162721,0.011266367,-0.006662029,-0.038084257,0.03322186,-0.021581985,-0.01519839,0.0036196345,-0.029527524,-0.019232279,-0.039958593,0.0004533032,-0.025765277,-0.012196742,0.0054328474,-0.011633084,0.0014558333,-0.0064582974,0.016773917,-0.0010144145,0.0078233,0.004308927,-0.02493677,0.021812882,-0.0074497918,0.0059489678,0.023958856,0.0053004217,0.00010186589,0.02302169,-0.037595302,-0.026145577,0.010179797,-0.002561927,0.03251559,-0.025344232,-0.01743944,0.008855541,0.004685831,-0.02173139,0.017344365,0.019503921,0.006257961,0.010376738,0.010302037,-0.0057690046,0.014818091,0.0130659975,-0.011184874,0.018050635,-0.023863781,0.024284827,-0.013371595,0.011728159,-0.019245861,0.016393617,-0.0027758454,-0.02936454,-0.0019948736,0.002828476,0.013908089,-0.007008373,0.0016884271,-0.005297026,-0.031238873,0.022532733,0.011300323,-0.00024065816,0.012923385,0.016502274,-0.0011706088,0.04191442,0.011667039,0.018688995,0.0038573216,0.016787497,-0.015307047,-0.019137204,0.0021323925,0.008482032,0.0012189951,0.008706138,-0.002071273,0.0040101204,-0.0060712066,0.027367968,0.028223641,-0.0027622632,-0.025167665,-0.011952264,0.017398693,0.0030050436,0.009534647,-0.0007656919,-0.043843076,-0.0043972107,0.0047299727,0.031320363,0.022220345,0.020889297,0.007972703,-0.01757526,-0.003088234,0.016515855,-0.025018262,0.017004812,-0.0054600113,-0.015307047,0.0036535899,-0.0052936305,-0.024298409,0.014560031,0.017860485,0.00871972,-0.0043870243,0.008984571,-0.005398892,0.016176302,-0.008950616,0.0008229915,0.0066654244,-0.015429286,-0.01353458,0.008454869,-0.010628007,0.0149946585,0.006400573,0.019843474,-0.025534382,-0.0030661633,0.0124004735,0.00827151,-0.007911583,0.017004812,-0.0015330816,-0.02731364,0.015470033,-0.0010220543,-0.020413924,-0.021595567,-0.013188236,0.0062749386,0.009840244,0.008380167,-0.006841992,0.0060712066,-0.023225421,-0.0057554226,-0.0002646391,0.0012088085,0.0033378056,-0.004957473,-0.03365649,0.006763895,-0.026879013,0.011986219,0.038627543,-0.023605721,-0.019829892,-0.0041968743,-0.024583634,-0.007898002,0.034607235,-0.0041595236,-0.0133512225,0.007898002,-0.015972571,0.007368299,0.015999734,-0.017982725,0.0032614062,-0.009860618,-0.018974219,-0.0054600113,-0.021364672,0.012380101,0.003118794,-0.015021822,0.0014125403,0.035367835,0.021011537,0.0062749386,0.021188105,-0.028522449,0.027082743,0.00017837141,0.013717939,-0.021419,0.03444425,0.020291684,0.013935253,0.031374693,-0.013283311,-0.020794222,0.01743944,0.02763961,0.025751695,0.022478405,-0.010947187,-0.0023293332,-0.011001516,0.004003329,-0.010166216,0.021432582,0.020441087,0.027177818,0.011300323,-0.006770686,-0.012318981,0.0060508335,0.002497412,-0.008407331,0.022233928,-0.011116964,0.0028743157,0.009229049,-0.0047028083,0.04490248,0.009718006,-0.012081294,-0.012060921,-0.0017503955,0.024434231,0.034389924,-0.0023327288,-0.003616239,-0.017031977,-0.0128758475,0.01231219,0.0041595236,0.007673897,0.00028564892,-0.0018250971,0.033900965,-0.008210391,0.015374958,0.013588909,-0.00037244713,-0.014845255,0.022070942,-0.00038560483,0.02955469,0.026430802,0.0042579942,0.0028845023,0.007490538,-0.008638227,-0.0030220214,0.021065865,-0.0021391837,-0.020699147,0.011639875,0.009466737,0.005405683,0.030152302,0.029609017,0.014641523,0.017072722,-0.005456616,0.032651413,-0.022885868,-0.028766926,-0.00066043047,-0.015741674,0.0017860485,0.003125585,0.008597481,0.007381881,-0.023143928,0.01148368,0.0028233828,0.034580074,0.014125403,0.009521064,0.026335727,-0.025615875,-0.017792575,0.009718006,0.008631436,-0.012597415,-0.0051000854,-0.011463308,0.010485395,-0.020115117,0.032108128,0.0037248959,0.03384664,-0.017290037,-0.008101733,-0.009568602,0.0009948901,0.025751695,-0.017914813,-0.009921737,-0.008570316,-0.0037962021,0.006617887,-0.013038834,-0.013670402,0.008210391,-0.0017843507,-0.0066009094,-0.036970526,0.0189878,-0.010240917,0.026213488,-0.023456318,0.0017860485,0.0007223989,0.025221994,0.022070942,-0.0053275856,0.020223774,0.028332299,0.020373177,0.00042083344,-0.027395133,-0.008998153,0.003904859,0.0074090455,0.02693334,0.0049269134,0.022152435,-0.010471813,-0.008149271,0.0003132376,0.0022104897,0.019422429,0.009636513,-0.014777345,-0.009548229,-0.006899716,0.0034447648,0.015945407,-0.01000323,0.00561281,-0.038491722,-0.03441709,-0.019476758,-0.021840045,0.013846969,0.01718138,0.024379902,0.001479602,-0.0019863849,0.002202001,0.009419199,0.0035483283,0.0030271146,0.01885198,0.020522581,0.022329003,0.005500758,-0.013989582,0.019273026,0.014532866,-0.003918441,-0.014532866,0.023768706,-0.0038742991,-0.01320861,-0.0018590525,0.013099953,-0.011748532,-0.019680489,0.002865827,-0.0114225615,0.043435615,0.005785982,0.009439572,0.0010212056,-0.030614095,-0.025221994,-0.004964264,-0.013025251,-0.013242565,-0.04251203,-0.015605854,-0.020169446,-0.02173139,-0.018444516,0.03892635,-0.0063903867,0.017778993,0.003227451,-0.024583634,0.005147623,-0.032651413,-0.0128758475,-0.0037045227,-0.02770752,-0.046206366,-0.016760334,0.0050559435,0.026702445,-0.0034939998,-0.013133909,-0.0041900836,0.015714511,-0.007361508,-0.007476956,0.015931824,-0.0110762175,-0.0070830747,0.019395264,-0.03237977,0.0007656919,0.0065805363,0.027028415,0.0056263926,0.026783938,-7.698302e-05,-0.04175143,0.007198523,-0.0072868066,-0.022682138,0.020821387,-0.0044481438,-0.019585414,0.0073139705,-0.010906441,-0.002937133,-0.011945473,-0.021432582,0.0008111071,0.00769427,-0.015700929,0.012318981,-0.01743944,-0.004943891,-0.011402188,0.012454802,-0.0112324115,0.021622732,0.0069438578,0.018227203,0.011463308,-0.0010925116,0.00043759888,0.0028267782,-0.015972571,0.005093294,0.01019338,-0.022587063,0.0050491523,-0.0021935122,0.0017164402,-0.01449212,0.015021822,-0.024651544,0.03251559,-0.014573613,0.00021317559,-0.020984372,-0.0051306454,-0.018526008,0.008869123,-0.0028862,-0.0064413194,0.0009982857,0.010363156,-0.018553173,0.007110239,-0.007836882,-0.015171226,-0.017493768,0.023334078,0.009093228,0.028196476,-0.0093377065,0.012264653,-0.024176171,-0.031727828,-0.022804376,-0.017140632,-0.005167996,-0.0030712564,0.0019439406,0.026539458,0.035693806,0.00012637736,0.03422694,0.009948902,-0.015986154,0.017493768,-0.007972703,0.008149271,-0.022396913,0.0015789212,0.013962418,0.0025534383,0.028114984,0.004553405,0.015592271,1.4364681e-05,-0.01821362,-0.010139051,-0.014777345,-0.012746817,-0.00189131,0.008733302,-0.009568602,-0.002548345,-0.022098105,0.005534713,-0.018390188,-0.017222125,0.000851429,0.0043055313,0.006366618,-0.003551724,0.008441286,0.002988066,0.003847135,-0.025221994,-0.03642724,-0.017344365,-0.010261291,0.00474695,-0.0075312844,0.024040349,0.01635287,0.012570251,-0.014845255,-0.013989582,-0.027802596,-0.013989582,0.0048827715,-0.010824949,-0.0024430836,-0.021391837,-0.026906176,0.015374958,0.012590623,0.004757137,-0.0255208,0.0041391505,0.0039863517,-0.017751828,0.009561812,0.0143291345,0.024176171,0.012414056,0.02219318,0.019191533,-0.025412142,-0.00071348564,0.0022529338,0.0072935973,0.0005352203,0.028196476,0.022179598,-0.010424276,-0.020848552,-0.0040610535,0.014790927,-0.01673317,-0.017941978,0.031320363,0.011823234,-0.02135109,-0.0065228124,-0.015334211,-0.026716026,-0.0086586,-0.0076603144,0.0012266351,-0.005059339,-0.0032240553,0.0048522116,-0.008583899,0.008617854,0.021595567,-0.002390453,-0.011884353,0.012536295,0.009785916,-0.006899716,-0.0021816278,0.012122041,0.022872288,0.024420649,0.020522581,-0.017914813,-0.0053140037,-0.0027537744,0.0076399413,-0.01475018,0.028359463,-0.019191533,0.012909804,0.0058572884,-0.0035245595,0.0051306454,-0.009847036,-0.015483615,-0.008196808,-0.0077350163,0.006444715,0.031754993,-0.0022172807,-0.02302169,-0.006835201,-0.007701061,-0.024298409,-0.014777345,0.00548378,-0.011551592,-0.037812617,-0.00290148,-0.016488692,0.011089799,0.023510646,0.0044073975,-0.010682336,0.013500625,0.015116897,-0.02732722,0.0020033624,0.007986286,-0.00077120966,-0.014573613,-0.00064939505,-0.024773784,-0.022396913,0.011633084,0.014084657,-0.05419265,-0.013099953,0.025806025,0.0041629192,0.01628496,0.00042741228,-0.008353002,0.013880924,-0.00041786235,0.009439572,0.0098741995,0.030097974,0.0064684837,0.008353002,0.0007190034,0.003565306,0.0057282583,-0.030695587,-0.0014974285,0.004241016,-0.030342452,0.004920122,-0.0070423284,0.008162853,-0.01186398,-0.0011451424,-0.028468119,-0.012794355,-0.012108458,0.048759803,0.02295378,-0.02936454,-0.029092897,-0.003918441,0.003928628,0.015768839,-0.020481834,-0.034580074,0.0017996307,-0.017385112,-0.029391704,-0.022845123,-0.013962418,0.037350826,0.010498977,-0.0076671056,-0.025873935,-0.014818091,-0.013908089,-0.013650029,-0.023564976,-0.0047299727,-0.00968405,0.023374826,-0.015823167,0.019028548,-0.0080406135,0.0016629606,-0.0217993,0.008665391,-0.0057112807,-0.019164369,0.025629457,0.0015619436,-0.033873804,0.018512428,0.00625117,-0.01659735,-0.0029065732,0.00060525315,-0.020549744,-0.015279883,-0.0043768375,-0.00026442687,0.013262939,0.02950036,-0.002561927,-0.013921671,-0.0122442795,0.025167665,-0.015768839,0.0055415044,-0.021717807,0.013303685,-0.00769427,-0.015442869,-0.02636289,-0.0122442795,-0.003962583,0.0010144145,0.003356481,0.012298608,-0.0076671056,0.011836816,0.016108392,0.0036535899,0.004302136,0.025086172,0.0037045227,0.01628496,0.015334211,-0.018607501,-0.019979296,0.0018828211,-0.02296736,0.0008200204,-0.010560097,-0.0075924043,-0.011259576,-0.021120194,0.008549944,0.019273026,-0.030179467,0.023497064,0.0040372848,-0.0007521098,0.018430935,0.012094876,0.00391165,-0.011205248,-0.017928395,-0.010071141,-0.018784069,-0.01647511,0.044793826,0.015076151,0.017941978,0.006604305,0.2205736,-0.017330782,0.0025364605,0.028223641,0.021323925,0.001089965,0.012984505,0.027680356,-0.027109908,0.0035890748,0.009908156,-0.013113535,-0.0050118016,0.003738478,0.007103448,0.0032817794,-0.036970526,-0.037405152,-0.030559765,0.028522449,0.012223906,0.013989582,0.013514208,-0.026498713,0.05155772,-0.0013514208,-0.011748532,-0.0006141664,-0.0014583799,-0.0043564644,-0.0065805363,-0.009181512,0.029663347,0.036916196,-0.0106008435,-0.017724665,-0.0034566491,0.004913331,0.01885198,-0.0032868725,-0.012488757,0.006981209,0.0046213157,-0.0114225615,-0.0098741995,-0.0036603808,-0.009656886,-0.010173007,0.002994857,0.022396913,-0.01635287,-0.010268081,0.017928395,0.027028415,-0.0014549844,-0.015958989,0.005504153,-0.014655106,0.004899749,-0.0067435214,-0.024352739,0.0073207617,-0.023877364,0.021364672,-0.03262425,0.002173139,-0.032977384,-0.0049812417,0.023592139,-0.019748399,-0.0320538,0.0033310144,-0.014274806,-0.0031374693,-0.027476626,-0.009718006,0.027979163,0.020943627,0.015782421,0.013493834,-0.007008373,-0.0054769893,-0.011653458,-0.018702576,-0.0024379902,-0.0074497918,0.015157644,-0.019544668,0.012726445,0.0026366287,0.0041187773,0.0016901249,-0.017996306,0.013378386,-0.0107095,0.018675413,0.0013692473,0.022261092,0.013575327,-0.025561547,-0.009256214,0.06117386,0.007707852,0.0069438578,0.0019082876,0.0059014303,-0.007137403,0.016054064,0.020087952,-0.019096458,0.015361375,-0.05019951,0.0026654906,-0.0032444284,-0.012468385,-0.006111953,-0.0050797123,-0.0019524295,0.002273307,-0.019408846,-0.0027979163,-0.024393484,-0.021188105,0.0053785187,0.01134786,-0.019694071,-0.02545289,-0.0019371496,-0.008624645,-0.002843756,-0.014220478,-0.011144128,0.002251236,0.016678842,-0.012454802,-0.02545289,-0.011571964,-0.02629498,0.00625117,0.009453154,0.0025092964,-0.010397111,0.0038267619,-0.022817958,0.0073886723,-0.01385376,0.0160269,0.011965846,0.004740159,-0.0160269,0.0013004878,0.0033955295,-0.013622864,-0.016054064,0.021880792,-0.006020274,-0.023714378,-0.015266301,-0.005609415,-0.0033259213,-0.017045557,-0.027137073,0.020074371,0.013133909,-0.028386626,-0.008509197,-0.175155,0.011490472,0.019680489,-0.051775035,0.014138985,0.027476626,0.01885198,0.01583675,-0.0346344,-0.011531218,0.02147333,-0.022356166,-0.0020084558,-0.011979428,-0.004835234,-0.011259576,-0.013779059,0.014220478,0.013975999,0.014587195,0.052671455,-0.015293465,-0.007911583,0.011965846,-0.017588843,-0.018838398,0.011524428,0.03327619,0.013330849,-0.051584885,-0.017235707,-0.012869057,0.031917978,0.020237356,-0.011802861,0.0045703826,0.01866183,-0.026783938,-0.022600645,0.0077553894,-0.005191765,0.014003164,-0.008950616,0.0054973625,-0.016244214,0.0040950086,0.03642724,0.0019235675,0.0011714577,-0.011266367,0.014356299,-0.005745236,0.007232478,-0.0058437064,0.0020050602,0.009507483,0.01750735,-0.0061391173,-0.005745236,-0.020821387,-0.020318849,-0.012590623,0.014098238,-0.0038607172,0.014641523,0.014206896,-0.014315552,0.020468252,-0.037160676,0.018512428,0.015008241,-0.018064218,-0.004047471,-0.0019541273,7.013891e-05,0.026498713,0.005456616,0.009561812,0.028794091,0.00018346471,-0.010668754,0.040936504,-0.020794222,0.022301838,-0.0060610203,-0.010376738,0.0024345948,0.034063954,-0.012644952,-0.02827797,0.010118678,-0.040610533,0.020685567,-0.0022699116,0.01781974,0.0124004735,0.0058131465,-0.01885198,0.008740093,-0.008108525,-0.020278102,-0.0025449493,-0.009799498,0.021201687,0.018566756,-0.011565174,-0.014397046,0.014668687,0.03718784,0.00378262,-0.009113601,-0.014953912,0.0047673234,0.0112324115,0.0031578424,0.012733236,-0.010043976,-0.0024719455,-0.01891989,0.023293333,0.047048457,0.009622931,-0.015728094,0.014845255,-0.013106744,-0.016529437,-0.095346466,-0.004336091,0.017778993,0.021256015,0.007361508,0.04387024,-0.00016733594,0.012515922,-0.042294715,0.024230499,-0.043951735,-0.018865563,0.0057893777,-0.0040746354,-0.003038999,-0.011558383,0.0009269796,-0.025439307,-0.02243766,0.028576776,0.0033819475,-0.049031448,0.00030559767,-0.010030394,0.018172873,-0.02436632,-0.021581985,0.010390321,-0.0009965879,-0.00078776287,0.01802347,-0.038953517,-0.023646468,-0.016121974,-0.010145842,0.024800947,-0.018498845,0.00407124,0.004954078,0.0007894606,0.022722883,-0.001003379,-0.022098105,0.00016362208,-0.02097079,-0.011891144,-0.023687214,0.028821254,0.0022563294,-0.01109659,-0.027680356,-0.005191765,-0.009880991,-0.0046247113,-0.02250557,0.008054196,0.009446363,0.030016482,0.015537943,0.01724929,-0.014003164,0.011110173,0.005938781,0.02327975,0.022016613,-0.016638095,-0.023660049,-0.00045202987,0.007218896,-0.0058437064,-0.02238333,-0.0008480335,-0.00019556128,-0.0052189287,-0.03341201,-0.0071509853,-0.014098238,-0.0020763662,-0.0029914614,-0.009921737,0.009229049,-0.008882705,0.0065363944,-0.004533032,-0.023687214,0.019490339,0.014573613,-0.017738245,-0.0268111,-0.017711082,-0.012380101,0.021921538,0.013473461,-0.02564304,0.0055822507,0.017683918,0.007972703,-0.007476956,0.004563592,0.0101594245,-0.008264719,-0.0035347464,-0.04560875,0.021147357,-0.0012240884,-0.02512692,-0.008393749,-0.018824816,0.011728159,-0.010831739,-0.0016366453,0.010145842,-0.012305399,0.004641689,-0.031320363,0.01847168,-0.01828153,0.005038966,0.011667039,-0.020862134,0.0073343436,0.0012334262,-0.011456517,0.012454802,0.0107909925,0.023171093,-0.010675545,-0.039442472,-0.028495284,0.009133974,-0.0093037505,0.00871972,0.02160915,-0.00814248,0.004706204,0.02808782,-0.009806289,-0.008373376,0.013086371,0.0064990437,0.03515052,-0.024325574,-0.00593199,-0.040175904,0.00029053,-0.007850464,0.032108128,0.009263004,0.0025670202,0.00062520185,-0.017303618,-0.0020220377,0.021894375,0.0070559103,-0.003891277,-0.033873804,-0.011728159,0.00917472,-0.01122562,-0.016529437,0.02000646,0.0020118512,0.02950036,-0.016312124,0.023374826,-0.0081696445,0.0047231815,0.000801345,0.0008102582,0.021106612,0.013745104,-0.0255208,-0.0005152716,-0.007925166,0.0059523634,0.037160676,0.031510513,0.015768839,0.010498977,0.0024991098,-0.007252851,0.04109949,0.031564843,-0.0051204585,-0.026620952,-0.015429286,0.011748532,0.0062104235,0.013683984,0.011008306,0.0026196511,0.004828443,-0.010145842,0.0036434033,-0.0024345948,0.0055686682,0.00093801506,0.0026943528,-0.0050695254,-0.010587261,-0.0062987073,0.015877496,-0.0054464294,0.0010517653,-0.014016746,-0.02128318,-0.039496798,-0.0031001184,-0.027300058,-0.025860352,-0.024434231,0.005117063,-0.0018726345,-0.0027113303,-0.0016519252,0.02122885,-0.010431067,0.025670202,-0.015116897,-0.014655106,-0.0320538,0.018390188,0.009324124,-0.003802993,0.009704423,-0.001104396,0.03585679,0.004855607,0.019150786,-0.029310212,0.033547834,0.015660182,0.013045625,0.0015313838,-0.0002204972,-0.011035471,-0.009704423,-0.029011404,-0.0032834772,0.0049065403,-0.004193479,0.06769328,-0.0017308712,0.009772334,0.031075886,0.008570316,0.026050502,0.02173139,0.0060236696,-0.0045364276,-0.017561678,0.013493834,-0.0074090455,-0.020223774,-0.016909737,0.013195028,-0.001955825,0.0029490173,0.02147333,-0.01692332,-0.018688995,-0.0010415787,-0.005891244,0.0014897885,0.011714577,0.010288455,0.0074158367,0.014166149,0.0066212825,-0.011694204,-0.039469637,0.0024498745,0.024094678,-0.041778594,-0.010675545,-0.0056399745,0.017520932,-0.0074294186,-0.01064159,0.020142281,-0.009622931,0.00030156548,0.0016188187,-0.01661093,-0.017982725,-0.01809138,0.006811432,0.00014367334,-0.01366361,-0.025249157]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.005989496,0.020684812,0.010912835,0.0026144625,-0.006390154,0.022287445,0.0022307816,-0.0025024142,-0.015917663,-0.04039175,0.0059419605,0.02459632,-0.0039624386,-0.0023869704,-0.0106751565,-0.01305194,0.012875379,0.015320072,0.0059419605,0.0090725245,-0.002395459,0.011272748,-0.0031967747,-0.007334077,-0.0142878685,0.03938671,0.029906735,-0.015646031,0.015211418,-0.022233117,0.015754683,-0.004142395,-0.01613497,-0.028657226,-0.014695317,0.02835843,0.010749855,0.002246061,0.027502788,0.016746143,0.016868377,0.023115924,-0.0044955174,0.004617752,0.0062101977,0.01614855,0.015415143,-0.014939786,-0.023319649,0.00962258,0.0012350789,0.019082181,-0.02164911,-0.00418314,-0.022314608,0.0016331903,-0.0025193912,0.004699242,0.009914585,-0.029662266,0.018158631,0.009751606,-0.018579662,0.01798207,-0.026660727,-0.004223885,0.00652597,0.00090402685,-0.0059657283,0.01472248,0.014070562,0.022980107,-0.0086650755,-0.017547458,0.017140009,-0.023550536,-0.00861075,-0.012012946,-0.008053903,0.017126428,0.009439228,-0.01053934,-0.011449309,0.016623907,-0.0042204894,-0.021893578,0.020820629,0.017832672,-0.03430718,0.009602208,0.009337367,0.026457002,0.017180754,-0.006115126,0.0062781055,-0.00577219,-0.006033636,0.026212534,0.015075603,0.0077075716,-0.010906043,-0.003183193,-0.059270207,-0.018009232,-0.011177677,-0.007510638,0.011177677,0.0042714207,0.00039662587,0.00031407506,-0.032813203,0.033954058,-0.0068723015,-0.034008387,-0.0071575157,-0.0009990982,0.034904774,-0.007211842,-0.0037655053,-0.01246114,0.021282405,0.008440979,0.03134639,-0.024990188,0.02458274,-0.0073408675,-0.017370896,-0.006855325,-0.0234283,-0.02266773,0.024351852,0.024664229,0.01882413,0.01838952,-0.023985147,0.033492286,-0.013289619,0.01715359,-0.023197414,-0.008862009,0.011931457,0.020698395,0.0045057037,0.023523372,0.0058027487,0.0107906,0.0139075825,0.027543534,0.0030796332,-0.009901004,-0.012793889,-0.014763225,0.02920049,0.02944496,0.0042374665,0.01187713,-0.005503953,-0.0003414505,-0.012793889,-0.0031764023,-0.009174387,0.024677811,-0.0085496325,-0.009242295,0.022042975,0.013574833,0.02416171,-0.012135181,0.00852926,-0.000988912,-0.025003769,0.03808287,-1.2023185e-05,-0.008814474,0.00068205217,0.013099476,0.02341472,-0.028059635,-0.012060482,-0.0078501785,-0.026878033,0.014559501,-0.009330575,0.0030915171,-0.030504327,0.022925781,0.0042306758,-0.029743755,-0.0030100273,-0.0059419605,0.016556,0.016012734,-0.00043121655,-0.00077882124,-0.6679986,-0.021445384,0.007585337,-0.019394558,0.0071031894,0.016121387,0.011184467,-0.006716113,-0.038028546,0.033193488,-0.021540456,-0.015157092,0.003588944,-0.029553613,-0.01923158,-0.040011466,0.00047238584,-0.02566927,-0.012264206,0.0053239963,-0.0115783345,0.0014337102,-0.006464853,0.016678235,-0.0009931562,0.007958831,0.0042883977,-0.024976606,0.02182567,-0.007408776,0.005928379,0.023944402,0.0053919046,0.00017910766,0.023034435,-0.03772975,-0.026185371,0.010145473,-0.0026263464,0.03262306,-0.025316147,-0.01739806,0.008848428,0.0046720784,-0.021784924,0.017384479,0.019503212,0.0061762435,0.010430687,0.01028808,-0.0058333073,0.014831133,0.013024777,-0.011136931,0.018158631,-0.023781423,0.024297524,-0.013350736,0.011659824,-0.019258743,0.016379438,-0.0026619982,-0.029417798,-0.0019897078,0.0028674204,0.014016236,-0.006967373,0.0017384478,-0.0053817183,-0.031292062,0.022464005,0.011252375,-0.00022621892,0.012943287,0.016515255,-0.00116038,0.04199438,0.011680196,0.018756222,0.003972625,0.016746143,-0.015265745,-0.019109344,0.0021187332,0.008291582,0.0012579979,0.00869903,-0.0020423366,0.0039828112,-0.0060675903,0.027285483,0.028222615,-0.0027400926,-0.025153168,-0.012006155,0.017425224,0.0029709802,0.0095682535,-0.00075123354,-0.04384148,-0.004417423,0.004801104,0.031319223,0.022273863,0.020888537,0.008006367,-0.01764253,-0.0030219113,0.01647451,-0.024963025,0.016949866,-0.005425859,-0.015415143,0.0036568523,-0.005252693,-0.024311107,0.0144915925,0.017927743,0.008732984,-0.0043359334,0.008991035,-0.0053715324,0.016216459,-0.008977453,0.0008407874,0.006658391,-0.015415143,-0.013540879,0.00861075,-0.010546131,0.015021276,0.006441085,0.019747682,-0.025533453,-0.0030626561,0.012474721,0.0082576275,-0.007829806,0.016936285,-0.0015347234,-0.027326226,0.015496633,-0.0010907742,-0.02049467,-0.021526873,-0.013174175,0.0062611285,0.009846677,0.008318745,-0.006797603,0.005955542,-0.02317025,-0.0056907004,-0.00034166273,0.0012036714,0.003308823,-0.004909757,-0.03370959,0.0068315566,-0.02685087,0.012046901,0.038598973,-0.023509791,-0.019802008,-0.004240862,-0.024609903,-0.00785697,0.034605976,-0.004166163,-0.013337155,0.007829806,-0.01597199,0.0073476583,0.016094225,-0.01790058,0.0032477058,-0.009839887,-0.019014273,-0.0055413027,-0.021295985,0.012420395,0.0031084942,-0.01504844,0.001348825,0.03528506,0.021037934,0.006271315,0.021173751,-0.028575737,0.027244737,0.00014324368,0.013697067,-0.021363894,0.03452449,0.020277364,0.013758185,0.031400714,-0.013235292,-0.020684812,0.017465968,0.027665768,0.025818666,0.022531914,-0.01096037,-0.0023292485,-0.011048651,0.004047324,-0.010138682,0.021363894,0.020399598,0.027217574,0.011313492,-0.006834952,-0.012332114,0.0059759147,0.0024888325,-0.008407026,0.022219537,-0.011177677,0.003035493,0.009174387,-0.004736591,0.04490085,0.009731233,-0.0121012265,-0.0120740635,-0.0018657756,0.024487669,0.034361508,-0.0023224577,-0.0036534567,-0.017044937,-0.012861798,0.012366069,0.004145791,0.0076736175,0.00032723224,-0.0018504963,0.033818245,-0.008162556,0.015442306,0.01355446,-0.0003739191,-0.014858296,0.022205954,-0.00043100433,0.029499287,0.02634835,0.0042680255,0.0029217468,0.00752422,-0.008603958,-0.0030592608,0.02099719,-0.0022375726,-0.020671232,0.011680196,0.009425647,0.0054122773,0.030232694,0.029635103,0.014613827,0.017167173,-0.0054937666,0.0325959,-0.02284429,-0.028820205,-0.00076396635,-0.015646031,0.0019082181,0.003045679,0.008739775,0.0073544495,-0.023102341,0.0115783345,0.0027672558,0.03463314,0.014111307,0.009473182,0.026402676,-0.02566927,-0.01780551,0.009792351,0.008651494,-0.012495094,-0.0050218054,-0.011422145,0.010505386,-0.020100804,0.032025468,0.0037960638,0.03379108,-0.017330151,-0.008115021,-0.009629372,0.0009795746,0.025818666,-0.01780551,-0.009914585,-0.008603958,-0.0039216937,0.0066040647,-0.012916124,-0.013758185,0.008237255,-0.0018674732,-0.006566715,-0.0368062,0.01897353,-0.010322034,0.026212534,-0.023346812,0.001852194,0.0006710171,0.025153168,0.022110883,-0.00526967,0.020358855,0.02835843,0.02041318,0.00042697228,-0.027394135,-0.008916336,0.003863972,0.0075174286,0.027054595,0.004950502,0.02216521,-0.01053255,-0.008128602,0.0002291899,0.0021866413,0.019476049,0.009710861,-0.014600245,-0.009500346,-0.0069809547,0.0034310578,0.015958408,-0.010009657,0.005537907,-0.038626138,-0.034361508,-0.019598283,-0.02192074,0.01388042,0.017180754,0.02443334,0.0014337102,-0.0020151734,0.0021102447,0.009371321,0.003538013,0.00305247,0.018905621,0.020453926,0.022396097,0.005537907,-0.014002654,0.019299487,0.014532338,-0.0039624386,-0.0144915925,0.023767842,-0.0038334134,-0.013113058,-0.001794472,0.01305194,-0.011734523,-0.019747682,0.0029930503,-0.011401773,0.043461196,0.005755213,0.0093916925,0.00095071364,-0.030585816,-0.025207493,-0.005008224,-0.013079104,-0.013282828,-0.042456158,-0.015523796,-0.020100804,-0.021757761,-0.018443845,0.038924932,-0.0064784344,0.017724019,0.0032307287,-0.02459632,0.00522553,-0.032650225,-0.012875379,-0.0036500613,-0.027774421,-0.046231847,-0.016881958,0.005110086,0.026769381,-0.0035414083,-0.013147011,-0.004172954,0.015741102,-0.00736124,-0.0074970564,0.015999153,-0.011069023,-0.0071235616,0.019313069,-0.03237859,0.0007448672,0.006559924,0.027041012,0.0056092106,0.026742218,-0.00010679613,-0.041777074,0.0071371435,-0.00736124,-0.022626985,0.0209157,-0.0044513773,-0.019625446,0.0073001226,-0.010892462,-0.0029421193,-0.011945038,-0.021377476,0.00084333395,0.007585337,-0.01572752,0.012304951,-0.017452387,-0.0048656166,-0.011462891,0.012400023,-0.011225212,0.021635527,0.0069436054,0.018280866,0.011483263,-0.0011136931,0.00047790338,0.00282328,-0.016080642,0.005048969,0.010226963,-0.022599822,0.005093109,-0.0022834104,0.0016790283,-0.014640991,0.01504844,-0.024555575,0.03262306,-0.014559501,0.00017985041,-0.020970028,-0.005167808,-0.018498171,0.008923126,-0.002857234,-0.0064173173,0.001040692,0.010417106,-0.018620405,0.0070284903,-0.007911296,-0.015170674,-0.017533876,0.023319649,0.009228714,0.028222615,-0.009310203,0.012277788,-0.024229618,-0.031835325,-0.02274922,-0.01723508,-0.0052798563,-0.003154332,0.0019421722,0.02652491,0.035692506,-6.0745933e-05,0.034171365,0.009873841,-0.016012734,0.01749313,-0.007972413,0.008182928,-0.022423262,0.001474455,0.013921164,0.0025414613,0.028086798,0.0046381247,0.015550959,-5.2363524e-05,-0.018172212,-0.010097938,-0.014790389,-0.012834635,-0.001972731,0.008637913,-0.0096836975,-0.002622951,-0.022205954,0.0056329784,-0.018416682,-0.017072102,0.00085394457,0.004356306,0.006291687,-0.003504059,0.008413816,0.0029964459,0.003908112,-0.025221076,-0.036507405,-0.017330151,-0.010172636,0.0047230097,-0.0076532452,0.02401231,0.016406601,0.012556211,-0.014831133,-0.014029818,-0.027801584,-0.013948328,0.004760359,-0.0107906,-0.0024107383,-0.021404639,-0.02683729,0.015360816,0.012583374,0.0047264053,-0.025465544,0.0041559767,0.0040439283,-0.01781909,0.009500346,0.014369358,0.024120964,0.012420395,0.022219537,0.019258743,-0.025384055,-0.0006073532,0.0022341772,0.0073069134,0.00062517915,0.028168287,0.021988649,-0.010389943,-0.020725558,-0.0040439283,0.01480397,-0.016827632,-0.017927743,0.03134639,0.011829594,-0.021255242,-0.0065123886,-0.015225001,-0.02667431,-0.008644704,-0.007687199,0.0012189507,-0.0050557596,-0.003274869,0.0048384536,-0.008644704,0.008603958,0.02164911,-0.0023615048,-0.01187713,0.012596956,0.009792351,-0.0069130464,-0.0021153379,0.012135181,0.022857873,0.024487669,0.020548997,-0.017954906,-0.005310415,-0.002731604,0.007721153,-0.014749643,0.028331267,-0.01923158,0.012984033,0.0058842385,-0.003588944,0.0051372494,-0.009860259,-0.015455888,-0.0081693465,-0.0077822707,0.0064716437,0.031781,-0.0022104094,-0.023129506,-0.006773835,-0.007727944,-0.024365433,-0.014790389,0.005385114,-0.0115783345,-0.03797422,-0.0028843973,-0.016460927,0.011096187,0.023482628,0.0043902597,-0.010600457,0.0135884145,0.015143511,-0.027366972,0.0019574515,0.007931668,-0.0008148974,-0.014586664,-0.0004961537,-0.0247593,-0.022477588,0.011591916,0.014124889,-0.054245006,-0.01305194,0.025832249,0.004200117,0.016257204,0.0003824076,-0.008339117,0.013934746,-0.0004108441,0.009473182,0.009846677,0.030069714,0.006464853,0.008311953,0.00067568576,0.0036161074,0.0057450267,-0.030612981,-0.0014719085,0.0042170943,-0.030314185,0.004984456,-0.007042072,0.008182928,-0.011843176,-0.0011187863,-0.028467083,-0.012705609,-0.012080855,0.04867654,0.022817127,-0.02936347,-0.029091839,-0.0039013214,0.00393188,0.0158905,-0.02048109,-0.03452449,0.0018097514,-0.01739806,-0.029417798,-0.022776384,-0.013948328,0.037403792,0.010525758,-0.0075717554,-0.025981646,-0.014817552,-0.013771767,-0.013663113,-0.023550536,-0.0046652877,-0.009717652,0.023319649,-0.015795428,0.01897353,-0.008019949,0.0016476207,-0.021798506,0.008651494,-0.005755213,-0.019109344,0.025560616,0.0016230041,-0.033845406,0.018538916,0.006145685,-0.016596744,-0.0028504434,0.0006035334,-0.020711977,-0.015279327,-0.0044276095,-0.00027502788,0.013242084,0.02952645,-0.0025227866,-0.013948328,-0.012182716,0.025098842,-0.01571394,0.0055073486,-0.021784924,0.013303201,-0.0077822707,-0.015469469,-0.026416259,-0.012182716,-0.0039013214,0.0009821212,0.003480291,0.012338906,-0.007687199,0.011931457,0.016202876,0.0035414083,0.0042917933,0.025071677,0.0035821532,0.016202876,0.015347235,-0.01864757,-0.02007364,0.0018538916,-0.02299369,0.00074401835,-0.010607248,-0.0074902656,-0.011225212,-0.02107868,0.00861075,0.01923158,-0.030151205,0.023455465,0.0040439283,-0.00068459875,0.01838952,0.012080855,0.0039624386,-0.011299911,-0.017886998,-0.010091146,-0.01882413,-0.016515255,0.044792194,0.014994113,0.018009232,0.0066516004,0.22045691,-0.017289408,0.0024480878,0.028195452,0.021363894,0.0011654731,0.012922915,0.027665768,-0.02709534,0.0034616163,0.009982494,-0.013113058,-0.0050965045,0.003704388,0.00710998,0.0032188448,-0.03713216,-0.037458118,-0.030667307,0.028494246,0.012223462,0.013934746,0.013500134,-0.02651133,0.05144719,-0.0012308346,-0.01178885,-0.00048511862,-0.0015355723,-0.004407237,-0.0065327613,-0.009154014,0.029662266,0.03705067,-0.010600457,-0.017764764,-0.0035108498,0.0048893844,0.018932784,-0.0032680782,-0.012447558,0.006967373,0.0046788696,-0.011524008,-0.009724443,-0.0036602477,-0.009710861,-0.010179427,0.0030422837,0.022314608,-0.016325112,-0.010247335,0.017995652,0.02710892,-0.0014719085,-0.015836174,0.0054326495,-0.014627409,0.0048282673,-0.006841743,-0.02433827,0.0073408675,-0.023781423,0.021445384,-0.03262306,0.0021696645,-0.032894693,-0.0050387825,0.023509791,-0.019693354,-0.032134123,0.0033173116,-0.014274287,-0.0031475413,-0.027529951,-0.009751606,0.027950982,0.021010771,0.015673194,0.013425435,-0.006967373,-0.005480185,-0.011680196,-0.01866115,-0.0025024142,-0.0074563115,0.015157092,-0.019679774,0.012759935,0.0027180226,0.004189931,0.0016926099,-0.018009232,0.013282828,-0.010647993,0.01866115,0.0014014539,0.022287445,0.013513716,-0.02550629,-0.009255877,0.061334614,0.007680408,0.0069877454,0.0019574515,0.005870657,-0.0071031894,0.016039899,0.020046476,-0.019055018,0.015225001,-0.050197683,0.002634835,-0.0031747045,-0.012529048,-0.006091358,-0.0050387825,-0.0019116135,0.002325853,-0.019340232,-0.0027876282,-0.02441976,-0.021295985,0.0053409734,0.011340656,-0.019693354,-0.02550629,-0.0018997296,-0.008678658,-0.002743488,-0.014247123,-0.0111708855,0.0022681311,0.016678235,-0.012400023,-0.025465544,-0.011551171,-0.026280442,0.0062169884,0.009466392,0.0024616693,-0.010437478,0.0039013214,-0.022803547,0.0073748217,-0.013826093,0.016039899,0.011938247,0.0047875224,-0.016080642,0.0012435674,0.0034208714,-0.013690277,-0.01598557,0.021947904,-0.0060234503,-0.02367277,-0.015238582,-0.0056329784,-0.0033478702,-0.017099265,-0.027149666,0.02007364,0.013153803,-0.028385594,-0.008508887,-0.17514862,0.011503635,0.019679774,-0.051800314,0.014070562,0.027570697,0.018783385,0.015822591,-0.034605976,-0.011483263,0.02149971,-0.022341771,-0.0019523584,-0.011924665,-0.004852035,-0.011313492,-0.013751394,0.0143150315,0.013975491,0.014640991,0.052669536,-0.015347235,-0.007884133,0.012006155,-0.017520295,-0.018701896,0.011564753,0.033193488,0.013377899,-0.05161017,-0.01723508,-0.012902543,0.03188965,0.020236619,-0.01187713,0.0045702164,0.018742641,-0.026823707,-0.022477588,0.007761898,-0.0051644123,0.014029818,-0.008977453,0.0055311164,-0.01629795,0.0041016503,0.03645308,0.001794472,0.0010814368,-0.011279538,0.014342194,-0.0057246545,0.0072797504,-0.00581633,0.0020134756,0.009602208,0.017465968,-0.0060574044,-0.005846889,-0.02075272,-0.020358855,-0.012583374,0.014165633,-0.0038605765,0.014640991,0.014192797,-0.014233542,0.020426761,-0.03721365,0.018430263,0.0151027655,-0.018104304,-0.0040337425,-0.0018827526,6.488409e-05,0.026592819,0.005442836,0.009466392,0.028711552,0.00016955809,-0.01053255,0.04096218,-0.020847792,0.022273863,-0.0061015445,-0.010410314,0.002337737,0.034062713,-0.0127735175,-0.028222615,0.010084355,-0.040527567,0.020793466,-0.002366598,0.017859835,0.012338906,0.005795958,-0.018892039,0.008787311,-0.008087857,-0.02015513,-0.0024379015,-0.00978556,0.021228079,0.01864757,-0.011435728,-0.014410103,0.014654572,0.03705067,0.0037587143,-0.009235504,-0.014980531,0.0047705453,0.01129312,0.0030541676,0.012793889,-0.010070774,-0.0023869704,-0.019014273,0.02325174,0.047046743,0.0095682535,-0.015768265,0.01488546,-0.013140221,-0.01673256,-0.09528867,-0.0043325378,0.017751181,0.021200914,0.0074155666,0.043868646,-0.00018473131,0.012563002,-0.042266015,0.024283944,-0.043922972,-0.018878456,0.005812935,-0.0041118367,-0.0031169828,-0.011557962,0.0009218527,-0.025384055,-0.02240968,0.028630063,0.00342936,-0.048975337,0.00029964457,-0.010057192,0.018145049,-0.024406178,-0.021540456,0.010281289,-0.0010109821,-0.00085394457,0.01806356,-0.039006423,-0.023659188,-0.016067062,-0.010131892,0.024786463,-0.018538916,0.003989602,0.005028596,0.00075080915,0.022681313,-0.0010602155,-0.022097303,0.00011395831,-0.02099719,-0.0118703395,-0.023686351,0.028820205,0.0023394348,-0.01112335,-0.027638605,-0.005201762,-0.00994854,-0.004498913,-0.022382516,0.008053903,0.009459601,0.030015389,0.015632449,0.017262245,-0.013921164,0.011164094,0.005894425,0.023210995,0.022029394,-0.016623907,-0.02367277,-0.0004796011,0.007218633,-0.0058910293,-0.022531914,-0.0008836544,-0.00022388458,-0.005167808,-0.03346512,-0.0071575157,-0.014124889,-0.0020644069,-0.0029540032,-0.009873841,0.009154014,-0.008929918,0.006600669,-0.0044106324,-0.02359128,0.01940814,0.014681735,-0.017764764,-0.026755799,-0.017778346,-0.012359277,0.021866415,0.013561252,-0.025628524,0.005670328,0.017737601,0.007992785,-0.0075513828,0.004600775,0.010165845,-0.008440979,-0.0034667095,-0.04566142,0.021200914,-0.001229137,-0.025207493,-0.008454561,-0.018756222,0.011700569,-0.010939998,-0.0016111201,0.010138682,-0.012277788,0.004719614,-0.03134639,0.01848459,-0.018280866,0.004926734,0.011619079,-0.020779885,0.007368031,0.0012546025,-0.011469682,0.012508675,0.010797391,0.023129506,-0.010770228,-0.0394682,-0.028412757,0.009249086,-0.009330575,0.008658285,0.021594781,-0.008108229,0.0047433823,0.028086798,-0.009792351,-0.008332326,0.013119848,0.0065022022,0.035094917,-0.024283944,-0.005931774,-0.040174443,0.00032277577,-0.0078501785,0.03210696,0.009249086,0.0024548785,0.0005848587,-0.017302988,-0.0019608468,0.021961486,0.0071575157,-0.0038469948,-0.033818245,-0.011822804,0.009086106,-0.011340656,-0.016501673,0.020046476,0.0019184044,0.029472124,-0.01631153,0.023373974,-0.008237255,0.0046856604,0.00081192644,0.000708791,0.0210651,0.013819302,-0.025587779,-0.0006018357,-0.007918087,0.0059691234,0.037104998,0.03145504,0.015618867,0.010552921,0.002458274,-0.007171097,0.041097995,0.031509366,-0.005161017,-0.026606401,-0.015455888,0.011775268,0.00618643,0.013595206,0.010980743,0.0026076718,0.0048384536,-0.010145473,0.0036704338,-0.0024328083,0.005554884,0.0008768636,0.0026246489,-0.0050218054,-0.010681947,-0.0062577333,0.01580901,-0.0053953,0.001005889,-0.014043399,-0.021282405,-0.03965834,-0.0030711447,-0.027366972,-0.025900157,-0.024392596,0.0051779943,-0.0018471008,-0.0027010455,-0.0015669798,0.021214496,-0.010362779,0.025696432,-0.015211418,-0.014708899,-0.032079797,0.018416682,0.00928304,-0.0038435995,0.0096565345,-0.0011799035,0.035991304,0.004909757,0.019109344,-0.029227654,0.033600938,0.015591704,0.013106267,0.0015491539,-0.00013242083,-0.011069023,-0.0096565345,-0.028983185,-0.0033020324,0.0048044994,-0.0041016503,0.06763649,-0.0017240173,0.009860259,0.031020429,0.008576795,0.0260903,0.021784924,0.0060574044,-0.004549844,-0.017601784,0.013527297,-0.007401985,-0.020250201,-0.017044937,0.013201338,-0.0018895434,0.0029506078,0.021567618,-0.016922703,-0.018742641,-0.0011714151,-0.005857075,0.0014939787,0.011720941,0.010213381,0.00744273,0.014124889,0.006739881,-0.011619079,-0.039631177,0.0024905303,0.02399873,-0.041749913,-0.010804182,-0.0056329784,0.01749313,-0.00744273,-0.010702319,0.020046476,-0.009608999,0.00042888222,0.0015729218,-0.016651072,-0.017914161,-0.018131468,0.0067127175,0.00021635102,-0.0137038585,-0.02518033]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":9,"total_tokens":9}}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."}]},"formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.\n\nGODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
0.6863487582074924
0.7446753117419691
0.7461735095308605
0.7457393025070737
0.7226856190894931
0.6874040869390607
0.7257557474125148
0.7078127895027475
0.7090071799539442
0.7080527122348053
0.735201437283397
0.7628721818956538
0.6927097708950588
0.7104669627641891
0.7219583421357482
0.671874805424928
0.7401782467158062
0.714355382602787
0.7917667155141748
0.7492922941643576
0.8085633575337898
0.7559315715747711
0.6902817159765553
0.7383496978637959
0.7156109216181494
0.6973710375677157
0.6741823258440063
0.7552050451769293
0.6863489276590771
0.6802774066058088
0.7083495046754315
0.726510872149254
0.6885528695415866
0.7066157396877316
0.6837954878072149
0.7531153602221925
0.646738896274869
0.7294897729929248
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."}]},"formatted":"We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.\n\nGODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
[{"sentences":["The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance.","And, we only pre-train ERNIE on the annotated corpus for one epoch.","To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length.","To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.","Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT.","For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs.","We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},{"sentences":["In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively.","In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4.","The total parameters are about 114M."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"}]
{"context":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.","formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.","formatted":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."}]},"question":"How many parameters are used in the model?","answer_model":"chatgpt","top_n":3,"relevance_method":"vector","relevance_model":null}
[{"sentences":["In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE .","The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size."],"sections":[{"title":"Experiments","number":"6"}],"section_type":"main"},{"sentences":["In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary.","To do this with the same template, for each example we randomly selected 4 out of the 5 summaries.","This resulted in fewer ratings per model, but did not increase the work (and cost) of the task."],"sections":[{"title":"F Human Evaluation Details","number":null}],"section_type":"main"},{"sentences":["With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider.","We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"}]
[{"sentences":["We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details).","We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d ."],"sections":[{"title":"Training regimen","number":null}],"section_type":"main"},{"sentences":["An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs.","These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set."],"sections":[{"title":"Training regimen","number":null}],"section_type":"main"},{"sentences":["To train, we use structures from the PDB with a maximum release date of 30 April 2018.","Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering.","We then randomly crop them to 256 residues and assemble into batches of size 128.","We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores.","The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration).","The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."],"sections":[{"title":"Training regimen","number":null}],"section_type":"main"}]
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.","formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.","formatted":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."}]},"formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.\n\nIn this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.\n\nThe pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.","formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.","formatted":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."}]},"formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.\n\nIn this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.\n\nThe pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"context":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.","formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.","formatted":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets.","formatted":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."}]},"question":"How many parameters are used in the model?","answer_model":"chatgpt","top_n":3,"relevance_method":"vector","relevance_model":null}
{"context":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .","formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d ."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.","formatted":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.","formatted":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."}]},"question":"How many parameters are used in the model?","answer_model":"chatgpt","top_n":3,"relevance_method":"vector","relevance_model":null}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.","formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.","formatted":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."}]},"formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.\n\nIn this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.\n\nThe pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.","formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.","formatted":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets.","formatted":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."}]},"formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.\n\nIn experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.\n\nWith our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .","formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d ."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.","formatted":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.","formatted":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."}]},"formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .\n\nAn initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.\n\nTo train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.","formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.","formatted":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets.","formatted":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."}]},"formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.\n\nIn experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.\n\nWith our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .","formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d ."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.","formatted":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.","formatted":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."}]},"formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .\n\nAn initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.\n\nTo train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.","formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.","formatted":"In experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets.","formatted":"With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."}]},"formatted":"In a similar strategy to Raffel et al. (2019) , to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUS BASE , smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUS LARGE . The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size.\n\nIn experiment 2, we evaluated 4 models (PEGASUS LARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries. This resulted in fewer ratings per model, but did not increase the work (and cost) of the task.\n\nWith our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal or exceed state-of-the-art on the 12 summarization tasks we consider. We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .","formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d ."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.","formatted":"An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.","formatted":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."}]},"formatted":"We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 for details). We also fine-tuned these models after CASP14 to add a pTM prediction objective (Supplementary Methods 1.9.7) and use the obtained models for Fig. 2d .\n\nAn initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set.\n\nTo train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"How many parameters are used in the model?","formatted":"How many parameters are used in the model?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7lg270jCwHWfRWhCDyjEqsGhFoWUx","object":"chat.completion","created":1691597347,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The model uses 13 billion parameters."},"finish_reason":"stop"}],"usage":{"prompt_tokens":360,"completion_tokens":8,"total_tokens":368}}
"The model uses 13 billion parameters."
["2023_Touvron.pdf","The model uses 13 billion parameters."]
{"id":"chatcmpl-7lg28xzAUp3GZq8VLux0RvaC4mVYO","object":"chat.completion","created":1691597348,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The model uses a total of 114 million parameters."},"finish_reason":"stop"}],"usage":{"prompt_tokens":488,"completion_tokens":11,"total_tokens":499}}
"The model uses a total of 114 million parameters."
["2019_Zhang.pdf","The model uses a total of 114 million parameters."]
{"id":"chatcmpl-7lg28IRDEKIXaroTbFAog8rnJouNx","object":"chat.completion","created":1691597348,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The background text does not provide information about the number of parameters used in the model."},"finish_reason":"stop"}],"usage":{"prompt_tokens":471,"completion_tokens":17,"total_tokens":488}}
"The background text does not provide information about the number of parameters used in the model."
["2019_Dai.pdf","The background text does not provide information about the number of parameters used in the model."]
{"id":"chatcmpl-7lg28fGA3evJDhPHg3IspGb9fV7Wx","object":"chat.completion","created":1691597348,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The background text does not provide information about the number of parameters used in the model."},"finish_reason":"stop"}],"usage":{"prompt_tokens":370,"completion_tokens":17,"total_tokens":387}}
["2021_Jumper.pdf","The background text does not provide information about the number of parameters used in the model."]
{"id":"chatcmpl-7lg28nH8377eN9H7xHk6DedPpnFLG","object":"chat.completion","created":1691597348,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The model initially used 223M parameters (PEGASUS BASE) and later scaled up to 568M parameters (PEGASUS LARGE)."},"finish_reason":"stop"}],"usage":{"prompt_tokens":326,"completion_tokens":29,"total_tokens":355}}
"The model initially used 223M parameters (PEGASUS BASE) and later scaled up to 568M parameters (PEGASUS LARGE)."
["2020_Zhang.pdf","The model initially used 223M parameters (PEGASUS BASE) and later scaled up to 568M parameters (PEGASUS LARGE)."]
{"id":"chatcmpl-7lg28F89bTQJNr9JGHHQyT4nzrvQS","object":"chat.completion","created":1691597348,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"Three sizes of models are considered: a 220M parameter base version (GODEL B), a 770M parameter version (GODEL L), and a 175B parameter version (GODEL XL)."},"finish_reason":"stop"}],"usage":{"prompt_tokens":628,"completion_tokens":45,"total_tokens":673}}
"Three sizes of models are considered: a 220M parameter base version (GODEL B), a 770M parameter version (GODEL L), and a 175B parameter version (GODEL XL)."
["2022_Peng.pdf","Three sizes of models are considered: a 220M parameter base version (GODEL B), a 770M parameter version (GODEL L), and a 175B parameter version (GODEL XL)."]
[["2023_Touvron.pdf","The model uses 13 billion parameters."],["2019_Dai.pdf","The background text does not provide information about the number of parameters used in the model."],["2022_Peng.pdf","Three sizes of models are considered: a 220M parameter base version (GODEL B), a 770M parameter version (GODEL L), and a 175B parameter version (GODEL XL)."],["2019_Zhang.pdf","The model uses a total of 114 million parameters."],["2021_Jumper.pdf","The background text does not provide information about the number of parameters used in the model."],["2020_Zhang.pdf","The model initially used 223M parameters (PEGASUS BASE) and later scaled up to 568M parameters (PEGASUS LARGE)."]]

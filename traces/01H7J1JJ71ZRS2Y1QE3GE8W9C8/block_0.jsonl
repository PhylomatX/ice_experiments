{"doc":null,"source":"async def summarize_paper(\n        paper: Paper, \n        summary_length: int = 3,\n        model: str = 'chatgpt',\n        context: str = None\n):\n    answer = await summarize_paragraphs(\n        paper.paragraphs,\n        summary_length,\n        context=context,\n        model=model\n    )\n\n    return answer\n"}
{"paper":{"document_id":"2022_Peng.pdf"},"summary_length":3,"model":"chatgpt","context":null}
{"doc":null,"source":"async def summarize_paragraphs(\n    paragraphs: list[Paragraph], summary_length: int = 5, context: str = None, **kwargs\n):\n    if len(paragraphs) > summary_length:\n        # split texts into chunks of size summary_length and recursively summarize\n        paragraphs = await map_async(\n            [paragraphs[i:i+summary_length] for i in range(0, len(paragraphs), summary_length)],\n            lambda chunk: summarize_paragraphs(chunk, summary_length, context, **kwargs)\n        )\n        summary = await summarize_paragraphs(paragraphs, summary_length, context, **kwargs)\n    else:\n        # concatenate summaries and summarize again\n        summary = await summarize(\"\\n\\n\".join([str(p) for p in paragraphs]), context, **kwargs)\n    return summary\n"}
{"paragraphs":[{"sentences":["We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog.","In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses.","Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation.","A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation).","We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics.","Code and data processing scripts are publicly available. 1"],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},{"sentences":["This work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL).","As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced.","GODEL should be of technical interest for two reasons.","First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks.","The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL","be conditioned on information external to the current conversation (e.g., a retrieved document.)","Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models.","We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Our approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019).","Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place?","We take it as a given that it should be fluent and socially engaging.","Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions.","But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user.","In other words, the model must offer utility to the user.","It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["In the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1","[cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) .","We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks.","Our results on these diverse tasks speak to the effectiveness of our approach.","We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["As baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation.","The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research.","While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility).","For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics."," Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities.","BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2 ","The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions.","We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly.","A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix."," Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness.","Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv).","By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets.","Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness).","In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation.","Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},{"sentences":["In the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility.","While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks.","This makes it difficult to analyze results across dialog tasks and datasets.","To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context.","For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat.","The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research.","Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["Considering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ).","The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility.","In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["At its core, GODEL is a standard pre-trained LM, trained on web text data.","We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment.","The dialog context S and environment E are concatenated as a long sequence, which is the input to the model.","A sample training example is shown in Figure 2 ."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food.","System : sure, which area do you prefer ?","User : Bellevue downtown.","<|environment|> Multiple expensive Chinese restaurants in Bellevue downtown.","=> There are multiple restaurants meet your requirement.","peony kitchen is a great Chinese Restaurant.","Would you like to book a table there?","For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens.","For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge.","This has 2 million examples for training.","\u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset.","Each question is associated with multiple passages retrieved by the Bing search engine.","Each passage is annotated as to whether it is selected to compose the final answer.","The selected passages serve as external knowledge for grounded training.","\u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["Similarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions.","\u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples.","Each conversation is annotated with belief states that track users' goals.","The belief states in plain text are used as the external knowledge for grounded training."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ).","The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings.","The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings.","These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions.","We use Byte-Level BPE tokenization as implemented in HuggingFace.","GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) .","We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing.","As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J.","Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs.","Each model is trained until there is no significant loss reduction on the validation data set.","The models are trained for at most 10 epochs, and we select the best versions on the validation set.","We use beam search for inference using a beam size of 5."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We evaluate fine-tuned models in two different setups: few-shot and full.","Our evaluation focuses primarily on few-shot finetuning, for the following reasons.","First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few.","Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models.","Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["GODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples.","We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Evaluation datasets:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["To focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog.","\u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences.","At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences.","Similarly, we use the gold retrieved sentences provided at each turn of the dialog.","\u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively.","Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi.","This dataset is challenging owing to its multi-domain setting and diverse language styles.","For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text.","\u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains.","For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["These existing goal-oriented dialog corpora contain thousands of dialog instances.","For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation.","We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1.","The best model is selected based on the perplexity score on the validation set."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Baseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) .","For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ).","For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ).","For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Automatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer.","\u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Besides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","All BLEU scores in this paper are based on corpuslevel BLEU-4.","In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe.","Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot.","BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ).","time HIT acceptance % > 95.","For each example, we presented workers with dialog history, related knowledge, and two responses from different systems.","The order of the system responses displayed in the interface is randomized.","We consider three questions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Extrinsic evaluation: Which response sounds more useful?","(i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human?","(e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer?","(e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)"],"sections":[{"title":"Model","number":null}],"section_type":"main"},{"sentences":["These were judged on a 5-point Likert scale.","A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."],"sections":[{"title":"Model","number":null}],"section_type":"main"},{"sentences":["Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively.","We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting.","This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence.","We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4 ","dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA.","Table 3 shows the aggregated results on WoW and WoI.","Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods.","Success is the utility function for this task.","As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points.","Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function.","Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting.","However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short.","In general, scaling up the model improves both utility and BLEU scores."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["Our recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3.","In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5 ","We observe that GODEL XL performs better than GPT-3 by a large margin.","This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["We also conducted an ablation study of pretraining stages of GODEL.","Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 .","We observe that dialog pre-training alone hurts performance.","This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation.","GODEL, which incorporates grounded pre-training, achieves the best performance."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["We compare GODEL against T5, since this model family performed best in terms of automatic evaluation.","Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe.","Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks.","We evaluated a total of 4,137 randomly sampled examples from all the tasks.","Table 8 shows the winning percentage of the total.","Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting.","We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks.","It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."],"sections":[{"title":"Human Evaluation Results","number":"5.3"}],"section_type":"main"},{"sentences":["Inter-annotator agreement is shown in Table 9 .","This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings.","Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety.","This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research.","We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic.","Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context.","Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["Table 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation.","Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021).","As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic.","This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic.","We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation.","We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility.","We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing.","Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics.","On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW)."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["We hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric.","These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems.","Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable.","The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging.","The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["In this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways.","First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment.","Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia).","In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness.","Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining.","The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition.","Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small.","We make all of GODEL's code, models, and dataset extraction scripts publicly available."],"sections":[{"title":"Conclusion","number":"7"}],"section_type":"main"},{"sentences":["Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI.","We performed extensive filtering of the social-media data (Reddit) used to train GODEL.","This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit.","This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL).","Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs.","Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available.","We have noted that dialogs geared towards utility appear to tend to be safer.","This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety."],"sections":[{"title":"Ethics Statement","number":null}],"section_type":"main"},{"sentences":["We first provide detailed automatic evaluation for individual datasets.","Results for MultiWOZ are shown in Table 11 .","GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings.","Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively.","Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting.","In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses.","Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem.","Table 14 list the results on CoQA."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["These results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L .","We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU.","Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL .","Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["Table 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts.","We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"},{"sentences":["We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."],"sections":[{"title":"Acknowledgements","number":null}],"section_type":"back"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog.","In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses.","Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation.","A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation).","We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics.","Code and data processing scripts are publicly available. 1"],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},{"sentences":["This work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL).","As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced.","GODEL should be of technical interest for two reasons.","First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks.","The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL","be conditioned on information external to the current conversation (e.g., a retrieved document.)","Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models.","We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Our approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019).","Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place?","We take it as a given that it should be fluent and socially engaging.","Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions.","But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user.","In other words, the model must offer utility to the user.","It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["In the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1","[cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) .","We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks.","Our results on these diverse tasks speak to the effectiveness of our approach.","We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["As baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation.","The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research.","While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility).","For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics."," Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities.","BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2 ","The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions.","We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly.","A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix."," Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness.","Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv).","By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets.","Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness).","In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation.","Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},{"sentences":["In the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility.","While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks.","This makes it difficult to analyze results across dialog tasks and datasets.","To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context.","For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat.","The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research.","Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["Considering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ).","The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility.","In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["At its core, GODEL is a standard pre-trained LM, trained on web text data.","We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment.","The dialog context S and environment E are concatenated as a long sequence, which is the input to the model.","A sample training example is shown in Figure 2 ."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food.","System : sure, which area do you prefer ?","User : Bellevue downtown.","<|environment|> Multiple expensive Chinese restaurants in Bellevue downtown.","=> There are multiple restaurants meet your requirement.","peony kitchen is a great Chinese Restaurant.","Would you like to book a table there?","For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens.","For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"],"sections":[{"title":"Models","number":"4"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge.","This has 2 million examples for training.","\u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset.","Each question is associated with multiple passages retrieved by the Bing search engine.","Each passage is annotated as to whether it is selected to compose the final answer.","The selected passages serve as external knowledge for grounded training.","\u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["Similarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions.","\u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples.","Each conversation is annotated with belief states that track users' goals.","The belief states in plain text are used as the external knowledge for grounded training."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ).","The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings.","The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings.","These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions.","We use Byte-Level BPE tokenization as implemented in HuggingFace.","GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) .","We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing.","As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J.","Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs.","Each model is trained until there is no significant loss reduction on the validation data set.","The models are trained for at most 10 epochs, and we select the best versions on the validation set.","We use beam search for inference using a beam size of 5."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We evaluate fine-tuned models in two different setups: few-shot and full.","Our evaluation focuses primarily on few-shot finetuning, for the following reasons.","First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few.","Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models.","Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["GODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples.","We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["Evaluation datasets:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["To focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog.","\u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences.","At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences.","Similarly, we use the gold retrieved sentences provided at each turn of the dialog.","\u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively.","Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi.","This dataset is challenging owing to its multi-domain setting and diverse language styles.","For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text.","\u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains.","For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["These existing goal-oriented dialog corpora contain thousands of dialog instances.","For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation.","We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1.","The best model is selected based on the perplexity score on the validation set."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Baseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) .","For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ).","For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ).","For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Automatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer.","\u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Besides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","All BLEU scores in this paper are based on corpuslevel BLEU-4.","In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe.","Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot.","BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ).","time HIT acceptance % > 95.","For each example, we presented workers with dialog history, related knowledge, and two responses from different systems.","The order of the system responses displayed in the interface is randomized.","We consider three questions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Extrinsic evaluation: Which response sounds more useful?","(i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human?","(e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer?","(e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)"],"sections":[{"title":"Model","number":null}],"section_type":"main"},{"sentences":["These were judged on a 5-point Likert scale.","A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."],"sections":[{"title":"Model","number":null}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively.","We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting.","This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence.","We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4 ","dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA.","Table 3 shows the aggregated results on WoW and WoI.","Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods.","Success is the utility function for this task.","As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points.","Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function.","Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting.","However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short.","In general, scaling up the model improves both utility and BLEU scores."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["Our recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3.","In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5 ","We observe that GODEL XL performs better than GPT-3 by a large margin.","This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["We also conducted an ablation study of pretraining stages of GODEL.","Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 .","We observe that dialog pre-training alone hurts performance.","This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation.","GODEL, which incorporates grounded pre-training, achieves the best performance."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["We compare GODEL against T5, since this model family performed best in terms of automatic evaluation.","Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe.","Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks.","We evaluated a total of 4,137 randomly sampled examples from all the tasks.","Table 8 shows the winning percentage of the total.","Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting.","We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks.","It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."],"sections":[{"title":"Human Evaluation Results","number":"5.3"}],"section_type":"main"},{"sentences":["Inter-annotator agreement is shown in Table 9 .","This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings.","Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety.","This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research.","We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic.","Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context.","Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["Table 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation.","Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021).","As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic.","This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic.","We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation.","We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility.","We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing.","Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics.","On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW)."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["We hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric.","These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems.","Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable.","The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging.","The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["In this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways.","First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment.","Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia).","In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness.","Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining.","The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition.","Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small.","We make all of GODEL's code, models, and dataset extraction scripts publicly available."],"sections":[{"title":"Conclusion","number":"7"}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI.","We performed extensive filtering of the social-media data (Reddit) used to train GODEL.","This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit.","This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL).","Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs.","Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available.","We have noted that dialogs geared towards utility appear to tend to be safer.","This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety."],"sections":[{"title":"Ethics Statement","number":null}],"section_type":"main"},{"sentences":["We first provide detailed automatic evaluation for individual datasets.","Results for MultiWOZ are shown in Table 11 .","GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings.","Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively.","Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting.","In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses.","Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem.","Table 14 list the results on CoQA."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["These results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L .","We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU.","Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL .","Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["Table 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts.","We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":[{"sentences":["We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."],"sections":[{"title":"Acknowledgements","number":null}],"section_type":"back"}],"summary_length":3,"context":null,"model":"chatgpt"}
{"doc":null,"source":"async def summarize(\n    text: str, context: str = None, model: str = \"chatgpt\"\n) -> str:\n    if context is None:\n        prompt = F(\nf\"\"\"Summarize the following text:\n\n{text}\n\nSummary:\n\"\"\"\n        ).strip()\n    else:\n        prompt = F(\nf\"\"\"You are given the following context:\n\n{context}\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n{text}\n\nSummary:\n\"\"\"\n        ).strip()\n    answer = await recipe.agent(model).complete(prompt=prompt)\n    return answer\n"}
{"text":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","context":null,"model":"chatgpt"}
{"text":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","context":null,"model":"chatgpt"}
{"text":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","context":null,"model":"chatgpt"}
{"text":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","context":null,"model":"chatgpt"}
{"text":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","context":null,"model":"chatgpt"}
{"text":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","context":null,"model":"chatgpt"}
{"text":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","context":null,"model":"chatgpt"}
{"text":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","context":null,"model":"chatgpt"}
{"text":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","context":null,"model":"chatgpt"}
{"text":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","context":null,"model":"chatgpt"}
{"text":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","context":null,"model":"chatgpt"}
{"text":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","context":null,"model":"chatgpt"}
{"text":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","context":null,"model":"chatgpt"}
{"text":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","context":null,"model":"chatgpt"}
{"text":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","context":null,"model":"chatgpt"}
{"text":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","context":null,"model":"chatgpt"}
{"doc":"Generate an answer to a question given some context.","source":"    async def complete(\n        self,\n        *,\n        prompt: str,\n        stop: Stop = None,\n        verbose: bool = False,\n        default: str = \"\",\n        max_tokens: int = 256,\n    ) -> str:\n        \"\"\"Generate an answer to a question given some context.\"\"\"\n        if verbose:\n            self._print_markdown(prompt)\n        response = await self._complete(prompt, stop=stop, max_tokens=max_tokens)\n        completion = self._extract_completion(response)\n        if verbose:\n            self._print_markdown(completion)\n        return completion\n"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","formatted":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","formatted":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","formatted":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","formatted":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","formatted":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","formatted":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","formatted":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","formatted":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","formatted":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","formatted":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","formatted":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","formatted":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"doc":"Send a completion request to the OpenAI API with the given prompt and parameters.","source":"    async def _complete(self, prompt, **kwargs) -> dict:\n        \"\"\"Send a completion request to the OpenAI API with the given prompt and parameters.\"\"\"\n        kwargs.update(\n            {\n                \"model\": self.model,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"n\": 1,\n            }\n        )\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = await openai_chatcomplete(messages, **kwargs)\n        if \"choices\" not in response:\n            raise ValueError(f\"No choices in response: {response}\")\n        return response\n"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","formatted":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","formatted":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","formatted":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","formatted":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","formatted":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","formatted":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","formatted":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","formatted":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","formatted":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","formatted":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","formatted":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","formatted":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"doc":"Send a completion request to the OpenAI API and return the JSON response.","source":"@trace\nasync def openai_chatcomplete(\n    messages: list[dict[str, str]],\n    stop: Optional[str] = \"\\n\",\n    top_p: float = 1,\n    temperature: float = 0,\n    model: str = \"gpt-3.5-turbo\",\n    max_tokens: int = 256,\n    logit_bias: Optional[Mapping[str, Union[int, float]]] = None,\n    n: int = 1,\n    cache_id: int = 0,  # for repeated non-deterministic sampling using caching\n) -> dict:\n    \"\"\"Send a completion request to the OpenAI API and return the JSON response.\"\"\"\n    params = {\n        \"messages\": messages,\n        \"stop\": stop,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"n\": n,\n    }\n    if logit_bias:\n        params[\"logit_bias\"] = logit_bias  # type: ignore[assignment]\n    response = await _post(\"chat/completions\", json=params, cache_id=cache_id)\n    if isinstance(response, TooLongRequestError):\n        raise response\n    add_fields(total_tokens=extract_total_tokens(response))\n    return response\n"}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","formatted":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","formatted":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","formatted":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","formatted":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","formatted":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","formatted":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","formatted":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","formatted":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","formatted":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","formatted":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","formatted":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","formatted":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJUIyfMhUT6AEOCKkpw7bl4Z6N5L","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."},"finish_reason":"stop"}],"usage":{"prompt_tokens":53,"completion_tokens":41,"total_tokens":94}}
"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."
{"id":"chatcmpl-7mJUIhsFS27qdRSJXVLEwOAmxby8E","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics."},"finish_reason":"stop"}],"usage":{"prompt_tokens":280,"completion_tokens":45,"total_tokens":325}}
"The text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics."
{"id":"chatcmpl-7mJUIjWaDTuz9xcnWVC4xYjdMjx2O","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed."},"finish_reason":"stop"}],"usage":{"prompt_tokens":589,"completion_tokens":90,"total_tokens":679}}
"The text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed."
{"id":"chatcmpl-7mJUIG5YVWArSi0hcDiXd0SZ9Rtpi","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"finish_reason":"stop"}],"usage":{"prompt_tokens":278,"completion_tokens":79,"total_tokens":357}}
"The text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor."
{"id":"chatcmpl-7mJUIVbHH28jqz2VXprpzpa3Vjy1P","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library."},"finish_reason":"stop"}],"usage":{"prompt_tokens":483,"completion_tokens":100,"total_tokens":583}}
{"id":"chatcmpl-7mJUIgCNV6zARbYearvrGVerPMI2a","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains."},"finish_reason":"stop"}],"usage":{"prompt_tokens":334,"completion_tokens":81,"total_tokens":415}}
{"id":"chatcmpl-7mJUI2AvZC2QbplZpqd6nfrKujqCH","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3."},"finish_reason":"stop"}],"usage":{"prompt_tokens":283,"completion_tokens":103,"total_tokens":386}}
{"id":"chatcmpl-7mJUI0tMT7NHLDSTumv87noUfsU5M","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models."},"finish_reason":"stop"}],"usage":{"prompt_tokens":600,"completion_tokens":101,"total_tokens":701}}
"The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library."
"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains."
"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3."
"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models."
{"id":"chatcmpl-7mJUIlrPKlfUbCEexNLWIdK4LMQxG","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content."},"finish_reason":"stop"}],"usage":{"prompt_tokens":191,"completion_tokens":114,"total_tokens":305}}
"The text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content."
{"id":"chatcmpl-7mJUIr0GBa3VNfQpunDIhj8L1sO5P","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment."},"finish_reason":"stop"}],"usage":{"prompt_tokens":1061,"completion_tokens":133,"total_tokens":1194}}
{"id":"chatcmpl-7mJUIGE1ADbwhDi7YzoSPvp6YPxvK","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary."},"finish_reason":"stop"}],"usage":{"prompt_tokens":573,"completion_tokens":128,"total_tokens":701}}
"The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment."
"The text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary."
{"id":"chatcmpl-7mJUIi9htDB11YIvg6l901pGuh3jG","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released."},"finish_reason":"stop"}],"usage":{"prompt_tokens":538,"completion_tokens":144,"total_tokens":682}}
"The text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released."
{"id":"chatcmpl-7mJUIKVvUlMEaE3bYwgwBxK5F2bmp","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog."},"finish_reason":"stop"}],"usage":{"prompt_tokens":365,"completion_tokens":125,"total_tokens":490}}
"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog."
{"id":"chatcmpl-7mJUIS77NQk4zijOGlE6gTFRJyiBW","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding."},"finish_reason":"stop"}],"usage":{"prompt_tokens":538,"completion_tokens":165,"total_tokens":703}}
"The text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding."
{"id":"chatcmpl-7mJUIe0jXOfcnbSExaZJddgyDkM2q","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models."},"finish_reason":"stop"}],"usage":{"prompt_tokens":683,"completion_tokens":179,"total_tokens":862}}
"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models."
{"id":"chatcmpl-7mJUIEkgxFzl1GDiLy5zNcM6bjeLU","object":"chat.completion","created":1691749010,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA."},"finish_reason":"stop"}],"usage":{"prompt_tokens":318,"completion_tokens":220,"total_tokens":538}}
"The text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA."
{"paragraphs":["The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.","The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.","The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment.","The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.","The text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.","The text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA.","The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.","The text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.","The text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor.","The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.","The text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.","The text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed.","The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.","The text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.","The text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content.","The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.","The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.","The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.","The text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.","The text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.","The text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.","The text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.","The text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.","The text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.","The text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.","The text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."],"summary_length":3,"context":null,"model":"chatgpt"}
{"text":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment.","context":null,"model":"chatgpt"}
{"text":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA.","context":null,"model":"chatgpt"}
{"text":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor.","context":null,"model":"chatgpt"}
{"text":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed.","context":null,"model":"chatgpt"}
{"text":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content.","context":null,"model":"chatgpt"}
{"text":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments.","context":null,"model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment.","formatted":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA.","formatted":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor.","formatted":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed.","formatted":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content.","formatted":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments.","formatted":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment.","formatted":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA.","formatted":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor.","formatted":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed.","formatted":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content.","formatted":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments.","formatted":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment.","formatted":"The text introduces GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on a utility-driven suite of benchmarks. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's ability to fulfill user goals. The authors explore this concept by finetuning the GODEL model on various tasks and show improved performance in goal-directed tasks. They also release three versions of GODEL as baselines for future research and provide code to download datasets for fine-tuning and evaluation. The release is fully open sourced and integrated with HuggingFace's transformers library.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and have shown strong performance gains in competitions. However, the evaluation of dialog pre-training has mostly focused on intrinsic measures, such as relevance, informativeness, and humanness. The text introduces the concept of utility as a unified measure of evaluation in dialog tasks and datasets. The goal is to generate human-like conversational responses that are useful to the user in a given context. The text also outlines the task of open-domain goal-directed dialog generation and the conditional probability of generating target sentences based on a source dialog context and environment."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA.","formatted":"The text discusses the environment E, which represents the state of the world and external knowledge needed for utility. In the pretraining stage, E is usually absent but can be substituted with a grounding text during the third phase of pre-training. GODEL is a pre-trained LM that uses a sequence-to-sequence Transformer model to generate responses based on dialog history and environment. It is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. For the final grounded dialog pre-training phase, a corpus is used that supports knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used for conversational modeling. The DSTC7 Task 2 corpus provides examples for training conversational models with informational responses grounded in external knowledge. MS MARCO is a question-answering dataset that uses passages from Bing search engine as external knowledge. UnifiedQA is a question-answering corpus that combines different datasets. The Schema-Guided Dialog dataset contains task-oriented conversations with belief states as external knowledge. The models used in the study include GODEL B, GODEL L, and GODEL XL, with varying parameters and embeddings. GODEL XL is initialized from GPT-3 and a substitute model pre-trained from GPT-J is released.\n\nThe text discusses the training and evaluation process of three models, GODEL B, GODEL L, and GODEL XL. These models were trained on Nvidia V100 machines and were trained until there was no significant loss reduction on the validation data set. The models were trained for a maximum of 10 epochs and the best versions were selected based on the validation set. Inference was done using beam search with a beam size of 5.\n\nThe evaluation of the models focused primarily on few-shot finetuning, which is important because goal-directed dialog datasets are expensive and smaller compared to chitchat datasets. Few-shot finetuning also directly evaluates the effectiveness of the pre-trained models. It is a realistic approach in scenarios where fast turnaround of updated models and greater developer control over model characteristics are desired.\n\nGODEL can be used as an initial model for finetuning in open-domain goal-directed dialog tasks with a small number of annotated examples. The evaluation of GODEL includes its success in finetuning knowledge-grounded response generation, task-oriented dialog, and conversational QA."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor.","formatted":"The text describes several evaluation datasets used for testing dialogue systems. The datasets include Wizard of Wikipedia, which consists of multi-turn conversations between an apprentice and a wizard who has access to Wikipedia sentences. Wizard of Internet is another dataset that is grounded on internet retrieved sentences. MultiWOZ is a task-oriented dialog dataset that spans multiple domains, and CoQA is a conversational question answering dataset collected from diverse domains.\n\nThe text discusses the methodology used in the study, including the use of existing goal-oriented dialog corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics.\n\nThe text discusses various metrics used to evaluate the performance of a model in generating responses. These metrics include F R 1, F K 1, Inform, Success, and Combined score. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed.","formatted":"The text describes a human evaluation setup conducted to assess the responses generated by GODEL. The evaluation focused on determining whether the responses were useful, human-like, and safe. Master-level workers were hired through Amazon Mechanical Turk, and they were presented with dialog history, related knowledge, and two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-likeness, and safety using a 5-point Likert scale. The evaluation interface is shown in Appendix Figure 3.\n\nThe text discusses the results of experiments comparing the performance of different models in few-shot and full fine-tuning settings. The GODEL model shows improvements over T5 and DIALOGPT on all metrics, particularly in few-shot fine-tuning. Open-domain dialog models like DialoGPT and Blenderbot perform poorly in comparison. GODEL also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. When compared to GPT-3, GODEL XL performs significantly better. The ablation study shows that dialog pre-training alone is not sufficient for good performance, and incorporating grounded pre-training is necessary.\n\nThe text discusses a comparison between the GODEL and T5 models in terms of automatic evaluation and human evaluation. GODEL, which is initialized from T5, is preferred by human raters in both Extrinsic and Intrinsic evaluations on all tasks. The text also mentions the inter-annotator agreement and suggests that Utility is more reliably evaluated than Humanness. Additionally, the correlation between human evaluation results and several automated evaluation metrics is analyzed."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content.","formatted":"The text discusses the correlation between different evaluation metrics for dialogue systems. The author observes that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The author suggests that this is because language modeling metrics focus more on fluency and well-formedness, while lexical metrics are better at recognizing utility. The author also notes that the chrF metric performs well in extrinsic evaluation, even though it is not considered superior to other metrics. The text also mentions the correlation between safety and automated metrics, suggesting that unsafe statements are less likely to be favored by reference-based metrics. The author argues for adopting extrinsic evaluation for dialogue systems, as conversations usually have goals and evaluating utility is important. The text also introduces a new pretrained model for dialogue called GODEL, which is designed for open-ended goal-directed dialog and performs well in comparison to other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses in dialog systems. The authors filtered social media data used to train GODEL, a conversational AI model, by removing offensive words, troll responses, and inappropriate subreddits. However, they acknowledge that there may still be undetected forms of social bias and toxicity in the training data. The authors provide evaluation results for different datasets, showing that GODEL B performs well in terms of combined score and BLEU and F R 1 metrics. They also note that dialogs focused on utility tend to be safer. GODEL XL, with a decoder-only transformer architecture, has lower BLEU scores compared to GODEL and GODEL L, which have encoder-decoder transformer models that are more likely to copy words directly from the grounding.\n\nThe text explains that due to the inability to distribute the GPT-3 version of GODEL XL, a version based on GPT-J called GODEL GPT-J XL is released as a substitute. The results of GODEL GPT-3 XL and GODEL GPT-J XL are compared in a few-shot setting. Additionally, examples are provided to show that GODEL L based on T5 is more extractive, while GODEL XL based on GPT-J is more abstractive and aims to introduce more factual content."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments.","formatted":"The author expresses gratitude to Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for their helpful discussions and comments."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJUQg7QqqfE7cQGxttSBG6Mys9Lr","object":"chat.completion","created":1691749018,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."},"finish_reason":"stop"}],"usage":{"prompt_tokens":57,"completion_tokens":40,"total_tokens":97}}
"The author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."
{"id":"chatcmpl-7mJUQyA6HmPsGW7rLEA6ZNkysolDQ","object":"chat.completion","created":1691749018,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation."},"finish_reason":"stop"}],"usage":{"prompt_tokens":350,"completion_tokens":102,"total_tokens":452}}
"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation."
{"id":"chatcmpl-7mJUQJhdPcCnLGIvOQ1Xatbv2qyMw","object":"chat.completion","created":1691749018,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA."},"finish_reason":"stop"}],"usage":{"prompt_tokens":505,"completion_tokens":115,"total_tokens":620}}
"The text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA."
{"id":"chatcmpl-7mJUQZI2bhK8eSXl4AXJyZqJEhLkF","object":"chat.completion","created":1691749018,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics."},"finish_reason":"stop"}],"usage":{"prompt_tokens":337,"completion_tokens":108,"total_tokens":445}}
"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics."
{"id":"chatcmpl-7mJUQSyGTCyGBChh3EYvxdXiUs23m","object":"chat.completion","created":1691749018,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests."},"finish_reason":"stop"}],"usage":{"prompt_tokens":221,"completion_tokens":126,"total_tokens":347}}
"The text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests."
{"id":"chatcmpl-7mJUQ3CLX8kozpRwrJFNrDb0NJmmB","object":"chat.completion","created":1691749018,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL."},"finish_reason":"stop"}],"usage":{"prompt_tokens":474,"completion_tokens":195,"total_tokens":669}}
"The text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL."
{"paragraphs":["The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.","The text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.","The text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests.","The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.","The text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.","The author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.","The text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.","The text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests."],"summary_length":3,"context":null,"model":"chatgpt"}
{"paragraphs":["The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.","The text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.","The author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."],"summary_length":3,"context":null,"model":"chatgpt"}
{"text":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests.","context":null,"model":"chatgpt"}
{"text":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments.","context":null,"model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests.","formatted":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments.","formatted":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests.","formatted":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments.","formatted":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests.","formatted":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It emphasizes the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. They introduce the concept of utility as a unified measure of evaluation and outline the task of open-domain goal-directed dialog generation.\n\nThe text discusses the GODEL pre-trained language model and its training process. It also mentions different datasets used for conversational modeling, including DSTC7 Task 2, MS MARCO, UnifiedQA, and Schema-Guided Dialog. The training and evaluation process of three GODEL models (GODEL B, GODEL L, and GODEL XL) are described, with a focus on few-shot finetuning. The evaluation of GODEL includes its performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA.\n\nThe text discusses different evaluation datasets for testing dialogue systems, including Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. The methodology used in the study is described, including the use of existing corpora, fine-tuning of the GODEL model, comparison with other pretrained models, and the use of automatic evaluation metrics. Various metrics are used to evaluate model performance, including F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are also reported, with significance determined through statistical tests."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments.","formatted":"The text describes a human evaluation setup to assess the responses generated by GODEL. The evaluation showed that GODEL outperformed other models in terms of usefulness, human-likeness, and safety. GODEL also performed well in knowledge-grounded generation tasks and achieved better utility scores than baseline methods. The comparison between GODEL and T5 models showed that GODEL was preferred by human raters in both Extrinsic and Intrinsic evaluations. The text also analyzed the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between different evaluation metrics for dialogue systems. Language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The chrF metric performs well in extrinsic evaluation. The text also mentions the correlation between safety and automated metrics. The author argues for adopting extrinsic evaluation for dialogue systems. The text introduces a new pretrained model called GODEL, which performs well in comparison to other models. The authors discuss the challenges of ensuring safe and inoffensive responses in dialog systems. They filtered social media data used to train GODEL but acknowledge the possibility of undetected social bias and toxicity. GODEL B performs well in evaluation results. GODEL XL has lower BLEU scores compared to GODEL and GODEL L. A version called GODEL GPT-J XL is released as a substitute for GPT-3. Examples show the differences between GODEL L and GODEL XL.\n\nThe author thanks members of the Microsoft Research Deep Learning team, including Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, and Hao Cheng, for their valuable discussions and comments."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJUXiQhkHahZagB44Tj6I8H6Q1f0","object":"chat.completion","created":1691749025,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance."},"finish_reason":"stop"}],"usage":{"prompt_tokens":359,"completion_tokens":126,"total_tokens":485}}
"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance."
{"id":"chatcmpl-7mJUXzhfHvu0LmBLaKcp43E5Z5Q8Y","object":"chat.completion","created":1691749025,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team."},"finish_reason":"stop"}],"usage":{"prompt_tokens":359,"completion_tokens":207,"total_tokens":566}}
"The text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team."
{"paragraphs":["The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.","The text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team."],"summary_length":3,"context":null,"model":"chatgpt"}
{"text":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team.","context":null,"model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team.","formatted":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team.","formatted":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["Summarize the following text:\n\n",{"source":"text","value":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team.","formatted":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance.\n\nThe text discusses the evaluation of GODEL, a pretrained model for dialogue systems. GODEL outperforms other models in terms of usefulness, human-likeness, and safety. It also performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The comparison with T5 models shows that GODEL is preferred by human raters. The text analyzes the correlation between human evaluation results and automated evaluation metrics. It also discusses the correlation between different evaluation metrics for dialogue systems and argues for adopting extrinsic evaluation. The challenges of ensuring safe and inoffensive responses in dialog systems are addressed, including the possibility of social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The author acknowledges the contributions of members of the Microsoft Research Deep Learning team."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJUhXNsnTklViuOMMi2TNYhNx0wJ","object":"chat.completion","created":1691749035,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance. GODEL is found to be superior in terms of usefulness, human-likeness, and safety compared to other models. It performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The text also discusses the challenges of ensuring safe and inoffensive responses in dialog systems, including social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The contributions of the Microsoft"},"finish_reason":"length"}],"usage":{"prompt_tokens":349,"completion_tokens":256,"total_tokens":605}}
"The text introduces GODEL, a large pretrained language model for dialog that outperforms other models in few-shot finetuning setups. It discusses the importance of utility in dialog models and proposes automated evaluation criteria based on the model's ability to fulfill user goals. The authors provide code and baselines for future research and discuss the impact of large pre-trained language models in Conversational AI. The text also describes the training and evaluation process of GODEL models, including their performance in knowledge-grounded response generation, task-oriented dialog, and conversational QA. Various evaluation datasets and metrics are used to assess the model's performance. GODEL is found to be superior in terms of usefulness, human-likeness, and safety compared to other models. It performs well in knowledge-grounded generation tasks and achieves better utility scores than baseline methods. The text also discusses the challenges of ensuring safe and inoffensive responses in dialog systems, including social bias and toxicity. Different versions of GODEL are mentioned, with GODEL B performing well in evaluation results and GODEL XL having lower BLEU scores compared to GODEL and GODEL L. GODEL GPT-J XL is introduced as a substitute for GPT-3. The contributions of the Microsoft"

{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"Given a corpus of tokens x = (x 1 , . . . , x T ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = t P (x t | x <t ). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x <t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.","formatted":"Given a corpus of tokens x = (x 1 , . . . , x T ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = t P (x t | x <t ). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x <t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In this section, we present some generated text from our best model trained the Wikitext-103 dataset.","formatted":"In this section, we present some generated text from our best model trained the Wikitext-103 dataset."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Trained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples.","formatted":"Trained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples."}]},"formatted":"Given a corpus of tokens x = (x 1 , . . . , x T ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = t P (x t | x <t ). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x <t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.\n\nIn this section, we present some generated text from our best model trained the Wikitext-103 dataset.\n\nTrained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Evaluation datasets:","formatted":"Evaluation datasets:"}]},"formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nThese existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nEvaluation datasets:"},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.","formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.","formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\n(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\n\nThe early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/).","formatted":"Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/)."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"Training and inference details are provided in Supplementary Methods 1.11-1.12 and Supplementary Tables 4, 5 .","formatted":"Training and inference details are provided in Supplementary Methods 1.11-1.12 and Supplementary Tables 4, 5 ."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.","formatted":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."}]},"formatted":"Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/).\n\nTraining and inference details are provided in Supplementary Methods 1.11-1.12 and Supplementary Tables 4, 5 .\n\nTo train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus.","formatted":"We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"The pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets.","formatted":"The pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"T5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans.","formatted":"T5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans."}]},"formatted":"We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus.\n\nThe pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets.\n\nT5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"doc":"Send a completion request to the OpenAI API and return the JSON response.","source":"@trace\nasync def openai_chatcomplete(\n    messages: list[dict[str, str]],\n    stop: Optional[str] = \"\\n\",\n    top_p: float = 1,\n    temperature: float = 0,\n    model: str = \"gpt-3.5-turbo\",\n    max_tokens: int = 256,\n    logit_bias: Optional[Mapping[str, Union[int, float]]] = None,\n    n: int = 1,\n    cache_id: int = 0,  # for repeated non-deterministic sampling using caching\n) -> dict:\n    \"\"\"Send a completion request to the OpenAI API and return the JSON response.\"\"\"\n    params = {\n        \"messages\": messages,\n        \"stop\": stop,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"n\": n,\n    }\n    if logit_bias:\n        params[\"logit_bias\"] = logit_bias  # type: ignore[assignment]\n    response = await _post(\"chat/completions\", json=params, cache_id=cache_id)\n    if isinstance(response, TooLongRequestError):\n        raise response\n    add_fields(total_tokens=extract_total_tokens(response))\n    return response\n"}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Gutenberg Project, which contains books that are in the public domain, and the Books3 section of ThePile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap.","formatted":"Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Gutenberg Project, which contains books that are in the public domain, and the Books3 section of ThePile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"We also removed the comments from the .tex files, and inline-expanded definitions and macros written by users to increase consistency across papers. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once during training, with the exception of the Wikipedia and Books domains, over which we perform approximately two epochs.","formatted":"We also removed the comments from the .tex files, and inline-expanded definitions and macros written by users to increase consistency across papers. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once during training, with the exception of the Wikipedia and Books domains, over which we perform approximately two epochs."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Our training dataset is a mixture of several sources, reported in Table 1 , that cover a diverse set of domains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of data and the percentage they represent in the training set:","formatted":"Our training dataset is a mixture of several sources, reported in Table 1 , that cover a diverse set of domains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of data and the percentage they represent in the training set:"}]},"formatted":"Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Gutenberg Project, which contains books that are in the public domain, and the Books3 section of ThePile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap.\n\nWe also removed the comments from the .tex files, and inline-expanded definitions and macros written by users to increase consistency across papers. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, each token is used only once during training, with the exception of the Wikipedia and Books domains, over which we perform approximately two epochs.\n\nOur training dataset is a mixture of several sources, reported in Table 1 , that cover a diverse set of domains. For the most part, we reuse data sources that have been leveraged to train other LLMs, with the restriction of only using data that is publicly available, and compatible with open sourcing. This leads to the following mixture of data and the percentage they represent in the training set:"},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"Given a corpus of tokens x = (x 1 , . . . , x T ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = t P (x t | x <t ). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x <t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.","formatted":"Given a corpus of tokens x = (x 1 , . . . , x T ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = t P (x t | x <t ). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x <t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"In this section, we present some generated text from our best model trained the Wikitext-103 dataset.","formatted":"In this section, we present some generated text from our best model trained the Wikitext-103 dataset."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Trained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples.","formatted":"Trained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples."}]},"formatted":"Given a corpus of tokens x = (x 1 , . . . , x T ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = t P (x t | x <t ). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x <t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.\n\nIn this section, we present some generated text from our best model trained the Wikitext-103 dataset.\n\nTrained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"Evaluation datasets:","formatted":"Evaluation datasets:"}]},"formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nThese existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nEvaluation datasets:"},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.","formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.","formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\n(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\n\nThe early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/).","formatted":"Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/)."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"Training and inference details are provided in Supplementary Methods 1.11-1.12 and Supplementary Tables 4, 5 .","formatted":"Training and inference details are provided in Supplementary Methods 1.11-1.12 and Supplementary Tables 4, 5 ."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.","formatted":"To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."}]},"formatted":"Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/).\n\nTraining and inference details are provided in Supplementary Methods 1.11-1.12 and Supplementary Tables 4, 5 .\n\nTo train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in relevant_paragraphs))[0]","value":"We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus.","formatted":"We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[1]","value":"The pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets.","formatted":"The pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in relevant_paragraphs))[2]","value":"T5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans.","formatted":"T5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans."}]},"formatted":"We used PEGASUS BASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, we evaluated the latter two using the publicly available C4 corpus.\n\nThe pre-training corpora are a large collection of documents from the Internet and potentially have overlap with the downstream test sets. In this section, we measured the extent of overlap between the pre-training corpus and downstream datasets. We also studied if the pre-trained model was able to exploit memorization to achieve higher performance on the downstream datasets.\n\nT5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7lfzrkPdWFjGfLakF9Mqpt1QAl68Y","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was English Wikipedia."},"finish_reason":"stop"}],"usage":{"prompt_tokens":345,"completion_tokens":8,"total_tokens":353}}
"The training corpus used was English Wikipedia."
["2019_Zhang.pdf","The training corpus used was English Wikipedia."]
{"id":"chatcmpl-7lfzrOjf0rTRAAd2HHtWR5iaTanCl","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was the WikiText-103 dataset."},"finish_reason":"stop"}],"usage":{"prompt_tokens":245,"completion_tokens":12,"total_tokens":257}}
"The training corpus used was the WikiText-103 dataset."
["2019_Dai.pdf","The training corpus used was the WikiText-103 dataset."]
{"id":"chatcmpl-7lfzrcjBUbxWuTORBygecij4s0mdx","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used includes the Gutenberg Project and the Books3 section of ThePile."},"finish_reason":"stop"}],"usage":{"prompt_tokens":279,"completion_tokens":18,"total_tokens":297}}
"The training corpus used includes the Gutenberg Project and the Books3 section of ThePile."
["2023_Touvron.pdf","The training corpus used includes the Gutenberg Project and the Books3 section of ThePile."]
{"id":"chatcmpl-7lfzrnXborVwsFA1a7BeixnfyMWIH","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was the DSTC7 Task 2 corpus, which has 2 million examples for training."},"finish_reason":"stop"}],"usage":{"prompt_tokens":276,"completion_tokens":23,"total_tokens":299}}
{"id":"chatcmpl-7lfzrnZiqymDzUqOgoL7ZVBxCzoZa","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was structures from the Protein Data Bank (PDB) with a maximum release date of 30 April 2018."},"finish_reason":"stop"}],"usage":{"prompt_tokens":381,"completion_tokens":28,"total_tokens":409}}
{"id":"chatcmpl-7lfzrxZlS3INqKXU3BdXUd9d16Gjn","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was the PEGASUS BASE, which was evaluated along with the publicly available C4 corpus for reproducibility."},"finish_reason":"stop"}],"usage":{"prompt_tokens":230,"completion_tokens":28,"total_tokens":258}}
"The training corpus used was the DSTC7 Task 2 corpus, which has 2 million examples for training."
"The training corpus used was structures from the Protein Data Bank (PDB) with a maximum release date of 30 April 2018."
"The training corpus used was the PEGASUS BASE, which was evaluated along with the publicly available C4 corpus for reproducibility."
["2022_Peng.pdf","The training corpus used was the DSTC7 Task 2 corpus, which has 2 million examples for training."]
["2021_Jumper.pdf","The training corpus used was structures from the Protein Data Bank (PDB) with a maximum release date of 30 April 2018."]
["2020_Zhang.pdf","The training corpus used was the PEGASUS BASE, which was evaluated along with the publicly available C4 corpus for reproducibility."]
[["2023_Touvron.pdf","The training corpus used includes the Gutenberg Project and the Books3 section of ThePile."],["2019_Dai.pdf","The training corpus used was the WikiText-103 dataset."],["2022_Peng.pdf","The training corpus used was the DSTC7 Task 2 corpus, which has 2 million examples for training."],["2019_Zhang.pdf","The training corpus used was English Wikipedia."],["2021_Jumper.pdf","The training corpus used was structures from the Protein Data Bank (PDB) with a maximum release date of 30 April 2018."],["2020_Zhang.pdf","The training corpus used was the PEGASUS BASE, which was evaluated along with the publicly available C4 corpus for reproducibility."]]

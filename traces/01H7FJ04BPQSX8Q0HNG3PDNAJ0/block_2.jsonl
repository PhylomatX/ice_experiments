{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["where linear(\u2022) is a linear layer.","Eq. 7 will be used to compute the cross-entropy loss function for dEA."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"where linear(\u2022) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text.","More details of these pre-training tasks can be found from Devlin et al. (2019).","The overall pre-training loss is the sum of the dEA, MLM and NSP loss."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["As shown in Figure 3 , for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT.","We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks.","For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:"],"sections":[{"title":"Fine-tuning for Specific Tasks","number":"3.5"}],"section_type":"main"},"formatted":"As shown in Figure 3 , for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["For relation classification, the task requires systems to classify relation labels of given entity pairs based on context.","The most straightforward way to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification.","In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions.","These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015).","Then, we also take the [CLS] token embedding for classification.","Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively."],"sections":[{"title":"Fine-tuning for Specific Tasks","number":"3.5"}],"section_type":"main"},"formatted":"For relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The specific fine-tuning procedure for entity typing is a simplified version of relation classification.","As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016;Yaghoobzadeh and Sch\u00fctze, 2017;Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively."],"sections":[{"title":"Fine-tuning for Specific Tasks","number":"3.5"}],"section_type":"main"},"formatted":"The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016;Yaghoobzadeh and Sch\u00fctze, 2017;Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."],"sections":[{"title":"Experiments","number":"4"}],"section_type":"main"},"formatted":"In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities.","To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.","The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"formatted":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively.","In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4.","The total parameters are about 114M."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},"formatted":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance.","And, we only pre-train ERNIE on the annotated corpus for one epoch.","To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length.","To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.","Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT.","For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs.","We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},"formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015).","As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data.","Hence, we use a larger batch size and less train- ing epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3.","As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},"formatted":"We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train- ing epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity .","The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.","Open Entity is a completely manually-annotated dataset.","The statistics of these two datasets are shown in Table 1 .","We compare our model with the following baseline models for entity typing: NFGEC.","NFGEC is a hybrid model proposed by Shimaoka et al. (2016).","NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.","As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity . The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1 . We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["UFET.","For Open Entity, we add a new hybrid model UFET for comparison.","UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"UFET. For Open Entity, we add a new hybrid model UFET for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Besides NFGEC and UFET, we also report the result of fine-tuning BERT with the same input format introduced in Section 3.5 for fair com- parison.","Following the same evaluation criteria used in the previous work, we compare NFGEC, BERT, ERNIE on FIGER, and adopt strict accuracy, loose macro, loose micro scores for evaluation.","We compare NFGEC, BERT, UFET, ERNIE on Open Entity, and adopt precision, recall, micro-F1 scores for evaluation."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"Besides NFGEC and UFET, we also report the result of fine-tuning BERT with the same input format introduced in Section 3.5 for fair com- parison. Following the same evaluation criteria used in the previous work, we compare NFGEC, BERT, ERNIE on FIGER, and adopt strict accuracy, loose macro, loose micro scores for evaluation. We compare NFGEC, BERT, UFET, ERNIE on Open Entity, and adopt precision, recall, micro-F1 scores for evaluation."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The results on FIGER are shown in Table 2 . From the results, we observe that: (1) BERT achieves comparable results with NFGEC on the macro and micro metrics.","However, BERT has lower accuracy than the best NFGEC model.","As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability.","(2) Compared with BERT, ERNIE significantly improves the strict accuracy, indicating the external knowledge regularizes ERNIE to avoid fitting the noisy labels and accordingly benefits entity typing."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"The results on FIGER are shown in Table 2 . From the results, we observe that: (1) BERT achieves comparable results with NFGEC on the macro and micro metrics. However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. (2) Compared with BERT, ERNIE significantly improves the strict accuracy, indicating the external knowledge regularizes ERNIE to avoid fitting the noisy labels and accordingly benefits entity typing."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The results on Open Entity are shown in Table 3. From the table, we observe that: (1) BERT and ERNIE achieve much higher recall scores than the previous entity typing models, which means pre-training language models make full use of both the unsupervised pre-training and manuallyannotated training data for better entity typing.","(2) Compared to BERT, ERNIE improves the precision by 2% and the recall by 2%, which means the informative entities help ERNIE predict the labels more precisely."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"The results on Open Entity are shown in Table 3. From the table, we observe that: (1) BERT and ERNIE achieve much higher recall scores than the previous entity typing models, which means pre-training language models make full use of both the unsupervised pre-training and manuallyannotated training data for better entity typing. (2) Compared to BERT, ERNIE improves the precision by 2% and the recall by 2%, which means the informative entities help ERNIE predict the labels more precisely."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TA-CRED (Zhang et al., 2017).","The statistics of these two datasets are shown in Lin et al., 2016;Wu et al., 2017;Han et al., 2018b)."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TA-CRED (Zhang et al., 2017). The statistics of these two datasets are shown in Lin et al., 2016;Wu et al., 2017;Han et al., 2018b)."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["PA-LSTM.","Zhang et al. (2017) propose PA-LSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"PA-LSTM. Zhang et al. (2017) propose PA-LSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["C-GCN.","Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.","To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models.","In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison.","As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances.","Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison.","From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%.","However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%.","(2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro.","The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively.","From Table 5 , we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy.","The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA.","(2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5 , we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In conclusion, we find that the pre-trained language models can provide more information for training set.","It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"In conclusion, we find that the pre-trained language models can provide more information for training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The General Language Understanding Evaluation (GLUE) benchmark ) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018;Socher et al., 2013;Dolan and Brockett, 2005;Agirre et al., 2007;Williams et al., 2018;Rajpurkar et al., 2016;Dagan et al., 2006;Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019)."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"The General Language Understanding Evaluation (GLUE) benchmark ) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018;Socher et al., 2013;Dolan and Brockett, 2005;Agirre et al., 2007;Williams et al., 2018;Rajpurkar et al., 2016;Dagan et al., 2006;Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019)."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In Table 6 , we report the results of our evaluation submissions and those of BERT from the leaderboard.","We notice that ERNIE is consistent with BERT BASE on big datasets like MNLI, QQP, QNLI, and SST-2.","The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"In Table 6 , we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERT BASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In short, ERNIE achieves comparable results with BERT BASE on GLUE.","On the one hand, it means GLUE does not require external knowledge for language representation.","On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"In short, ERNIE achieves comparable results with BERT BASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset.","w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively.","As shown in Table 7 , we have the following observations:"],"sections":[{"title":"Ablation Study","number":"4.6"}],"section_type":"main"},"formatted":"In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively. As shown in Table 7 , we have the following observations:"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%.","(2) Although the informative entities bring much knowledge informa-tion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%."],"sections":[{"title":"Ablation Study","number":"4.6"}],"section_type":"main"},"formatted":"(1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%. (2) Although the informative entities bring much knowledge informa-tion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this paper, we propose ERNIE to incorporate knowledge information into language representation models.","Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs.","The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT.","There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as Con-ceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata;"],"sections":[{"title":"Conclusion","number":"5"}],"section_type":"main"},"formatted":"In this paper, we propose ERNIE to incorporate knowledge information into language representation models. Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs. The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT. There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as Con-ceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata;"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(3) annotate more real-world corpora heuristically for building larger pre-training data.","These directions may lead to more general and effective language understanding."],"sections":[{"title":"Conclusion","number":"5"}],"section_type":"main"},"formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":[],"sections":[],"section_type":"main"},"formatted":""},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001)."],"sections":[{"title":"Acknowledgement","number":null}],"section_type":"back"},"formatted":"This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001)."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]},"stop":null,"max_tokens":256}
{"input":"What was the training corpus used?","model":"text-embedding-ada-002","cache_id":0}
{"object":"list","data":[{"object":"embedding","index":0,"embedding":[-0.018201998,-0.010606056,0.0171421,-0.010231558,0.0106767155,0.016082201,-0.00011890742,0.0004330129,-0.00800577,-0.03295579,0.0058471095,0.039626088,-0.0131780775,0.009058603,-0.0021074326,0.020618567,0.028574876,-0.004366784,-0.016421368,-0.031853497,-0.008669973,-0.003038377,-0.019304294,0.017693248,-0.0065007135,-0.0028175649,0.027091017,-0.03179697,-0.00989239,-0.018527035,0.011213731,0.012994361,-0.032842737,0.00019906227,-0.02603112,0.0026038184,-0.0020933005,-0.011708351,0.03942824,0.007306237,0.034538575,0.017891094,0.0017576658,-0.018866202,-0.019954365,-0.017664982,0.0052818297,0.01906405,0.0068292823,0.010090238,-0.0002980966,0.021819787,-0.014697266,-0.013397123,0.00030427933,-0.0012824778,-0.015997408,0.025918063,0.0079351105,-0.019304294,0.014068392,-0.0048048752,-0.0033810777,0.014824454,-0.01472553,0.002655047,-0.014697266,0.01088163,-0.014626605,-0.001612813,-0.0049002664,0.0047942763,0.0033316158,0.0028970572,0.024844032,-0.0047342153,-0.01693012,-0.024660315,0.009108066,-9.423164e-05,0.004038215,0.0017929957,-0.017778039,0.006150947,0.022794895,-0.02234267,0.019445613,0.01074031,-0.008196552,-0.0010493,0.022950346,0.032192666,0.021862183,0.0018318588,-0.007532349,-0.0033634128,-0.0028051992,0.02873033,-0.023727605,-0.014669002,-0.026483342,0.0029288542,-0.022809027,-0.020816416,-0.009263517,-0.0056174644,0.019035786,-0.011899132,0.005430216,-0.0025614225,0.0057587842,0.031203426,-0.0046140933,-0.03679969,0.0019219501,0.0023282447,0.020547908,-0.026992094,0.010309284,-0.001077564,0.011800209,-0.002095067,0.03244704,9.1085065e-05,0.026016986,0.013411255,-0.003169098,-0.0056845914,-0.0006602287,-0.0037944384,-0.0068257493,0.010768574,0.015432129,0.0011526401,0.004168936,0.021000132,-0.005172307,0.022286143,-0.018004151,7.4468946e-05,0.012761184,0.023868926,-0.0032415243,0.00082186336,-0.0055397386,0.027430186,0.010825101,0.03253183,0.008175354,-0.017820435,0.016590953,-0.026554001,-0.0014750261,0.0027610369,0.021890447,-0.018908598,-0.0055432715,0.011065345,-0.018442241,-0.013277001,0.00041932255,0.024420073,0.010189163,-0.003321017,-0.0017320516,0.016661612,0.03549955,0.007214379,0.021664334,-0.020095685,-0.01102295,-0.0016931886,-0.023049269,0.010203294,0.006373526,0.009051537,0.026723586,-0.027189942,-0.008832491,-0.019883705,-0.02020874,0.02176326,0.013029692,0.003260956,0.0011332086,-0.003573626,-0.0007026247,-0.009517893,-0.016633349,-0.023317777,0.0032432908,-0.0066314344,-0.0058188452,-0.023939585,-0.68014425,-0.036008302,0.015771296,-0.018329186,0.025932195,0.0047377483,-0.008274278,-0.012683458,-0.028249841,0.035753924,-0.012068716,0.0016216454,-0.020053288,-0.002073869,0.0014167316,-0.025946327,-0.0123442905,0.008140024,0.016972516,0.01913471,-0.032192666,-0.006126216,0.006147414,0.017311683,-1.3048626e-05,-0.0006169495,-0.014979905,-0.01779217,0.018512903,-7.9161204e-05,-0.00618981,0.004073545,0.009673345,0.0093695065,0.027698694,0.0028140317,-0.033916768,0.021155583,0.0046882867,0.013799884,-0.026992094,-0.0013248738,0.030214187,0.0048260735,-0.013093285,0.006267536,0.010125568,-0.013488981,0.011849671,0.0059813634,-0.015262545,-0.025140803,0.01059899,-0.0027804682,-0.0029606512,-0.0068893433,-0.0004672388,-0.0055786017,0.01622352,-0.0073768967,0.001960813,0.011114807,-0.02105666,-0.022724234,-0.009447233,0.0011729549,-0.020194609,0.011786076,0.028631404,-0.023925453,0.029535852,0.0071154553,-0.00307724,0.0036248546,-0.005882439,0.013616169,-0.0021074326,-0.025437575,0.011305589,-0.0048543373,-0.01671814,0.012542138,-0.004070012,0.008980878,-0.008860756,0.0054019517,-0.023812396,-0.0059319013,0.0026444478,-0.007920979,0.0018883867,0.031118633,-0.017057307,-0.0053560226,-0.005454947,0.011835539,0.0021180315,-0.022653574,0.025762612,-0.0022399197,0.015135357,0.0054902765,0.0032132603,-0.0015730667,0.010365812,0.008231882,0.016350709,0.02959238,0.025875667,-0.018244395,-0.007482887,0.009970116,-0.001709087,0.0012268331,0.0064865816,-0.029705435,0.014979905,0.0056563276,-0.014683134,-0.02802373,0.013213407,-0.021989372,0.015107093,-0.0043844488,0.0033157172,-0.007263841,0.004988591,-0.005829444,0.011552899,-0.022964478,-0.0011703051,0.0028475951,0.020095685,-0.015969144,0.024095036,0.022441594,0.008952613,-0.013305265,-0.01074031,0.0017744475,-0.03629094,-0.004172469,0.014838586,-0.0013416555,-0.016901856,-0.026794245,-0.01842811,0.022611178,-0.0077584605,0.00821775,0.013792818,-0.01202632,-0.02234267,-0.027642164,-0.009588553,-0.020025024,-0.03329496,-0.021226244,0.019120578,-0.02860314,-0.0077443286,0.008620512,-0.005313627,0.0130650215,-0.011708351,-0.011955661,0.009383639,0.028687932,0.00650778,-0.034114614,0.03295579,-0.004140672,0.012648128,0.021000132,-0.0034411387,0.010655518,-0.022385066,-0.011545833,-0.008443862,-0.005500876,-0.0016260616,0.019445613,-0.01451355,0.004352652,0.01394827,0.0021180315,-0.0005988429,0.00052685814,-0.019219501,0.0167888,0.00821775,-0.028377028,-0.015432129,0.028800989,0.017820435,0.0083944,-0.004214865,-0.0016499094,0.009136329,0.019586934,0.033747185,0.016209388,0.03728018,0.0038721643,-0.006836348,-0.021070791,-0.0032273924,-0.016605085,0.029507587,-0.010697913,0.009934787,-0.0019502142,-0.021819787,-0.005313627,-0.011722483,0.017608454,0.0056245304,-0.009553223,-0.023148194,0.020887075,0.011616493,0.013446585,0.030412035,-0.009510827,-0.02873033,0.016605085,0.017014911,0.01493751,0.027981333,-0.014033062,0.012860107,0.013531377,0.017749775,0.01419558,0.028687932,-5.0952436e-05,0.01416025,-0.0007724014,0.039936993,0.0029288542,0.013163946,0.0110370815,0.019120578,-0.023981981,0.05129911,-0.014640738,0.01636484,0.0002788859,-0.018442241,0.007207313,-0.002278783,-0.015629977,-0.02176326,-0.0034287733,-0.011347985,-0.0014829753,0.0069282064,0.0049638604,-0.0014255642,0.03826942,0.032475304,-0.010507132,0.017170364,-0.009517893,0.00860638,0.022611178,-0.004490439,-0.0023918387,-0.013411255,-0.0065925717,0.009496694,0.007412227,0.0065678405,-0.012796514,0.025649555,0.009051537,0.010556594,0.0018318588,0.013856413,0.012754118,-0.03920213,-0.029479323,0.018555298,0.013022626,0.0013584372,-0.003570093,-0.023388438,0.0025402245,-0.020957736,0.0054938095,0.0019749452,0.0349908,-0.021353431,-0.011729549,-0.020109817,0.016534423,0.0167888,-0.015149489,0.0002980966,-0.005165241,-0.011121873,0.007210846,0.0004924114,-0.001308092,0.0513839,-0.0009618583,-0.015813693,-0.0076948665,-0.011051213,-0.005083982,0.009320045,0.0127117215,0.0064901146,0.023586284,0.023600418,0.021607807,-0.002091534,0.0155734485,0.01962933,0.015700636,-0.007899781,-0.029931547,-0.0074899527,-0.020618567,0.060767543,0.043441728,-0.011800209,9.406603e-05,-0.007461689,-0.020872943,-0.020010892,-0.011404513,0.009433101,0.0075606126,-0.026228966,-0.010493,0.011312655,-0.016986648,0.002474864,-0.0031849963,-0.017636718,-0.007111922,-0.014216778,-0.022356803,0.008945547,0.0055468045,0.02738779,0.026455078,0.004087677,-0.03784546,0.012047518,0.024844032,0.00024134784,-0.0016136962,0.010132634,0.0045469664,0.020053288,-0.0018565897,-0.03236225,-0.0033174837,-0.0014944576,0.015375601,0.0058153123,-0.0047942763,0.007659537,0.005274764,0.007306237,-0.006839881,0.023105798,-0.0068257493,0.01770738,0.0018707217,0.0117507465,-0.009652147,0.008655841,0.013453651,-0.017778039,-0.009086867,0.028193314,-0.0081682885,0.015022301,-0.004882601,-0.003942824,-0.015163621,-0.013057956,-0.025225597,0.020222872,-0.0011349751,-0.031175163,-0.014612474,0.0067868866,-0.008224816,-0.016562687,0.0033404483,-0.017693248,-0.024815768,-0.0255365,0.014753793,0.023063403,0.012633996,0.037025806,-0.0071825823,0.0019413816,0.0077937907,-0.008026969,-0.027288865,0.008599314,-0.0255365,-0.0035594942,0.011411579,-0.030581618,0.0023529758,0.0029624177,0.02035006,-0.025296256,0.0056139315,0.013510179,-0.023303645,0.0038862962,0.0012815945,0.027755221,0.03699754,-0.0025437574,-0.013284068,-0.008846624,-0.017608454,-0.013743357,-0.007461689,0.0036213214,0.007518217,0.00842973,0.021282772,-0.009199923,0.0020897675,0.007963374,-0.013792818,-0.00021109654,0.007175516,0.016166992,-0.0068151504,0.0019908436,0.022823159,-0.0033634128,-0.010902828,0.0038015044,-0.034623366,0.016619217,0.03329496,-0.031853497,0.018795542,-0.016265916,-0.0005847109,-0.029677171,0.008945547,-0.020675097,0.018611826,0.00019453562,-0.0053595556,-0.01900752,-0.020887075,-0.0019131176,0.004115941,0.005066317,0.0036389865,0.0010819802,-0.011482239,-0.0046423576,-0.044657078,0.0119909905,-0.017014911,-0.015686505,0.03295579,-0.020378325,0.013976535,0.0045893625,0.01606807,-0.019671725,0.010627254,-0.029790226,-0.028800989,-0.017438872,-0.0014211478,0.054549467,0.01394827,0.04262207,-0.015884353,-0.005394886,0.019685857,-0.002981849,-0.0010369346,-0.014570078,0.0021604274,-0.012577468,0.007391029,0.014033062,-0.03179697,0.020265268,-0.013432453,0.0028687932,0.018230263,-0.007277973,-0.008973812,0.012888372,-0.010627254,-0.024490733,0.0037944384,0.0036319206,0.008231882,-0.022526387,-0.023868926,0.033323225,0.01252094,0.0026709454,-0.01245028,0.017311683,-0.007836186,-0.0030719405,0.0020049755,0.003580692,-0.0064795157,0.013743357,-0.016802931,0.001506823,0.03572566,0.011136006,-0.0042678597,-0.0045610988,-0.0030242451,-0.022243746,-0.0018813207,0.006391191,-0.0034111082,0.0028263973,-0.0054160836,-0.035132118,-0.021565411,-0.009390705,-0.0008567517,0.0059248353,0.005207637,-0.004048814,0.020406589,-0.0020862345,-0.0026073514,0.004320855,-0.008267212,0.028433556,0.014880981,0.021593675,0.023134062,-0.013043824,0.005440815,0.008175354,0.028362896,-0.01871075,0.026426814,0.023600418,-0.0019502142,-0.011284391,0.008012836,0.024589656,-0.013821082,-0.006249871,0.01878141,-0.0010766807,0.026243098,-0.002013808,-0.018936861,-0.04917931,-0.013750423,-0.012181772,-0.0057587842,-0.0016560921,0.008140024,-0.01405426,0.0052465,-0.026342023,0.010450603,-0.0048508043,-0.0035047326,0.004363251,-0.011764878,-0.01077564,-0.007327435,0.012739985,0.018724881,-0.00097422383,-0.00052420836,0.006168612,0.013757489,0.003073707,0.021000132,-0.017919358,0.026016986,0.0096097505,0.000248193,-0.0076524704,0.00024333513,0.020590303,-0.0015183053,0.0013602037,-0.021565411,0.0027681028,-0.0020067422,0.014697266,0.030694675,-0.02504188,-0.012951965,-0.013022626,-0.00437385,-0.02795307,0.00010532746,0.015870221,-0.040502273,-0.034340728,0.032107875,-0.018385714,0.03484948,-0.026497474,0.013022626,0.0075464807,0.038665116,-0.027430186,-0.023444965,0.014782058,0.015290809,-0.037760668,0.03066641,-0.012012188,-0.016449632,0.038241155,-0.0029606512,-0.013008493,-0.028433556,0.011736615,0.0022204884,0.020010892,-0.014697266,0.0006898176,-0.0045434334,0.0034764686,0.002052671,0.0033899103,0.0032185598,0.012478544,0.016958384,0.010966421,-0.035047326,-0.0009848227,0.0005895688,0.007299171,0.017127968,-0.034623366,-0.017749775,0.004433911,-0.0023953717,-0.015615845,-0.0074263588,-0.017820435,-0.008549851,-0.009560289,0.03258836,0.012499742,-0.0121040465,0.0026815445,0.011701285,0.02091534,-0.0038474333,-0.029733699,0.005896571,-0.02056204,-0.0011499904,-0.03377545,0.008062298,-0.016633349,0.01024569,-0.0128247775,-0.047483474,-0.030129395,-0.0055326726,-0.022780761,0.008599314,0.0103163505,0.0033492807,0.0056280633,0.027218206,0.00601316,0.0342842,-0.0035683266,0.0052217687,0.0031108034,0.011453975,-0.027543241,-0.014457022,0.0075111506,-0.0040170173,-0.013467783,0.0036071895,0.01202632,-0.0057375864,0.026073515,0.0022699502,-0.006232206,-0.0033987428,0.00017388968,0.0090020755,0.0061050183,0.023713473,0.019120578,-0.02020874,0.0069918004,0.011814341,-0.014280372,-0.007588877,-0.019360822,0.030977314,-0.00506985,-0.007659537,-0.004352652,-0.0057269875,0.018823806,-0.023572152,0.0134183215,0.008231882,-0.03157086,0.00030979965,0.028687932,-0.023303645,-0.03109037,0.007928045,-0.014153184,-0.010330482,0.0035965906,-0.02817918,-0.008719436,-0.031062106,0.0075111506,-0.022215482,-0.014782058,-0.019403217,0.012231234,-0.008366136,0.008620512,0.0014167316,-0.030101132,-0.005045119,-0.0064300536,0.0029094226,0.004363251,-0.012563336,-0.008945547,-0.02255465,-0.021664334,0.0124714775,-0.013609103,-0.019120578,-0.008267212,0.028080257,-0.0046600224,0.011595295,0.20768367,-0.02461792,0.020293532,0.035838716,-0.012782382,0.011383315,0.015912617,0.014810322,-0.017481267,0.02176326,-0.0038333014,-0.009108066,-0.031825233,0.008450928,0.00814709,-0.009885324,-0.014068392,-0.030892523,-0.020463116,-0.0111501375,0.019869573,0.010627254,-0.0017037876,-0.014782058,0.008118826,-0.010655518,0.0054125506,-0.0025260926,0.008380268,0.009560289,-0.0119909905,0.025875667,-0.0047059515,0.0070659933,-0.0009901223,0.008775963,-0.00899501,0.0023777068,0.0021180315,-0.015559317,0.032475304,-0.027091017,0.005055718,0.00832374,-0.012754118,0.042311165,0.0010846299,-0.0018618891,-0.014061326,0.0033263164,-0.018908598,0.01585609,0.023755869,0.008662907,0.0039640223,0.0013522544,-0.0019908436,-0.010323416,0.017467136,0.00089252327,-0.029422795,-0.004306723,-0.013962403,0.026850775,-0.006952937,-0.007949242,0.011432777,-0.00016549882,-0.005066317,-0.025833271,-0.012648128,0.0029182553,0.0024695646,0.009461365,-0.022427462,-0.0138988085,0.010372878,0.030298978,0.013093285,0.031740442,0.016096333,-0.021212112,0.012443214,0.003126702,-0.00857105,-0.018950993,-0.008267212,-0.012330158,0.0096097505,0.009235253,0.0022558183,0.0025861536,-0.020660965,-0.010641386,-0.001971412,-0.0005255332,0.004942662,0.030609882,0.008797161,0.016138729,-0.04163283,0.044289645,0.0036354535,0.010097304,-0.0046246927,0.019869573,-0.013361793,-0.0032945194,0.0073698307,0.0064300536,-0.00032061944,-0.023798265,0.005193505,-0.00070880743,-0.0066279015,0.015149489,0.010690847,0.00010168406,0.02767043,-0.0123442905,-0.0009291781,-0.024815768,-0.010867498,0.01493751,0.010535396,-0.026243098,-0.02084468,-0.0077019325,-0.01419558,-0.016039804,0.022088295,-0.02148062,0.026864907,-0.003988753,-0.020731624,-0.016958384,-0.009715741,0.002432468,-0.020081552,0.0025826206,0.008330806,-0.017735643,0.0015960312,-0.037110597,0.0076242066,-0.0035895244,0.013672696,0.010160898,-0.0071225213,-0.021904578,-0.0213393,-0.015559317,-0.0096026845,-0.011051213,0.02532452,-0.022455726,-0.034199405,0.0009159294,-0.013736291,-0.014294504,-0.039739147,-0.034199405,0.0047907433,-0.007161384,-0.0032450573,-0.011114807,-0.1850725,-0.0065996377,0.012853041,-0.041293666,0.010570725,0.002158661,0.017961755,0.008719436,-0.0047200834,-0.008457994,0.0096026845,-0.010259822,-0.032107875,-0.0021745595,-0.024660315,0.010613122,-0.016590953,-0.002464265,0.012118178,0.012337224,0.034821216,-0.015036434,-0.006006094,-0.024504865,0.0009795233,-0.007412227,-0.021890447,0.012139376,-0.0014405793,-0.025225597,0.0010033711,0.0148527175,0.0057552513,-0.0028687932,-0.011658888,0.0014299804,0.024646183,-0.0043915147,-0.008613446,-0.000650513,0.015587581,-0.0056775254,-0.0031214026,-0.005896571,-0.024349412,0.036884483,0.031146897,-0.014096657,0.034764685,-0.017255155,0.0037767733,-0.011058279,0.024999484,0.020265268,0.005497343,-0.0043279207,0.03521691,-0.0044833724,-0.009984248,-0.00028286054,-0.01252094,-0.029903283,0.014322768,5.5092667e-05,0.001262163,-0.008274278,-0.009871192,-0.0011897366,-0.011771944,0.01629418,-0.009906522,-0.020519644,-3.2569813e-05,-0.013206341,0.012535072,0.01052833,-0.011439843,0.035753924,0.0067409575,0.027430186,-0.036036566,0.03564087,-0.0011614726,-0.0066031707,0.009171659,-0.01430157,-0.021862183,-0.003974621,-0.007270907,-0.008514522,0.0064830487,-0.009546157,-0.016209388,-0.0013407723,0.018936861,0.01685946,-0.025310388,0.019332558,-0.0016658079,-0.017071439,0.0013884676,-0.009729872,-0.011256127,0.01465487,0.0422829,0.009729872,-0.023812396,0.009284715,0.04646597,-0.0026409149,-0.024193961,0.0112702595,0.012923702,0.0048331395,0.0077513945,0.0248299,0.0069494043,-0.031994816,0.0066137696,-0.013121549,0.05095994,0.0020367724,-0.021409959,-0.0020102751,-0.020025024,-0.0031461334,-0.11181228,-0.0060414243,0.0009892391,0.0038650983,0.003594824,0.04168936,0.008012836,0.026370287,-0.034792952,0.010747376,-0.00014982113,-0.005878906,0.0009706908,-0.012393752,-0.0057093226,0.01021036,0.01700078,-0.0010219193,-0.0038121033,0.011998056,0.02461792,-0.002914722,0.0040134843,-0.026243098,0.024603788,-0.0062957997,-0.038439002,0.015417997,0.024702713,0.004281992,-0.0016878891,-0.026285494,0.009616816,-0.0023335442,-0.009397771,0.00043985806,-0.010189163,-0.011623559,0.02056204,-0.023261249,0.004320855,0.006899942,0.0021939909,-0.020533776,0.008613446,0.0008064065,-0.019742385,0.0047165505,0.018767279,-0.0094896285,-0.03222093,-0.00055600534,-0.006228673,0.006196876,-0.0011685386,0.023360174,0.02696383,0.005200571,0.0015262546,-0.006073221,-0.009277649,0.009051537,-0.0143369,0.010302218,0.033492807,-0.016633349,-0.030835994,-0.044996243,-0.014110789,-0.029677171,-0.0016693409,0.014753793,-0.010401142,-0.0023741736,-0.0046034944,-0.009913589,-0.025663687,0.0013178077,0.015347337,-0.012542138,0.0007039496,-0.0068186834,0.0075040846,-0.015644109,-0.01066965,0.018371582,0.0013637367,-0.016195256,-0.0043985806,-0.00818242,-0.009206989,0.011962727,0.0058011804,-0.033888504,-0.018315054,0.01472553,0.002109199,-0.0071861153,-0.006090886,0.018103074,-0.00029964227,0.026780114,-0.067211725,0.010697913,0.00060635054,-0.019954365,-0.0027787017,0.009729872,0.0076171407,0.0058612414,-0.001308092,0.008450928,-0.03694101,0.0049179313,-0.013891743,-0.003510032,-0.028362896,-0.0076736687,0.025932195,-0.01458421,0.027557373,0.021155583,-0.0015739499,-0.003510032,0.01035168,0.010507132,-0.011934463,0.017735643,-0.0045858296,0.024730977,-0.008118826,-0.0117507465,-0.013976535,-0.018272659,-0.019756516,0.011355051,-0.0021639604,0.0044657076,0.008034035,0.011764878,0.028348764,0.028278105,-0.021226244,-0.01629418,-0.007539415,-0.011333853,-0.0064936476,-0.0049179313,0.006006094,0.007843252,0.019049918,0.0037237783,0.002109199,0.016520292,-0.011510503,-0.0068681454,-0.0007154318,0.018385714,0.008373202,-0.002718641,-0.0015536352,-0.012775316,0.037449762,0.006168612,-0.0034959002,0.007157851,0.010648452,-0.0068504806,-0.01906405,-0.017226892,0.0048543373,-0.0132981995,-0.023699341,0.009093933,0.008973812,-8.032047e-05,-0.0056810584,-0.003485301,0.01578543,0.04163283,0.0023282447,0.029111892,-0.011376249,-0.011008818,-0.021155583,0.0093695065,0.035273436,0.004066479,0.0057552513,0.018965125,-0.021508884,0.008549851,-0.00012376529,-0.0015006403,0.0040134843,-0.008811293,-0.013086219,0.010620188,0.018456373,0.011800209,0.008903151,0.0081682885,0.028249841,-0.0028104987,-0.005822378,-0.016831195,-0.041350193,0.010493,-0.018046547,-0.036686637,-0.028292237,0.018583562,0.023699341,-0.0112702595,-0.0001439696,0.0038262352,-0.018625958,0.010712045,0.004486906,-0.0050415862,-0.02184805,0.013552575,0.00860638,0.024603788,0.030214187,-0.0075676786,0.020731624,0.00860638,0.026667058,-0.021409959,0.015884353,0.019120578,-0.010768574,0.021296903,-0.012068716,-0.02035006,0.0067586224,-0.010909894,0.0030207122,0.0015880819,-0.01216764,0.06545936,0.017382342,-0.012506808,0.0057905815,-0.009051537,0.016053937,-0.0023388437,0.0048932005,-0.018654222,-0.016082201,0.037647612,-0.009418969,0.024377676,-0.033068847,-0.02945106,-0.008874888,0.013163946,0.019233633,-0.0047801444,-0.024095036,0.011213731,-0.01059899,-0.002186925,0.01112894,-0.005762317,0.0107968375,0.01479619,-0.01856943,-0.0021038994,-0.02056204,0.022738365,-0.011998056,-0.04688993,-0.02396785,-0.0030154125,0.0068116174,-0.008302542,-0.016407236,0.011418645,-0.0028935242,-0.0025472906,0.032051343,-0.027599769,-0.026808377,-0.01934669,0.00057190383,-0.01708557,-0.027274733,-0.017453004]}],"model":"text-embedding-ada-002-v2","usage":{"prompt_tokens":7,"total_tokens":7}}
{"doc":"Send a completion request to the OpenAI API and return the JSON response.","source":"@trace\nasync def openai_complete(\n    prompt: str,\n    stop: Optional[str] = \"\\n\",\n    top_p: float = 1,\n    temperature: float = 0,\n    model: str = \"text-davinci-002\",\n    max_tokens: int = 256,\n    logprobs: Optional[int] = None,\n    logit_bias: Optional[Mapping[str, Union[int, float]]] = None,\n    n: int = 1,\n    echo: bool = False,\n    cache_id: int = 0,  # for repeated non-deterministic sampling using caching\n) -> dict:\n    \"\"\"Send a completion request to the OpenAI API and return the JSON response.\"\"\"\n    params = {\n        \"prompt\": prompt,\n        \"stop\": stop,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"model\": model,\n        \"echo\": echo,\n        \"max_tokens\": max_tokens,\n        \"logprobs\": logprobs,\n        \"n\": n,\n    }\n    if logit_bias:\n        params[\"logit_bias\"] = logit_bias  # type: ignore[assignment]\n    response = await _post(\"completions\", json=params, cache_id=cache_id)\n    if isinstance(response, TooLongRequestError):\n        raise response\n    add_fields(davinci_equivalent_tokens=extract_total_tokens(response))\n    return response\n"}
{"prompt":"Here is a paragraph from a research paper: \"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Pre-trained language representation models, including feature-based (Mikolov et al., 2013;Pennington et al., 2014;Peters et al., 2017Peters et al., , 2018 and fine-tuning (Dai and Le, 2015;Howard and Ruder, 2018;Radford et al., 2018;Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question Figure 1 : An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"answering (Rajpurkar et al., 2016;Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification .\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1 , without knowing Blowin' in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For the existing pre-trained language representation models, these two sentences are syntactically ambiguous, like \"UNK wrote UNK in UNK\". Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity typing and relation classification.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"For incorporating external knowledge into language representation models, there are two main challenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs. Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives. Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments. Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation classification. The experimental results show that ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks, by taking full advantage of lexical, syntactic, and knowledge information. We also evaluate ERNIE on other common NLP tasks, and ERNIE still achieves comparable results.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Many efforts are devoted to pre-training language representation models for capturing language information from text and then utilizing the information for specific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. Howard and Ruder (2018) present AWD-LSTM (Merity et al., 2018) to build a universal language model (ULMFiT). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018;Zhong et al., 2018), machine translation (Zaremoodi et al., 2018) The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"After information fusion, it outputs new token embeddings and entity embeddings for the next layer.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"inference , knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014;Toutanova et al., 2015;Han et al., 2016;Yamada et al., 2016;Cao et al., 2017Cao et al., , 2018. Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"1 It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"We denote a token sequence as {w 1 , . . . , w n } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e 1 , . . . , e m }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V, and the entity list containing all entities in KGs as E. If a token w \u2208 V has a corresponding entity e \u2208 E, their alignment is defined as f (w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2 .\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"As shown in Figure 2 , the whole model architecture of ERNIE consists of two stacked modules:\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(1) the underlying textual encoder (T-Encoder) responsible to capture basic lexical and syntactic information from the input tokens, and (2) the upper knowledgeable encoder (K-Encoder) responsible to integrate extra token-oriented knowledge information into textual information from the underlying layer, so that we can represent heterogeneous information of tokens and entities into a united feature space. Besides, we denote the number of T-Encoder layers as N , and the number of K-Encoder layers as M .\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"To be specific, given a token sequence {w 1 , . . . , w n } and its corresponding entity sequence {e 1 , . . . , e m }, the textual encoder firstly sums the token embedding, segment embedding, positional embedding for each token to compute its input embedding, and then computes lexical and syntactic features {w 1 , . . . , w n } as follows, {w1, . . . , wn} = T-Encoder({w1, . . . , wn}),\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(1)\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"where T-Encoder( To be specific, we represent {e 1 , . . . , e m } with their entity embeddings {e 1 , . . . , e m }, which are pre-trained by the effective knowledge embedding model TransE (Bordes et al., 2013). Then, both {w 1 , . . . , w n } and {e 1 , . . . , e m } are fed into K-Encoder for fusing heterogeneous information and computing final output embeddings, (2)\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"{w o 1 , . . . , w o n } and {e o 1 , . . . , e o n } will be used as features for specific tasks. More details of the knowledgeable encoder K-Encoder will be introduced in Section 3.3.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"As shown in Figure 2 , the knowledgeable encoder K-Encoder consists of stacked aggregators, which are designed for encoding both tokens and entities as well as fusing their heterogeneous features. In the i-th aggregator, the input token embeddings {w } from the preceding aggregator are fed into two multi-head self-attentions (MH-ATTs) (Vaswani et al., 2017) respectively,\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Then, the i-th aggregator adopts an information fusion layer for the mutual integration of the token and entity sequence, and computes the output embedding for each token and entity. For a token w j and its aligned entity e k = f (w j ), the information fusion process is as follows,\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"where h j is the inner hidden state integrating the information of both the token and the entity. \u03c3(\u2022) is the non-linear activation function, which usually is the GELU function (Hendrycks and Gimpel, 2016). For the tokens without corresponding entities, the information fusion layer computes the output embeddings without integration as follows,\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(5)\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"For simplicity, the i-th aggregator operation is denoted as follows,\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"1 , . . . , e (i) m } = Aggregator( {w\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The output embeddings of both tokens and entities computed by the top aggregator will be used as the final output embeddings of the knowledgeable encoder K-Encoder.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA). Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs. Given the token sequence {w 1 , . . . , w n } and its corresponding entity sequence {e 1 , . . . , e m }, we define the aligned entity distribution for the token w i as follows, .\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"[SEP]\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \".\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Figure 3 : Modifying the input sequence for the specific tasks. To align tokens among different types of input, we use dotted rectangles as placeholder. The colorful rectangles present the specific mark tokens.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"where linear(\u2022) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"As shown in Figure 3 , for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"For relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016;Yaghoobzadeh and Sch\u00fctze, 2017;Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train- ing epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity . The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1 . We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"UFET. For Open Entity, we add a new hybrid model UFET for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Besides NFGEC and UFET, we also report the result of fine-tuning BERT with the same input format introduced in Section 3.5 for fair com- parison. Following the same evaluation criteria used in the previous work, we compare NFGEC, BERT, ERNIE on FIGER, and adopt strict accuracy, loose macro, loose micro scores for evaluation. We compare NFGEC, BERT, UFET, ERNIE on Open Entity, and adopt precision, recall, micro-F1 scores for evaluation.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The results on FIGER are shown in Table 2 . From the results, we observe that: (1) BERT achieves comparable results with NFGEC on the macro and micro metrics. However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. (2) Compared with BERT, ERNIE significantly improves the strict accuracy, indicating the external knowledge regularizes ERNIE to avoid fitting the noisy labels and accordingly benefits entity typing.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The results on Open Entity are shown in Table 3. From the table, we observe that: (1) BERT and ERNIE achieve much higher recall scores than the previous entity typing models, which means pre-training language models make full use of both the unsupervised pre-training and manuallyannotated training data for better entity typing. (2) Compared to BERT, ERNIE improves the precision by 2% and the recall by 2%, which means the informative entities help ERNIE predict the labels more precisely.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TA-CRED (Zhang et al., 2017). The statistics of these two datasets are shown in Lin et al., 2016;Wu et al., 2017;Han et al., 2018b).\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"PA-LSTM. Zhang et al. (2017) propose PA-LSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5 , we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In conclusion, we find that the pre-trained language models can provide more information for training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"The General Language Understanding Evaluation (GLUE) benchmark ) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018;Socher et al., 2013;Dolan and Brockett, 2005;Agirre et al., 2007;Williams et al., 2018;Rajpurkar et al., 2016;Dagan et al., 2006;Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019).\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In Table 6 , we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERT BASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In short, ERNIE achieves comparable results with BERT BASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively. As shown in Table 7 , we have the following observations:\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%. (2) Although the informative entities bring much knowledge informa-tion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"In this paper, we propose ERNIE to incorporate knowledge information into language representation models. Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs. The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT. There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as Con-ceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata;\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"prompt":"Here is a paragraph from a research paper: \"This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001).\"\n\nQuestion: Does this paragraph answer the question 'What was the training corpus used?'? Say Yes or No.\nAnswer:","stop":"\n","top_p":1.0,"temperature":0.0,"model":"text-davinci-002","max_tokens":1,"logprobs":5,"logit_bias":null,"n":1,"echo":false,"cache_id":0}
{"doc":"Send a completion request to the OpenAI API and return the JSON response.","source":"@trace\nasync def openai_chatcomplete(\n    messages: list[dict[str, str]],\n    stop: Optional[str] = \"\\n\",\n    top_p: float = 1,\n    temperature: float = 0,\n    model: str = \"gpt-3.5-turbo\",\n    max_tokens: int = 256,\n    logit_bias: Optional[Mapping[str, Union[int, float]]] = None,\n    n: int = 1,\n    cache_id: int = 0,  # for repeated non-deterministic sampling using caching\n) -> dict:\n    \"\"\"Send a completion request to the OpenAI API and return the JSON response.\"\"\"\n    params = {\n        \"messages\": messages,\n        \"stop\": stop,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"n\": n,\n    }\n    if logit_bias:\n        params[\"logit_bias\"] = logit_bias  # type: ignore[assignment]\n    response = await _post(\"chat/completions\", json=params, cache_id=cache_id)\n    if isinstance(response, TooLongRequestError):\n        raise response\n    add_fields(total_tokens=extract_total_tokens(response))\n    return response\n"}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks.","However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.","We argue that informative entities in KGs can enhance language representation with external knowledge.","In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously.","The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks.","The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},"formatted":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Pre-trained language representation models, including feature-based (Mikolov et al., 2013;Pennington et al., 2014;Peters et al., 2017Peters et al., , 2018 and fine-tuning (Dai and Le, 2015;Howard and Ruder, 2018;Radford et al., 2018;Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications.","BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question Figure 1 : An example of incorporating extra knowledge information for language understanding.","The solid lines present the existing knowledge facts.","The red dotted lines present the facts extracted from the sentence in red.","The green dotdash lines present the facts extracted from the sentence in green."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"Pre-trained language representation models, including feature-based (Mikolov et al., 2013;Pennington et al., 2014;Peters et al., 2017Peters et al., , 2018 and fine-tuning (Dai and Le, 2015;Howard and Ruder, 2018;Radford et al., 2018;Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question Figure 1 : An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["answering (Rajpurkar et al., 2016;Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification ."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"answering (Rajpurkar et al., 2016;Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification ."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding.","As shown in Figure 1 , without knowing Blowin' in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task.","Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task.","For the existing pre-trained language representation models, these two sentences are syntactically ambiguous, like \"UNK wrote UNK in UNK\".","Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g.","entity typing and relation classification."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1 , without knowing Blowin' in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For the existing pre-trained language representation models, these two sentences are syntactically ambiguous, like \"UNK wrote UNK in UNK\". Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity typing and relation classification."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["For incorporating external knowledge into language representation models, there are two main challenges.","(1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces.","How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"For incorporating external knowledge into language representation models, there are two main challenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs.","Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE.","Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs. Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives.","Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments.","Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"(2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives. Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments. Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation classification.","The experimental results show that ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks, by taking full advantage of lexical, syntactic, and knowledge information.","We also evaluate ERNIE on other common NLP tasks, and ERNIE still achieves comparable results."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"formatted":"We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation classification. The experimental results show that ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks, by taking full advantage of lexical, syntactic, and knowledge information. We also evaluate ERNIE on other common NLP tasks, and ERNIE still achieves comparable results."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Many efforts are devoted to pre-training language representation models for capturing language information from text and then utilizing the information for specific NLP tasks.","These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},"formatted":"Many efforts are devoted to pre-training language representation models for capturing language information from text and then utilizing the information for specific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations.","As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010).","Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},"formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models.","Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed.","Howard and Ruder (2018) present AWD-LSTM (Merity et al., 2018) to build a universal language model (ULMFiT).","Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations.","Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},"formatted":"Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. Howard and Ruder (2018) present AWD-LSTM (Merity et al., 2018) to build a universal language model (ULMFiT). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information.","As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018;Zhong et al., 2018), machine translation (Zaremoodi et al., 2018) The left part is the architecture of ERNIE.","The right part is the aggregator for the mutual integration of the input of tokens and entities.","Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},"formatted":"Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018;Zhong et al., 2018), machine translation (Zaremoodi et al., 2018) The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["After information fusion, it outputs new token embeddings and entity embeddings for the next layer."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},"formatted":"After information fusion, it outputs new token embeddings and entity embeddings for the next layer."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["inference , knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018).","Hence, we argue that extra knowledge information can effectively benefit existing pre-training models.","In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014;Toutanova et al., 2015;Han et al., 2016;Yamada et al., 2016;Cao et al., 2017Cao et al., , 2018.","Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 .","In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},"formatted":"inference , knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014;Toutanova et al., 2015;Han et al., 2016;Yamada et al., 2016;Cao et al., 2017Cao et al., , 2018. Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5."],"sections":[{"title":"Methodology","number":"3"}],"section_type":"main"},"formatted":"In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["1 It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT.","Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th."],"sections":[{"title":"Methodology","number":"3"}],"section_type":"main"},"formatted":"1 It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["We denote a token sequence as {w 1 , . . .",", w n } 2 , where n is the length of the token sequence.","Meanwhile, we denote the entity sequence aligning to the given tokens as {e 1 , . . .",", e m }, where m is the length of the entity sequence.","Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs.","Furthermore, we denote the whole vocabulary containing all tokens as V, and the entity list containing all entities in KGs as E. If a token w \u2208 V has a corresponding entity e \u2208 E, their alignment is defined as f (w) = e.","In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2 ."],"sections":[{"title":"Notations","number":"3.1"}],"section_type":"main"},"formatted":"We denote a token sequence as {w 1 , . . . , w n } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e 1 , . . . , e m }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V, and the entity list containing all entities in KGs as E. If a token w \u2208 V has a corresponding entity e \u2208 E, their alignment is defined as f (w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2 ."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["As shown in Figure 2 , the whole model architecture of ERNIE consists of two stacked modules:"],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},"formatted":"As shown in Figure 2 , the whole model architecture of ERNIE consists of two stacked modules:"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(1) the underlying textual encoder (T-Encoder) responsible to capture basic lexical and syntactic information from the input tokens, and (2) the upper knowledgeable encoder (K-Encoder) responsible to integrate extra token-oriented knowledge information into textual information from the underlying layer, so that we can represent heterogeneous information of tokens and entities into a united feature space.","Besides, we denote the number of T-Encoder layers as N , and the number of K-Encoder layers as M ."],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},"formatted":"(1) the underlying textual encoder (T-Encoder) responsible to capture basic lexical and syntactic information from the input tokens, and (2) the upper knowledgeable encoder (K-Encoder) responsible to integrate extra token-oriented knowledge information into textual information from the underlying layer, so that we can represent heterogeneous information of tokens and entities into a united feature space. Besides, we denote the number of T-Encoder layers as N , and the number of K-Encoder layers as M ."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["To be specific, given a token sequence {w 1 , . . .",", w n } and its corresponding entity sequence {e 1 , . . .",", e m }, the textual encoder firstly sums the token embedding, segment embedding, positional embedding for each token to compute its input embedding, and then computes lexical and syntactic features {w 1 , . . .",", w n } as follows, {w1, . . .",", wn} = T-Encoder({w1, . . .",", wn}),"],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},"formatted":"To be specific, given a token sequence {w 1 , . . . , w n } and its corresponding entity sequence {e 1 , . . . , e m }, the textual encoder firstly sums the token embedding, segment embedding, positional embedding for each token to compute its input embedding, and then computes lexical and syntactic features {w 1 , . . . , w n } as follows, {w1, . . . , wn} = T-Encoder({w1, . . . , wn}),"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(1)"],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},"formatted":"(1)"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["where T-Encoder( To be specific, we represent {e 1 , . . .",", e m } with their entity embeddings {e 1 , . . .",", e m }, which are pre-trained by the effective knowledge embedding model TransE (Bordes et al., 2013).","Then, both {w 1 , . . .",", w n } and {e 1 , . . .",", e m } are fed into K-Encoder for fusing heterogeneous information and computing final output embeddings, (2)"],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},"formatted":"where T-Encoder( To be specific, we represent {e 1 , . . . , e m } with their entity embeddings {e 1 , . . . , e m }, which are pre-trained by the effective knowledge embedding model TransE (Bordes et al., 2013). Then, both {w 1 , . . . , w n } and {e 1 , . . . , e m } are fed into K-Encoder for fusing heterogeneous information and computing final output embeddings, (2)"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["{w o 1 , . . .",", w o n } and {e o 1 , . . .",", e o n } will be used as features for specific tasks.","More details of the knowledgeable encoder K-Encoder will be introduced in Section 3.3."],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},"formatted":"{w o 1 , . . . , w o n } and {e o 1 , . . . , e o n } will be used as features for specific tasks. More details of the knowledgeable encoder K-Encoder will be introduced in Section 3.3."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["As shown in Figure 2 , the knowledgeable encoder K-Encoder consists of stacked aggregators, which are designed for encoding both tokens and entities as well as fusing their heterogeneous features.","In the i-th aggregator, the input token embeddings {w } from the preceding aggregator are fed into two multi-head self-attentions (MH-ATTs) (Vaswani et al., 2017) respectively,"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"As shown in Figure 2 , the knowledgeable encoder K-Encoder consists of stacked aggregators, which are designed for encoding both tokens and entities as well as fusing their heterogeneous features. In the i-th aggregator, the input token embeddings {w } from the preceding aggregator are fed into two multi-head self-attentions (MH-ATTs) (Vaswani et al., 2017) respectively,"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Then, the i-th aggregator adopts an information fusion layer for the mutual integration of the token and entity sequence, and computes the output embedding for each token and entity.","For a token w j and its aligned entity e k = f (w j ), the information fusion process is as follows,"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"Then, the i-th aggregator adopts an information fusion layer for the mutual integration of the token and entity sequence, and computes the output embedding for each token and entity. For a token w j and its aligned entity e k = f (w j ), the information fusion process is as follows,"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["where h j is the inner hidden state integrating the information of both the token and the entity.","\u03c3(\u2022) is the non-linear activation function, which usually is the GELU function (Hendrycks and Gimpel, 2016).","For the tokens without corresponding entities, the information fusion layer computes the output embeddings without integration as follows,"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"where h j is the inner hidden state integrating the information of both the token and the entity. \u03c3(\u2022) is the non-linear activation function, which usually is the GELU function (Hendrycks and Gimpel, 2016). For the tokens without corresponding entities, the information fusion layer computes the output embeddings without integration as follows,"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(5)"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"(5)"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["For simplicity, the i-th aggregator operation is denoted as follows,"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"For simplicity, the i-th aggregator operation is denoted as follows,"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["1 , . . .",", e (i) m } = Aggregator( {w"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"1 , . . . , e (i) m } = Aggregator( {w"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The output embeddings of both tokens and entities computed by the top aggregator will be used as the final output embeddings of the knowledgeable encoder K-Encoder."],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},"formatted":"The output embeddings of both tokens and entities computed by the top aggregator will be used as the final output embeddings of the knowledgeable encoder K-Encoder."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens.","As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA).","Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs.","Given the token sequence {w 1 , . . .",", w n } and its corresponding entity sequence {e 1 , . . .",", e m }, we define the aligned entity distribution for the token w i as follows, ."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA). Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs. Given the token sequence {w 1 , . . . , w n } and its corresponding entity sequence {e 1 , . . . , e m }, we define the aligned entity distribution for the token w i as follows, ."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["[SEP]"],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"[SEP]"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Figure 3 : Modifying the input sequence for the specific tasks.","To align tokens among different types of input, we use dotted rectangles as placeholder.","The colorful rectangles present the specific mark tokens."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"Figure 3 : Modifying the input sequence for the specific tasks. To align tokens among different types of input, we use dotted rectangles as placeholder. The colorful rectangles present the specific mark tokens."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["where linear(\u2022) is a linear layer.","Eq. 7 will be used to compute the cross-entropy loss function for dEA."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"where linear(\u2022) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text.","More details of these pre-training tasks can be found from Devlin et al. (2019).","The overall pre-training loss is the sum of the dEA, MLM and NSP loss."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},"formatted":"Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["As shown in Figure 3 , for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT.","We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks.","For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:"],"sections":[{"title":"Fine-tuning for Specific Tasks","number":"3.5"}],"section_type":"main"},"formatted":"As shown in Figure 3 , for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["For relation classification, the task requires systems to classify relation labels of given entity pairs based on context.","The most straightforward way to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification.","In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions.","These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015).","Then, we also take the [CLS] token embedding for classification.","Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively."],"sections":[{"title":"Fine-tuning for Specific Tasks","number":"3.5"}],"section_type":"main"},"formatted":"For relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The specific fine-tuning procedure for entity typing is a simplified version of relation classification.","As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016;Yaghoobzadeh and Sch\u00fctze, 2017;Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively."],"sections":[{"title":"Fine-tuning for Specific Tasks","number":"3.5"}],"section_type":"main"},"formatted":"The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016;Yaghoobzadeh and Sch\u00fctze, 2017;Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."],"sections":[{"title":"Experiments","number":"4"}],"section_type":"main"},"formatted":"In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities.","To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.","The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"formatted":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively.","In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4.","The total parameters are about 114M."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},"formatted":"In this work, we denote the hidden dimension of token embeddings and entity embeddings as H w , H e respectively, and the number of self-attention heads as A w , A e respectively. In detail, we have the following model size: N = 6, M = 6, H w = 768, H e = 100, A w = 12, A e = 4. The total parameters are about 114M."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance.","And, we only pre-train ERNIE on the annotated corpus for one epoch.","To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length.","To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.","Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT.","For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs.","We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},"formatted":"The total amount of parameters of BERT BASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512. Except for setting the learning rate as 5e \u22125 , we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e \u22125 , 3e \u22125 , 2e \u22125 , number of epochs ranging from 3 to 10."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015).","As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data.","Hence, we use a larger batch size and less train- ing epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3.","As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs."],"sections":[{"title":"Parameter Settings and Training Details","number":"4.2"}],"section_type":"main"},"formatted":"We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train- ing epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity .","The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.","Open Entity is a completely manually-annotated dataset.","The statistics of these two datasets are shown in Table 1 .","We compare our model with the following baseline models for entity typing: NFGEC.","NFGEC is a hybrid model proposed by Shimaoka et al. (2016).","NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.","As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity . The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1 . We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["UFET.","For Open Entity, we add a new hybrid model UFET for comparison.","UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"UFET. For Open Entity, we add a new hybrid model UFET for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Besides NFGEC and UFET, we also report the result of fine-tuning BERT with the same input format introduced in Section 3.5 for fair com- parison.","Following the same evaluation criteria used in the previous work, we compare NFGEC, BERT, ERNIE on FIGER, and adopt strict accuracy, loose macro, loose micro scores for evaluation.","We compare NFGEC, BERT, UFET, ERNIE on Open Entity, and adopt precision, recall, micro-F1 scores for evaluation."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"Besides NFGEC and UFET, we also report the result of fine-tuning BERT with the same input format introduced in Section 3.5 for fair com- parison. Following the same evaluation criteria used in the previous work, we compare NFGEC, BERT, ERNIE on FIGER, and adopt strict accuracy, loose macro, loose micro scores for evaluation. We compare NFGEC, BERT, UFET, ERNIE on Open Entity, and adopt precision, recall, micro-F1 scores for evaluation."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The results on FIGER are shown in Table 2 . From the results, we observe that: (1) BERT achieves comparable results with NFGEC on the macro and micro metrics.","However, BERT has lower accuracy than the best NFGEC model.","As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability.","(2) Compared with BERT, ERNIE significantly improves the strict accuracy, indicating the external knowledge regularizes ERNIE to avoid fitting the noisy labels and accordingly benefits entity typing."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"The results on FIGER are shown in Table 2 . From the results, we observe that: (1) BERT achieves comparable results with NFGEC on the macro and micro metrics. However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. (2) Compared with BERT, ERNIE significantly improves the strict accuracy, indicating the external knowledge regularizes ERNIE to avoid fitting the noisy labels and accordingly benefits entity typing."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The results on Open Entity are shown in Table 3. From the table, we observe that: (1) BERT and ERNIE achieve much higher recall scores than the previous entity typing models, which means pre-training language models make full use of both the unsupervised pre-training and manuallyannotated training data for better entity typing.","(2) Compared to BERT, ERNIE improves the precision by 2% and the recall by 2%, which means the informative entities help ERNIE predict the labels more precisely."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"The results on Open Entity are shown in Table 3. From the table, we observe that: (1) BERT and ERNIE achieve much higher recall scores than the previous entity typing models, which means pre-training language models make full use of both the unsupervised pre-training and manuallyannotated training data for better entity typing. (2) Compared to BERT, ERNIE improves the precision by 2% and the recall by 2%, which means the informative entities help ERNIE predict the labels more precisely."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"formatted":"In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TA-CRED (Zhang et al., 2017).","The statistics of these two datasets are shown in Lin et al., 2016;Wu et al., 2017;Han et al., 2018b)."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TA-CRED (Zhang et al., 2017). The statistics of these two datasets are shown in Lin et al., 2016;Wu et al., 2017;Han et al., 2018b)."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["PA-LSTM.","Zhang et al. (2017) propose PA-LSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"PA-LSTM. Zhang et al. (2017) propose PA-LSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["C-GCN.","Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.","To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models.","In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison.","As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances.","Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison.","From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%.","However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%.","(2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro.","The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively.","From Table 5 , we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy.","The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA.","(2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5 , we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In conclusion, we find that the pre-trained language models can provide more information for training set.","It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},"formatted":"In conclusion, we find that the pre-trained language models can provide more information for training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["The General Language Understanding Evaluation (GLUE) benchmark ) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018;Socher et al., 2013;Dolan and Brockett, 2005;Agirre et al., 2007;Williams et al., 2018;Rajpurkar et al., 2016;Dagan et al., 2006;Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019)."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"The General Language Understanding Evaluation (GLUE) benchmark ) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018;Socher et al., 2013;Dolan and Brockett, 2005;Agirre et al., 2007;Williams et al., 2018;Rajpurkar et al., 2016;Dagan et al., 2006;Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019)."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In Table 6 , we report the results of our evaluation submissions and those of BERT from the leaderboard.","We notice that ERNIE is consistent with BERT BASE on big datasets like MNLI, QQP, QNLI, and SST-2.","The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"In Table 6 , we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERT BASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In short, ERNIE achieves comparable results with BERT BASE on GLUE.","On the one hand, it means GLUE does not require external knowledge for language representation.","On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion."],"sections":[{"title":"GLUE","number":"4.5"}],"section_type":"main"},"formatted":"In short, ERNIE achieves comparable results with BERT BASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset.","w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively.","As shown in Table 7 , we have the following observations:"],"sections":[{"title":"Ablation Study","number":"4.6"}],"section_type":"main"},"formatted":"In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively. As shown in Table 7 , we have the following observations:"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%.","(2) Although the informative entities bring much knowledge informa-tion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%."],"sections":[{"title":"Ablation Study","number":"4.6"}],"section_type":"main"},"formatted":"(1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%. (2) Although the informative entities bring much knowledge informa-tion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["In this paper, we propose ERNIE to incorporate knowledge information into language representation models.","Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs.","The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT.","There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as Con-ceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata;"],"sections":[{"title":"Conclusion","number":"5"}],"section_type":"main"},"formatted":"In this paper, we propose ERNIE to incorporate knowledge information into language representation models. Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs. The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT. There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as Con-ceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata;"},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["(3) annotate more real-world corpora heuristically for building larger pre-training data.","These directions may lead to more general and effective language understanding."],"sections":[{"title":"Conclusion","number":"5"}],"section_type":"main"},"formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":[],"sections":[],"section_type":"main"},"formatted":""},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Here is a paragraph from a research paper: \"",{"source":"paragraph","value":{"sentences":["This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001)."],"sections":[{"title":"Acknowledgement","number":null}],"section_type":"back"},"formatted":"This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001)."},"\"\n\nQuestion: Does this paragraph answer the question '",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"'? Say Yes or No.\nAnswer:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
0.7903062313478622
0.7933457258134865
0.7842064141991764
0.7715400829200912
0.7954357289934834
0.7950293130886347
0.7687823629544498
0.7992601278753597
0.7665961426972413
0.7824535343100101
0.8017496909567801
0.7709524289632266
0.7791092268314219
0.7478027040262549
0.7920656050832577
0.76490201697404
0.7528459802178409
0.7522023342381544
0.7368027964147998
0.7578434808070286
0.7616710420560483
0.7524964075613049
0.7563676248395584
0.7584066835609555
0.752584456833388
0.753175526182583
0.7429953929050187
0.7436333281684899
0.7177034640865214
0.722739904884336
0.7657549524026327
0.7671569291432268
0.7456900711144206
0.7647184144230901
0.7719992039295612
0.7236611548893204
0.7769892203068677
0.7828407166884741
0.7690314309307429
0.7605830056542991
0.7592982440386424
0.7907971222322991
0.828203473804982
0.7887016331687458
0.7555498827796242
0.7880749417699714
0.7765729545564081
0.7676428162442399
0.7697344053459455
0.7625080059028724
0.7628776072677006
0.7758062185143338
0.7520722995641611
0.7590590082568989
0.7573258169432487
0.7700933865897904
0.7758367271237837
0.7959153138551651
0.7808684586722175
0.7872542111897515
0.7746791649812417
0.7629502336584775
0.7716184038718527
0.7592855193152802
0.7792418094429122
0.8281187742391836
0.6861716759405848
0.7822856974641264
[{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},{"sentences":["(3) annotate more real-world corpora heuristically for building larger pre-training data.","These directions may lead to more general and effective language understanding."],"sections":[{"title":"Conclusion","number":"5"}],"section_type":"main"},{"sentences":["The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations.","As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010).","Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},{"sentences":["(2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives.","Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments.","Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["In conclusion, we find that the pre-trained language models can provide more information for training set.","It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable."],"sections":[{"title":"Relation Classification","number":"4.4"}],"section_type":"main"},{"sentences":["For incorporating external knowledge into language representation models, there are two main challenges.","(1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces.","How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Pre-trained language representation models, including feature-based (Mikolov et al., 2013;Pennington et al., 2014;Peters et al., 2017Peters et al., , 2018 and fine-tuning (Dai and Le, 2015;Howard and Ruder, 2018;Radford et al., 2018;Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications.","BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question Figure 1 : An example of incorporating extra knowledge information for language understanding.","The solid lines present the existing knowledge facts.","The red dotted lines present the facts extracted from the sentence in red.","The green dotdash lines present the facts extracted from the sentence in green."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["inference , knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018).","Hence, we argue that extra knowledge information can effectively benefit existing pre-training models.","In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014;Toutanova et al., 2015;Han et al., 2016;Yamada et al., 2016;Cao et al., 2017Cao et al., , 2018.","Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 .","In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},{"sentences":["In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."],"sections":[{"title":"Experiments","number":"4"}],"section_type":"main"}]
{"id":"cmpl-7lxmUPVaLk5RB5BUkrOupxLHzfHLQ","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.23811315],"top_logprobs":[{" No":-0.23811315,"\n":-2.1018903," Yes":-2.5830877," This":-5.7729077,"No":-5.9653826}],"text_offset":[372]},"finish_reason":"length"}],"usage":{"prompt_tokens":80,"completion_tokens":1,"total_tokens":81}}
{" No":0.7881135110047318,"\n":0.1222251675116074," Yes":0.07554039746969947," This":0.0031106993708047838,"No":0.002566062643980439}
[{" Yes":0.08746605176966656," No":0.9125339482303335},null]
0.08746605176966656
{"id":"cmpl-7lxmUER32eZkSL4W2eaGd7PlhuyF5","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.15282361],"top_logprobs":[{" No":-0.15282361,"\n":-2.053362," Yes":-5.0444345,"No":-5.877564," no":-6.8058434}],"text_offset":[289]},"finish_reason":"length"}],"usage":{"prompt_tokens":73,"completion_tokens":1,"total_tokens":74}}
{" No":0.8582811006634895,"\n":0.12830282357542772," Yes":0.006445104039838639,"No":0.0028016016709663145," no":0.0011072858922687042}
[{" Yes":0.007453346509893078," No":0.9925466534901068},null]
0.007453346509893078
{"id":"cmpl-7lxmUXTrt73xJqHrnHyUx07ouj7cv","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1888038],"top_logprobs":[{" No":-0.1888038,"\n":-1.953337," Yes":-4.759405,"No":-5.396989," This":-5.4744635}],"text_offset":[887]},"finish_reason":"length"}],"usage":{"prompt_tokens":197,"completion_tokens":1,"total_tokens":198}}
{" No":0.8279489343409782,"\n":0.14180009428059812," Yes":0.008570707451610263,"No":0.004530200862330026," This":0.00419247721075096}
[{" Yes":0.010245673889072092," No":0.989754326110928},null]
0.010245673889072092
{"id":"cmpl-7lxmUVctKgKKzJDuV1y0jKUBkbnVr","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.15832208],"top_logprobs":[{" No":-0.15832208,"\n":-2.0068662,"No":-5.53921," NO":-6.3091373," no":-6.393386}],"text_offset":[256]},"finish_reason":"length"}],"usage":{"prompt_tokens":58,"completion_tokens":1,"total_tokens":59}}
{" No":0.8535748183132293,"\n":0.13440922699674815,"No":0.003929630020103305," NO":0.0018196023332171228," no":0.0016725832356423965}
[{" Yes":0.0," No":1.0},null]
0.0
{"id":"cmpl-7lxmU4uqvyKzwZ9uSRIRPj6oVqKK9","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16482681],"top_logprobs":[{" No":-0.16482681,"\n":-1.9835114,"No":-5.474528," no":-6.2271934," Yes":-6.477078}],"text_offset":[482]},"finish_reason":"length"}],"usage":{"prompt_tokens":104,"completion_tokens":1,"total_tokens":105}}
{" No":0.8480405635154519,"\n":0.13758527118759428,"No":0.004192206804691553," no":0.001974987124284348," Yes":0.0015382990284139788}
[{" Yes":0.0018106606652240629," No":0.9981893393347759},null]
0.0018106606652240629
{"id":"cmpl-7lxmUWjUrP9TnB4LHxwh3ryASMhYK","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1700207],"top_logprobs":[{" No":-0.1700207,"\n":-1.9381939,"No":-5.6528854," This":-6.121759," no":-6.326418}],"text_offset":[547]},"finish_reason":"length"}],"usage":{"prompt_tokens":153,"completion_tokens":1,"total_tokens":154}}
{" No":0.8436473529154299,"\n":0.14396372800350693,"No":0.0035073819605379738," This":0.00219459227811065," no":0.001788428460116646}
{"id":"cmpl-7lxmUK1EId1iY8n0Dzp7fvBCrs1ow","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16817866],"top_logprobs":[{" No":-0.16817866,"\n":-1.9623748,"No":-5.488574," no":-5.874931," This":-5.9586663}],"text_offset":[781]},"finish_reason":"length"}],"usage":{"prompt_tokens":146,"completion_tokens":1,"total_tokens":147}}
{"id":"cmpl-7lxmUF9bVmbbhqm6myCDlJEObRHx9","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.119301885],"top_logprobs":[{" No":-0.119301885,"\n":-2.3138287,"No":-5.5044017," This":-5.896399," NO":-6.462557}],"text_offset":[453]},"finish_reason":"length"}],"usage":{"prompt_tokens":91,"completion_tokens":1,"total_tokens":92}}
{"id":"cmpl-7lxmUQLj20pa1G8LGx00lT2ha86vK","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.23931652],"top_logprobs":[{" No":-0.23931652,"\n":-2.0128615," Yes":-2.8799663," The":-4.915693," This":-5.274038}],"text_offset":[1216]},"finish_reason":"length"}],"usage":{"prompt_tokens":240,"completion_tokens":1,"total_tokens":241}}
{" No":0.8452028172593423,"\n":0.14052430722666676,"No":0.00413373467897961," no":0.002808988008011423," This":0.0025833550961612286}
{" No":0.88753982535493,"\n":0.0988819366107346,"No":0.004068822229121192," This":0.002749327342365438," NO":0.0015607996389173468}
{" No":0.7871656892535028,"\n":0.13360581411522152," Yes":0.05613665460751743," The":0.0073306360009929225," This":0.005122882551576758}
[{" Yes":0.06656764921403917," No":0.9334323507859608},null]
0.06656764921403917
{"id":"cmpl-7lxmUXsi2SSY6i3FBvgXpTciVbsYn","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16465892],"top_logprobs":[{" No":-0.16465892,"\n":-1.9522364,"No":-5.947657," no":-6.5366473," NO":-6.807567}],"text_offset":[250]},"finish_reason":"length"}],"usage":{"prompt_tokens":57,"completion_tokens":1,"total_tokens":58}}
{"id":"cmpl-7lxmUD8IvEIfoSSFdzUPo8dXwBfb3","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16818592],"top_logprobs":[{" No":-0.16818592,"\n":-1.9578903,"No":-5.401216," no":-6.088334," This":-6.33013}],"text_offset":[440]},"finish_reason":"length"}],"usage":{"prompt_tokens":100,"completion_tokens":1,"total_tokens":101}}
{"id":"cmpl-7lxmUK0xrfgVXYUmXCEhWVZhIzMve","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.13947517],"top_logprobs":[{" No":-0.13947517,"\n":-2.1120903,"No":-5.923315," This":-6.421521," no":-6.8286157}],"text_offset":[321]},"finish_reason":"length"}],"usage":{"prompt_tokens":73,"completion_tokens":1,"total_tokens":74}}
{"id":"cmpl-7lxmU6xsSWxZbdF2EFfBsgKkNrssx","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.25568113],"top_logprobs":[{" No":-0.25568113,"\n":-2.0686855," Yes":-2.430562," This":-6.3734603,"No":-6.606397}],"text_offset":[160]},"finish_reason":"length"}],"usage":{"prompt_tokens":39,"completion_tokens":1,"total_tokens":40}}
{"id":"cmpl-7lxmUVG5yqgoXNlc6chnl52UfaEJf","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1306631],"top_logprobs":[{" No":-0.1306631,"\n":-2.189519,"No":-5.7485595," This":-6.005399," no":-6.3789754}],"text_offset":[1047]},"finish_reason":"length"}],"usage":{"prompt_tokens":238,"completion_tokens":1,"total_tokens":239}}
{"id":"cmpl-7lxmU8bbnzb4apbKTsbmblIe2ydCN","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.15648858],"top_logprobs":[{" No":-0.15648858,"\n":-2.061882,"No":-5.020625," no":-5.7753544," This":-6.273134}],"text_offset":[853]},"finish_reason":"length"}],"usage":{"prompt_tokens":179,"completion_tokens":1,"total_tokens":180}}
{"id":"cmpl-7lxmUeIdAQcqMkFcz7LY4YXV3VB8P","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.12684128],"top_logprobs":[{" No":-0.12684128,"\n":-2.2162359," NO":-6.049808,"No":-6.097142," no":-6.098762}],"text_offset":[225]},"finish_reason":"length"}],"usage":{"prompt_tokens":53,"completion_tokens":1,"total_tokens":54}}
{" No":0.8481829529982111,"\n":0.14195624537855012,"No":0.002611953160907461," no":0.0014493395631484892," NO":0.0011053790181210082}
{" No":0.8451966811091632,"\n":0.14115590362095093,"No":0.004511092118068116," no":0.0022691862316976874," This":0.0017818021197690422}
{" No":0.8698146204332932,"\n":0.12098480739349493,"No":0.0026763134762118604," This":0.0016261809316004472," no":0.0010823553855904542}
{" No":0.7743888588026541,"\n":0.12635176198172696," Yes":0.08798736978280981," This":0.0017062448796043347,"No":0.001351693538179116}
{" No":0.8775133588476418,"\n":0.11197059352207524,"No":0.0031873688960323467," This":0.002465405455558673," no":0.0016968606696829856}
{" No":0.8551412833606038,"\n":0.12721432709808345,"No":0.006600400169799413," no":0.0031030977259186763," This":0.0018863076218229372}
{"id":"cmpl-7lxmUKKUUBE6y8enEOGHIQtMAVbjz","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1586984],"top_logprobs":[{" No":-0.1586984,"\n":-2.059979,"No":-5.3730826," no":-5.7161765," This":-5.791683}],"text_offset":[563]},"finish_reason":"length"}],"usage":{"prompt_tokens":125,"completion_tokens":1,"total_tokens":126}}
{"id":"cmpl-7lxmUBX6bVmWCRshT6ALwgu9YcLaT","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.23540309],"top_logprobs":[{" No":-0.23540309,"\n":-1.6577482,"No":-5.1313868," This":-5.804178," no":-5.8060355}],"text_offset":[1088]},"finish_reason":"length"}],"usage":{"prompt_tokens":209,"completion_tokens":1,"total_tokens":210}}
{"id":"cmpl-7lxmUb1lpx2Rzug8DOA5zCa3kBoxG","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16729647],"top_logprobs":[{" No":-0.16729647,"\n":-2.006052,"No":-5.17741," This":-5.9863257," The":-6.108106}],"text_offset":[983]},"finish_reason":"length"}],"usage":{"prompt_tokens":232,"completion_tokens":1,"total_tokens":233}}
[{" Yes":0.10202898325146728," No":0.8979710167485326},null]
{" No":0.8808734737399968,"\n":0.10901869474907483," NO":0.002358314759458373,"No":0.002249287004216493," no":0.002245646109190897}
{"id":"cmpl-7lxmUACgPWgorjtiDsb7GYETe2qbz","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.20235035],"top_logprobs":[{" No":-0.20235035,"\n":-2.133949," Yes":-2.8461156,"No":-6.6619897," This":-6.959476}],"text_offset":[162]},"finish_reason":"length"}],"usage":{"prompt_tokens":40,"completion_tokens":1,"total_tokens":41}}
{"id":"cmpl-7lxmUqTOWpUIqIOdV5NGiTZlILos4","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16003184],"top_logprobs":[{" No":-0.16003184,"\n":-2.0157046,"No":-5.5583506," no":-5.910231," This":-5.9308963}],"text_offset":[725]},"finish_reason":"length"}],"usage":{"prompt_tokens":151,"completion_tokens":1,"total_tokens":152}}
0.10202898325146728
{" No":0.8532536614702634,"\n":0.12745664645629237,"No":0.0046398065751637385," no":0.0032922748808583845," This":0.0030528399208422405}
{" No":0.7902522426429176,"\n":0.1905676174786152,"No":0.005908361074394523," This":0.003014932008907652," no":0.0030093369707015565}
{" No":0.8459487757228696,"\n":0.13451870755285525,"No":0.005642601843939653," This":0.0025128801840735827," The":0.0022247605213733196}
{"id":"cmpl-7lxmUnzPdv7xLVMbUhqTJXKH7rRns","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.09888972],"top_logprobs":[{" No":-0.09888972,"\n":-2.5204797,"No":-5.609146," The":-6.517559," no":-6.6403475}],"text_offset":[898]},"finish_reason":"length"}],"usage":{"prompt_tokens":187,"completion_tokens":1,"total_tokens":188}}
{" No":0.8168087088755848,"\n":0.11836893077055274," Yes":0.05806944831428685,"No":0.0012785998077390387," This":0.0009495940314682642}
{" No":0.8521166571399116,"\n":0.13322649889616744,"No":0.003855129807617612," no":0.0027115604445725893," This":0.002656100258901518}
[{" Yes":0.0663743263414042," No":0.9336256736585958},null]
{" No":0.9058425988372676,"\n":0.08042101953205874,"No":0.0036641972463808607," The":0.0014772707226220294," no":0.0013065731307758681}
{"id":"cmpl-7lxmUN2KICFC2FPRWozBGBNPCgm7Z","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.20298687],"top_logprobs":[{" No":-0.20298687,"\n":-2.1268208," Yes":-2.8509092,"No":-6.683228," This":-6.932558}],"text_offset":[162]},"finish_reason":"length"}],"usage":{"prompt_tokens":40,"completion_tokens":1,"total_tokens":41}}
{"id":"cmpl-7lxmUOu001c3T6IZ1Dss6iCKqUhM3","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.21203643],"top_logprobs":[{" No":-0.21203643,"\n":-1.7126572,"No":-5.794418," no":-6.2009296," This":-6.506713}],"text_offset":[530]},"finish_reason":"length"}],"usage":{"prompt_tokens":128,"completion_tokens":1,"total_tokens":129}}
{"id":"cmpl-7lxmU6YoSce8OmND4K5un0iJJq6Vh","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.14976668],"top_logprobs":[{" No":-0.14976668,"\n":-2.079618,"No":-5.607529," no":-5.855477," This":-6.4511194}],"text_offset":[475]},"finish_reason":"length"}],"usage":{"prompt_tokens":115,"completion_tokens":1,"total_tokens":116}}
{"id":"cmpl-7lxmUzVJfJI8NDwEqCPlEZZN5Xfh1","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.17352675],"top_logprobs":[{" No":-0.17352675,"\n":-1.9811325," Yes":-4.7723713," This":-5.5369215,"No":-6.039689}],"text_offset":[342]},"finish_reason":"length"}],"usage":{"prompt_tokens":92,"completion_tokens":1,"total_tokens":93}}
{"id":"cmpl-7lxmUt1MfdjGSHY0i02A2XIzdhpg5","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.20383352],"top_logprobs":[{" No":-0.20383352,"\n":-2.131791," Yes":-2.8300235,"No":-6.6796184," This":-6.943609}],"text_offset":[162]},"finish_reason":"length"}],"usage":{"prompt_tokens":40,"completion_tokens":1,"total_tokens":41}}
0.0663743263414042
{" No":0.816288959229282,"\n":0.11921570257680898," Yes":0.057791752719403795,"No":0.0012517308568259762," This":0.0009755023391553354}
{" No":0.8089352275193676,"\n":0.18038583398841807,"No":0.00304450181122122," no":0.0020275449541770755," This":0.0014933804057687596}
{" No":0.8609088202396455,"\n":0.12497794465596121,"No":0.0036701270462794265," no":0.0028641690665769676," This":0.0015787539220928238}
{" No":0.8406946622788778,"\n":0.13791296240700518," Yes":0.008460294458503824," This":0.003938633276451041,"No":0.002382299693624326}
{" No":0.8155981406639856,"\n":0.11862464674144982," Yes":0.05901146688368142,"No":0.0012562572688806204," This":0.00096478141040435}
{"id":"cmpl-7lxmUIMB1YNoXMOWrOjoSOpNVLPM4","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.14523712],"top_logprobs":[{" No":-0.14523712,"\n":-2.0735924," Yes":-6.16601,"No":-6.384129," no":-6.6145134}],"text_offset":[367]},"finish_reason":"length"}],"usage":{"prompt_tokens":77,"completion_tokens":1,"total_tokens":78}}
{"id":"cmpl-7lxmUrzGkLlQvZos09GHTiaTzUllr","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.17562915],"top_logprobs":[{" No":-0.17562915,"\n":-2.030231," Yes":-3.8166423,"No":-6.300642," This":-6.4109497}],"text_offset":[195]},"finish_reason":"length"}],"usage":{"prompt_tokens":57,"completion_tokens":1,"total_tokens":58}}
{"id":"cmpl-7lxmUYPBOxrvIHPkACkCpPNljZcRd","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.25445843],"top_logprobs":[{" No":-0.25445843,"\n":-2.1157494," Yes":-2.4955673," This":-5.328538," The":-5.8713403}],"text_offset":[384]},"finish_reason":"length"}],"usage":{"prompt_tokens":95,"completion_tokens":1,"total_tokens":96}}
[{" Yes":0.06611718109024758," No":0.9338828189097523},null]
[{" Yes":0.009963192691013568," No":0.9900368073089865},null]
[{" Yes":0.06747177983688596," No":0.9325282201631141},null]
{"id":"cmpl-7lxmUuSg8Y1EtB3pG4ENW9ifeFA58","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.26056883],"top_logprobs":[{" No":-0.26056883,"\n":-1.996797," Yes":-2.4797096,"No":-6.5982103," This":-6.7034593}],"text_offset":[160]},"finish_reason":"length"}],"usage":{"prompt_tokens":39,"completion_tokens":1,"total_tokens":40}}
{"id":"cmpl-7lxmU1YqUNHDXeq2PUnn4umSlIpUU","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1928476],"top_logprobs":[{" No":-0.1928476,"\n":-1.8172104,"No":-5.4487724," no":-6.1254454," This":-6.2546344}],"text_offset":[663]},"finish_reason":"length"}],"usage":{"prompt_tokens":142,"completion_tokens":1,"total_tokens":143}}
{"id":"cmpl-7lxmUSYZHbhUsxPnpJ6UAMjk7wui8","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.21658143],"top_logprobs":[{" No":-0.21658143,"\n":-1.7133398,"No":-5.623663," no":-5.9183507," This":-6.017961}],"text_offset":[822]},"finish_reason":"length"}],"usage":{"prompt_tokens":205,"completion_tokens":1,"total_tokens":206}}
0.06611718109024758
0.009963192691013568
0.06747177983688596
{" No":0.8648172033410029,"\n":0.12573328516376397," Yes":0.0020995967006620297,"No":0.0016881382238395327," no":0.0013407670545842049}
{" No":0.8389290424907732,"\n":0.13130518614693534," Yes":0.022001551618437858,"No":0.0018351262477104447," This":0.0016434629639147187}
{" No":0.7753362831501539,"\n":0.12054292083135648," Yes":0.08244966442724774," This":0.004851157222094896," The":0.002819092371263955}
{"id":"cmpl-7lxmUeN6bLoVJ1yjjeU7iJ62UF4sN","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16907404],"top_logprobs":[{" No":-0.16907404,"\n":-1.9812353," Yes":-5.016612," This":-6.038806,"No":-6.0645843}],"text_offset":[862]},"finish_reason":"length"}],"usage":{"prompt_tokens":183,"completion_tokens":1,"total_tokens":184}}
{"id":"chatcmpl-7lxmVQmNyVTWYHuLHDm7aDI3p6lBz","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":257,"completion_tokens":1,"total_tokens":258}}
{"id":"cmpl-7lxmUEsFCQtetYZNel8JrD6BdE1mo","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16193278],"top_logprobs":[{" No":-0.16193278,"\n":-1.9895979,"No":-5.694153," no":-6.2283516," This":-6.5090246}],"text_offset":[997]},"finish_reason":"length"}],"usage":{"prompt_tokens":202,"completion_tokens":1,"total_tokens":203}}
[{" Yes":0.0024219125763407985," No":0.9975780874236592},null]
[{" Yes":0.025555546253066375," No":0.9744444537469337},null]
[{" Yes":0.09611915963429554," No":0.9038808403657044},null]
{" No":0.7706131132500079,"\n":0.13576945710708316," Yes":0.08376754815637331,"No":0.0013628048681721412," This":0.001226661165654808}
{" No":0.8246076347670637,"\n":0.16247836898682838,"No":0.00430158207298517," no":0.002186517026585148," This":0.0019215283384129071}
{" No":0.8052669593642979,"\n":0.18026274463331385,"No":0.0036113883358558966," no":0.002689632531742308," This":0.0024346287454645784}
0.0024219125763407985
0.025555546253066375
0.09611915963429554
[{" Yes":0.09804476147492032," No":0.9019552385250796},null]
{" No":0.8444463782615362,"\n":0.1378987856831658," Yes":0.006626940793825041," This":0.0023844041932536445,"No":0.0023237237857582443}
"No"
{" No":0.8504983731194166,"\n":0.13675040172833644,"No":0.003365586624081988," no":0.0019727010183365106," This":0.00148993229448522}
{"id":"chatcmpl-7lxmVLHId5w3tHReGBawCwjMjaKHF","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"Yes"},"finish_reason":"stop"}],"usage":{"prompt_tokens":83,"completion_tokens":1,"total_tokens":84}}
{"id":"cmpl-7lxmUemllwZSOSGPhGzQnQIIMnIk4","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.16368252],"top_logprobs":[{" No":-0.16368252,"\n":-2.0989757," Yes":-4.2812557," This":-5.570016,"No":-5.683593}],"text_offset":[334]},"finish_reason":"length"}],"usage":{"prompt_tokens":79,"completion_tokens":1,"total_tokens":80}}
{"id":"cmpl-7lxmUjhzrptyucsULLPLqIyhKQEEk","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.24141671],"top_logprobs":[{" No":-0.24141671,"\n":-2.1021192," Yes":-2.505017,"No":-6.379395," no":-6.732176}],"text_offset":[158]},"finish_reason":"length"}],"usage":{"prompt_tokens":38,"completion_tokens":1,"total_tokens":39}}
{"id":"cmpl-7lxmURkLgxRX25k5nPWApYshR1p79","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.12320845],"top_logprobs":[{" No":-0.12320845,"\n":-2.23142,"No":-5.716624," no":-6.842444," NO":-7.1586285}],"text_offset":[262]},"finish_reason":"length"}],"usage":{"prompt_tokens":67,"completion_tokens":1,"total_tokens":68}}
{"id":"cmpl-7lxmUD2BJlGrktYl0UsZjp4PizCCf","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.13482939],"top_logprobs":[{" No":-0.13482939,"\n":-2.1798503,"No":-5.7871804," no":-5.9184995," This":-6.0080285}],"text_offset":[832]},"finish_reason":"length"}],"usage":{"prompt_tokens":158,"completion_tokens":1,"total_tokens":159}}
{"id":"cmpl-7lxmUKgeIRixyBsh0uyktJFdPesZI","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.22655724],"top_logprobs":[{" No":-0.22655724,"\n":-1.6985755," Yes":-5.424482,"No":-5.723932," This":-5.8438635}],"text_offset":[948]},"finish_reason":"length"}],"usage":{"prompt_tokens":212,"completion_tokens":1,"total_tokens":213}}
{"id":"cmpl-7lxmUY4e5GFYlNTDHZmyi7m2wT3Mp","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.10796789],"top_logprobs":[{" No":-0.10796789,"\n":-2.5289779," Yes":-4.5389676,"No":-5.897394," This":-5.994408}],"text_offset":[596]},"finish_reason":"length"}],"usage":{"prompt_tokens":151,"completion_tokens":1,"total_tokens":152}}
0.09804476147492032
[{" Yes":0.007786568613360521," No":0.9922134313866394},null]
{"id":"chatcmpl-7lxmV3rimdJklqkHeUwULTXzTKHqX","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":206,"completion_tokens":1,"total_tokens":207}}
0.007786568613360521
"Yes"
{" No":0.8490115232757012,"\n":0.12258192463444073," Yes":0.013825290764564542," This":0.0038104194653928564,"No":0.0034013155354633766}
{" No":0.7855142265443454,"\n":0.12219719337252634," Yes":0.08167420951192976,"No":0.0016961488163032702," no":0.0011919364804165466}
{" No":0.8840793570102734,"\n":0.10737584812386945,"No":0.0032908019174493442," no":0.0010674912602654217," NO":0.0007781210146961097}
{" No":0.8738649890764125,"\n":0.11305845422428538,"No":0.0030666166301003173," no":0.002689232344196277," This":0.0024589311876873675}
{" No":0.7972737050599837,"\n":0.18294394217151616," Yes":0.004407348577174765,"No":0.0032668403990671635," This":0.0028976259940084515}
{" No":0.8976564197153116,"\n":0.0797404813941398," Yes":0.010684431468530665,"No":0.002746593122159914," This":0.0024926521870456686}
{"id":"cmpl-7lxmU5LMpmmNxLN9t1S6rVFixU1KU","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.24688672],"top_logprobs":[{" No":-0.24688672,"\n":-2.0955288," Yes":-2.4642222,"No":-6.4089937," no":-6.735755}],"text_offset":[158]},"finish_reason":"length"}],"usage":{"prompt_tokens":38,"completion_tokens":1,"total_tokens":39}}
{"id":"cmpl-7lxmUyxThsECGZhPxx38wVwCnXGCb","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1715584],"top_logprobs":[{" No":-0.1715584,"\n":-1.9144363,"No":-5.936413," no":-6.0636973," NO":-6.5156207}],"text_offset":[502]},"finish_reason":"length"}],"usage":{"prompt_tokens":112,"completion_tokens":1,"total_tokens":113}}
{"id":"cmpl-7lxmUE1Zi1C6B4zvR4zW7IqrGid0M","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.46157834],"top_logprobs":[{" No":-0.46157834," Yes":-1.5946484,"\n":-2.0022714," This":-4.1973906," The":-5.5784554}],"text_offset":[525]},"finish_reason":"length"}],"usage":{"prompt_tokens":124,"completion_tokens":1,"total_tokens":125}}
{"id":"cmpl-7lxmUNat0x5pS1KPi1rY7kbuGVGtR","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.14446598],"top_logprobs":[{" No":-0.14446598,"\n":-2.115933,"No":-5.6823406," no":-6.009939," This":-6.200708}],"text_offset":[789]},"finish_reason":"length"}],"usage":{"prompt_tokens":164,"completion_tokens":1,"total_tokens":165}}
{"id":"chatcmpl-7lxmVOgfi5RjTzIQHBzLxmXZo5jxA","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":146,"completion_tokens":1,"total_tokens":147}}
[{" Yes":1.0," No":0.0},null]
[{" Yes":0.01602306547379116," No":0.9839769345262088},null]
[{" Yes":0.09418277056756048," No":0.9058172294324396},null]
[{" Yes":0.005497633450583622," No":0.9945023665494164},null]
[{" Yes":0.011762579492715345," No":0.9882374205072846},null]
{"id":"chatcmpl-7lxmVmP2N5PiKaITSnWjcfsQsZjr0","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":83,"completion_tokens":1,"total_tokens":84}}
{"id":"cmpl-7lxmUyJ0G8nHO693pbPK95sCFvHTb","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1707819],"top_logprobs":[{" No":-0.1707819,"\n":-1.9715497," no":-5.3823657,"No":-5.427814," This":-6.35284}],"text_offset":[518]},"finish_reason":"length"}],"usage":{"prompt_tokens":129,"completion_tokens":1,"total_tokens":130}}
{"id":"cmpl-7lxmUFtJCBUa8sB2KFkyj4vuzy3pV","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.14898485],"top_logprobs":[{" No":-0.14898485,"\n":-2.0782652,"No":-5.6816688," This":-5.991077," no":-6.4020123}],"text_offset":[603]},"finish_reason":"length"}],"usage":{"prompt_tokens":137,"completion_tokens":1,"total_tokens":138}}
{"id":"chatcmpl-7lxmV7ZnSzJZOveVvK4HuDAcZVp9D","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"Yes"},"finish_reason":"stop"}],"usage":{"prompt_tokens":232,"completion_tokens":1,"total_tokens":233}}
{"id":"cmpl-7lxmUabnKvvtK8RckCvALOOvKhQxW","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1833555],"top_logprobs":[{" No":-0.1833555,"\n":-2.0251112," Yes":-4.028493," This":-5.691404," The":-5.709493}],"text_offset":[648]},"finish_reason":"length"}],"usage":{"prompt_tokens":145,"completion_tokens":1,"total_tokens":146}}
{"id":"chatcmpl-7lxmV4yrZvjTLVXouAnioRCmETLLj","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":91,"completion_tokens":1,"total_tokens":92}}
1.0
0.01602306547379116
0.09418277056756048
0.005497633450583622
0.011762579492715345
{" No":0.7812291861612676,"\n":0.12300518131312256," Yes":0.08507498796998474,"No":0.0016466807234150151," no":0.001187678164554852}
{" No":0.8423510732808722,"\n":0.1474249125910902,"No":0.0026414876942308495," no":0.0023257858431434647," NO":0.0014801368933156636}
{" No":0.6302880511777351," Yes":0.20297988368467146,"\n":0.13502823152491125," This":0.015034757375271301," The":0.003778397126083427}
{" No":0.8654843556800541,"\n":0.12052079118265574,"No":0.003405578011645434," no":0.0024542378843588454," This":0.002027994307925479}
{"id":"cmpl-7lxmUnKRSJotcs6R1nr5lHgkfaQyr","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.23632625],"top_logprobs":[{" No":-0.23632625,"\n":-2.0675802," Yes":-2.7742727,"No":-5.47717," The":-5.7450247}],"text_offset":[772]},"finish_reason":"length"}],"usage":{"prompt_tokens":184,"completion_tokens":1,"total_tokens":185}}
{"id":"chatcmpl-7lxmVsU29QwHxzbl8j0tJAGGhb6yJ","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":43,"completion_tokens":1,"total_tokens":44}}
{"id":"chatcmpl-7lxmVPctORoYKTNUUPObEmLzspcAB","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":126,"completion_tokens":1,"total_tokens":127}}
{"id":"chatcmpl-7lxmVXDoyHCdUaqrQB2PlJTrMrg9d","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":150,"completion_tokens":1,"total_tokens":151}}
{"id":"cmpl-7lxmVRtQdalTxgp6JufMVBsa9Uehf","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.5520292],"top_logprobs":[{" No":-0.5520292," Yes":-1.2710387,"\n":-2.0785387," The":-5.2389245,"Yes":-5.9133077}],"text_offset":[787]},"finish_reason":"length"}],"usage":{"prompt_tokens":171,"completion_tokens":1,"total_tokens":172}}
[{" Yes":0.09820452274202614," No":0.9017954772579739},null]
[{" Yes":0.24359497730845545," No":0.7564050226915445},null]
{" No":0.8430054129037554,"\n":0.13924090729621227," no":0.004596934089413046,"No":0.0043926877302505945," This":0.0017417934120121344}
{" No":0.8615821677699242,"\n":0.12514712923013613,"No":0.0034078666476204072," This":0.002500969055535162," no":0.0016582170833322693}
{" No":0.832472159274215,"\n":0.13197916628851844," Yes":0.01780113606222925," This":0.0033748513502432358," The":0.0033143524957117013}
{"id":"cmpl-7lxmU1J6yW0h75dsPKs1GXDcIqWjp","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.11746513],"top_logprobs":[{" No":-0.11746513,"\n":-2.369566,"No":-5.493884," no":-6.204465," Yes":-6.23291}],"text_offset":[519]},"finish_reason":"length"}],"usage":{"prompt_tokens":133,"completion_tokens":1,"total_tokens":134}}
{"id":"chatcmpl-7lxmVyM2JvAhPCnC2PpoelWLJHRn1","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":215,"completion_tokens":1,"total_tokens":216}}
0.09820452274202614
0.24359497730845545
[{" Yes":0.020935781659690404," No":0.9790642183403097},null]
{" No":0.7895230500151189,"\n":0.12649149579391072," Yes":0.06239483995842287,"No":0.004181145612582413," The":0.003198655543875397}
{" No":0.5757802508616704," Yes":0.2805400734146938,"\n":0.1251129061705084," The":0.005305960328600826,"Yes":0.0027032306073288058}
{"id":"chatcmpl-7lxmV8rLCKVDJGIa4FFh4Lv8Vbbtg","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":151,"completion_tokens":1,"total_tokens":152}}
{"id":"chatcmpl-7lxmVkC1O7pxuI23S4uooUTMblESV","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":180,"completion_tokens":1,"total_tokens":181}}
{"id":"chatcmpl-7lxmVJK4T0pFFNNhN2CcpQEf8Iz66","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":124,"completion_tokens":1,"total_tokens":125}}
{"id":"chatcmpl-7lxmVc0vdBT5CVR0h0yarI3M3YxSN","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":125,"completion_tokens":1,"total_tokens":126}}
{"id":"cmpl-7lxmU1piTcsBDhho4KwVB4QLoXDdo","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.21774393],"top_logprobs":[{" No":-0.21774393,"\n":-1.7059155,"No":-5.568226," This":-6.2575445," no":-6.2906947}],"text_offset":[560]},"finish_reason":"length"}],"usage":{"prompt_tokens":126,"completion_tokens":1,"total_tokens":127}}
{"id":"chatcmpl-7lxmVGyz1WVqJbdx2KxYKCKnljjGM","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":42,"completion_tokens":1,"total_tokens":43}}
{"id":"chatcmpl-7lxmVJbpOOhORlI2ZO97DR42Gzvek","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":217,"completion_tokens":1,"total_tokens":218}}
{"id":"chatcmpl-7lxmVxmrIU8m4pKAdab2222ICGZjw","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":190,"completion_tokens":1,"total_tokens":191}}
{"id":"chatcmpl-7lxmVUKiL2XuHWenP5JM0BYPuTRQ9","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":70,"completion_tokens":1,"total_tokens":71}}
{"id":"chatcmpl-7lxmV94Hn758cg43egvAXt6BBU01a","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":60,"completion_tokens":1,"total_tokens":61}}
{"id":"cmpl-7lxmVelGUN4xevu93TUC7laUoh86H","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.22199479],"top_logprobs":[{" No":-0.22199479,"\n":-1.7558445,"No":-5.0344915," This":-5.4253073," Yes":-5.6560936}],"text_offset":[1030]},"finish_reason":"length"}],"usage":{"prompt_tokens":248,"completion_tokens":1,"total_tokens":249}}
{"id":"chatcmpl-7lxmVrMUxc2jG3vA2InCiHz2hj4bZ","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":114,"completion_tokens":1,"total_tokens":115}}
{"id":"cmpl-7lxmV1dxMMG49XTc8IDCmQVMvS1mT","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.109252214],"top_logprobs":[{" No":-0.109252214,"\n":-2.500892," Yes":-4.481286,"No":-6.1751995," This":-6.2910357}],"text_offset":[433]},"finish_reason":"length"}],"usage":{"prompt_tokens":90,"completion_tokens":1,"total_tokens":91}}
{"id":"cmpl-7lxmVIb2OSrS4xo5L2l83ezsVQOSX","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.14127304],"top_logprobs":[{" No":-0.14127304,"\n":-2.3659205," Yes":-3.7338252,"No":-5.8982153," ":-6.2812724}],"text_offset":[564]},"finish_reason":"length"}],"usage":{"prompt_tokens":138,"completion_tokens":1,"total_tokens":139}}
{"id":"cmpl-7lxmUJumHhmjloGrYQohTRrJrt1ch","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.23603375],"top_logprobs":[{" No":-0.23603375,"\n":-1.6811907," Yes":-5.289303," Trans":-5.649211,"No":-5.843148}],"text_offset":[529]},"finish_reason":"length"}],"usage":{"prompt_tokens":149,"completion_tokens":1,"total_tokens":150}}
{"id":"chatcmpl-7lxmV28lU0XwngTIYPygbZGZ948nW","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":58,"completion_tokens":1,"total_tokens":59}}
0.020935781659690404
[{" Yes":0.07324043865349592," No":0.9267595613465041},null]
[{" Yes":0.32761113506416534," No":0.6723888649358347},null]
{" No":0.8891715166166594,"\n":0.09352130571836477,"No":0.0041118427224178565," no":0.0020203894280227687," Yes":0.001963729122295569}
{"id":"chatcmpl-7lxmVoefoKBvY9880gZWinSozZqoQ","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":230,"completion_tokens":1,"total_tokens":231}}
0.07324043865349592
0.32761113506416534
[{" Yes":0.002203626365004998," No":0.997796373634995},null]
{" No":0.804331380434652,"\n":0.1816060497079597,"No":0.003817246224362385," This":0.0019159446273146804," no":0.0018534718925778775}
{" No":0.8009195371145774,"\n":0.17276128376526337,"No":0.006509507359819463," This":0.0044037126929480225," Yes":0.003496147608392291}
{" No":0.8965042780516825,"\n":0.08201181145145789," Yes":0.01131884775291117,"No":0.002080390838074543," This":0.0018528399664120437}
{" No":0.8682522117463604,"\n":0.0938628598278524," Yes":0.023901233750910526,"No":0.002744338271309933," ":0.0018710183951827634}
{" No":0.7897540192848572,"\n":0.186152192611107," Yes":0.0050452755915428174," Trans":0.003520293190799571,"No":0.002899699987289781}
{"id":"chatcmpl-7lxmVCJAvx7AoQLFbLRKlEapwUujk","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":193,"completion_tokens":1,"total_tokens":194}}
{"id":"chatcmpl-7lxmV4gXRKKBd72vrCPF9KtuMAJ3E","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":71,"completion_tokens":1,"total_tokens":72}}
{"id":"chatcmpl-7lxmVKnj2wcZeZjFQfo1xCSIsO4HD","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":155,"completion_tokens":1,"total_tokens":156}}
{"id":"cmpl-7lxmVQEOY9lUeCUP64mP49KAvLB5r","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.25529012],"top_logprobs":[{" No":-0.25529012,"\n":-2.0963213," Yes":-2.3962882,"No":-6.3746095," This":-6.6435018}],"text_offset":[157]},"finish_reason":"length"}],"usage":{"prompt_tokens":37,"completion_tokens":1,"total_tokens":38}}
{"id":"cmpl-7lxmUCGciR2PlTxfSt0Empamf9OT7","object":"text_completion","created":1691665570,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1845619],"top_logprobs":[{" No":-0.1845619,"\n":-1.8564988,"No":-5.618343," This":-6.0347705," NO":-6.50987}],"text_offset":[473]},"finish_reason":"length"}],"usage":{"prompt_tokens":127,"completion_tokens":1,"total_tokens":128}}
{"id":"chatcmpl-7lxmVY3bsGc66O2lrCHEUV5eDE6Cp","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":85,"completion_tokens":1,"total_tokens":86}}
{"id":"chatcmpl-7lxmVhBtmV0n5u4P9mmtmqQnbXnVW","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":118,"completion_tokens":1,"total_tokens":119}}
{"id":"cmpl-7lxmVnPFYK7ELWy6wT5oIQJ7G7H7c","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.17434236],"top_logprobs":[{" No":-0.17434236,"\n":-2.0191374," Yes":-4.326518,"No":-5.8700533," This":-6.160873}],"text_offset":[483]},"finish_reason":"length"}],"usage":{"prompt_tokens":127,"completion_tokens":1,"total_tokens":128}}
{"id":"cmpl-7lxmV0iYcchXANhbW6zV6nHpwaYG3","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.19130048],"top_logprobs":[{" No":-0.19130048,"\n":-1.9557058," Yes":-4.4194937," This":-5.1587963,"No":-5.7938557}],"text_offset":[1133]},"finish_reason":"length"}],"usage":{"prompt_tokens":271,"completion_tokens":1,"total_tokens":272}}
{"id":"chatcmpl-7lxmV4ZuIYU69dDwFlQ7nzbqw5k6A","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":44,"completion_tokens":1,"total_tokens":45}}
0.002203626365004998
[{" Yes":0.004346195225664103," No":0.9956538047743358},null]
[{" Yes":0.012468120089890197," No":0.9875318799101098},null]
[{" Yes":0.028767029922156528," No":0.9712329700778435},null]
[{" Yes":0.0063478611821458815," No":0.9936521388178542},null]
{"id":"chatcmpl-7lxmV1Hh3QB1htw4ZpK8uvjbvF2wc","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":169,"completion_tokens":1,"total_tokens":170}}
{"id":"chatcmpl-7lxmV9GgBGmycIYxDnQTaOhefXz3k","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":193,"completion_tokens":1,"total_tokens":194}}
{"id":"chatcmpl-7lxmV6PPahmKO575Vt6P44EqZ0dtF","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":152,"completion_tokens":1,"total_tokens":153}}
{"id":"chatcmpl-7lxmVYycRV0UKEMb332lp86Ibu2fa","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":156,"completion_tokens":1,"total_tokens":157}}
{"id":"chatcmpl-7lxmV0iGytxsC9zzBbm6TAz64HBtc","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":41,"completion_tokens":1,"total_tokens":42}}
{"id":"chatcmpl-7lxmVqb9D6g9V3UxN7fXD9HKWEkOC","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":97,"completion_tokens":1,"total_tokens":98}}
{"id":"chatcmpl-7lxmV0H0g60pywDL09TIjhnfG0HIq","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":97,"completion_tokens":1,"total_tokens":98}}
{"id":"cmpl-7lxmVmwP2ax90gKGhW9fyzb36NKbG","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.12227479],"top_logprobs":[{" No":-0.12227479,"\n":-2.349935," Yes":-4.758583,"No":-5.8303304," This":-6.2695303}],"text_offset":[315]},"finish_reason":"length"}],"usage":{"prompt_tokens":71,"completion_tokens":1,"total_tokens":72}}
{"id":"chatcmpl-7lxmV6THax3pq04qQXocGd4jPJipx","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"Yes"},"finish_reason":"stop"}],"usage":{"prompt_tokens":242,"completion_tokens":1,"total_tokens":243}}
{"id":"chatcmpl-7lxmVjPkBdMee3Mnvr1vnQMZ6JsDX","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":108,"completion_tokens":1,"total_tokens":109}}
0.004346195225664103
0.012468120089890197
0.028767029922156528
0.0063478611821458815
{" No":0.7746917117957502,"\n":0.12290773832381638," Yes":0.09105530589380893,"No":0.0017042851892424484," This":0.0013024583002661453}
{" No":0.8314684704081214,"\n":0.15621862664477051,"No":0.003630652118028977," This":0.002394045897840148," NO":0.001488673238001625}
{" No":0.8400092628526846,"\n":0.13276994305155054," Yes":0.013213476806975774,"No":0.0028227228788762974," This":0.0021104100792970534}
{" No":0.8258843891223957,"\n":0.14146459573861883," Yes":0.012040326759215556," This":0.005748615130791505,"No":0.0030462142159871516}
{"id":"cmpl-7lxmVYORzZgbGVXoJ2Vju6WsbkpoS","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.09819828],"top_logprobs":[{" No":-0.09819828,"\n":-2.5395987," Yes":-5.714175,"No":-5.72963," no":-5.8687396}],"text_offset":[438]},"finish_reason":"length"}],"usage":{"prompt_tokens":94,"completion_tokens":1,"total_tokens":95}}
{"id":"cmpl-7lxmVTeKtErMk1MGxqfeOn3zV2cm3","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.2546559],"top_logprobs":[{" No":-0.2546559,"\n":-2.103179," Yes":-2.392085,"No":-6.3728733," This":-6.643425}],"text_offset":[157]},"finish_reason":"length"}],"usage":{"prompt_tokens":37,"completion_tokens":1,"total_tokens":38}}
{"id":"cmpl-7lxmVndqDKPaA2YQu9V8iNBV1rz13","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.23640229],"top_logprobs":[{" No":-0.23640229,"\n":-1.6665285,"No":-4.8852816," no":-5.693328," This":-6.0436935}],"text_offset":[537]},"finish_reason":"length"}],"usage":{"prompt_tokens":118,"completion_tokens":1,"total_tokens":119}}
{"id":"chatcmpl-7lxmVlirw2tsRKKLayU0FsZe5tCIw","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":128,"completion_tokens":1,"total_tokens":129}}
{"id":"chatcmpl-7lxmVqk6pcwMrv3nGkIs37vGiwpso","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":42,"completion_tokens":1,"total_tokens":43}}
{"id":"chatcmpl-7lxmVuS2fxj8jjNBHe3uGYIbjXP1J","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":82,"completion_tokens":1,"total_tokens":83}}
{"id":"cmpl-7lxmVTt9eiwOUjFXJZfXBNAb9yzHJ","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.15075912],"top_logprobs":[{" No":-0.15075912,"\n":-2.137651," Yes":-4.9461746,"No":-5.70099," The":-6.0533533}],"text_offset":[425]},"finish_reason":"length"}],"usage":{"prompt_tokens":93,"completion_tokens":1,"total_tokens":94}}
{"id":"cmpl-7lxmVssgzkuT0doOzGmf3QxtDMIiy","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.13705194],"top_logprobs":[{" No":-0.13705194,"\n":-2.1572046,"No":-5.5205708," NO":-6.350145," This":-6.563331}],"text_offset":[382]},"finish_reason":"length"}],"usage":{"prompt_tokens":87,"completion_tokens":1,"total_tokens":88}}
{"id":"cmpl-7lxmVxg20hQ4UBkD1rHKiOyWXhcdF","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.124508515],"top_logprobs":[{" No":-0.124508515,"\n":-2.2484539,"No":-5.803576," no":-6.1398892," This":-6.3073263}],"text_offset":[920]},"finish_reason":"length"}],"usage":{"prompt_tokens":190,"completion_tokens":1,"total_tokens":191}}
{"id":"chatcmpl-7lxmVGPSWukxj7d617Zs3zcz1URoL","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":94,"completion_tokens":1,"total_tokens":95}}
[{" Yes":0.1051754196471973," No":0.8948245803528027},null]
[{" Yes":0.015486550220457626," No":0.9845134497795424},null]
[{" Yes":0.014369222593640155," No":0.9856307774063598},null]
{" No":0.8849051719979604,"\n":0.09537536141256536," Yes":0.00857775546947398,"No":0.0029371063999379765," This":0.0018931175717380854}
{"id":"cmpl-7lxmVWKndwkWeRhtIKfghtlLc9Lij","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1972087],"top_logprobs":[{" No":-0.1972087,"\n":-1.8192408," Yes":-5.0882397,"No":-6.123213," This":-6.1420135}],"text_offset":[464]},"finish_reason":"length"}],"usage":{"prompt_tokens":114,"completion_tokens":1,"total_tokens":115}}
{"id":"chatcmpl-7lxmVgHjN4AZ1fiCCUjiQYWWPzXdh","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":63,"completion_tokens":1,"total_tokens":64}}
{"id":"chatcmpl-7lxmVdWL5nUH5sr9mc4pB9NI2m2ki","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":138,"completion_tokens":1,"total_tokens":139}}
{"id":"cmpl-7lxmVUi2YLnMCX6Y4UFRy12AncQJI","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.32880282],"top_logprobs":[{" No":-0.32880282,"\n":-1.6784197," Yes":-2.9698741," The":-4.0861316," This":-4.8266435}],"text_offset":[1096]},"finish_reason":"length"}],"usage":{"prompt_tokens":239,"completion_tokens":1,"total_tokens":240}}
{"id":"chatcmpl-7lxmV2kzuhUewYAqMe1sDNQsLSnbF","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":70,"completion_tokens":1,"total_tokens":71}}
{"id":"chatcmpl-7lxmVAsXvJx1YJUG0qeneofVH368v","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":43,"completion_tokens":1,"total_tokens":44}}
{"id":"chatcmpl-7lxmVHqc1bn1hm0C5VQ0Oq2WPUTZM","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":100,"completion_tokens":1,"total_tokens":101}}
0.1051754196471973
0.015486550220457626
0.014369222593640155
[{" Yes":0.009600357439159489," No":0.9903996425608406},null]
{" No":0.9064691512305387,"\n":0.07889805522814966," Yes":0.0032988709678645005,"No":0.0032482788744768744," no":0.002826433526727665}
{" No":0.7751831926102122,"\n":0.12206775738723372," Yes":0.09143883501569729,"No":0.001707246739365734," This":0.0013025583329048106}
{" No":0.7894630169648812,"\n":0.18890170095292383,"No":0.0075569954124758325," no":0.00336836437871309," This":0.0023727788503327216}
{" No":0.8600548437204081,"\n":0.11793153910035216," Yes":0.007110557695908482,"No":0.003342654590817785," The":0.0023499686296506467}
{" No":0.8719249371747285,"\n":0.115647951891873,"No":0.004003562056538875," NO":0.0017464938762881523," This":0.001411177241354336}
{" No":0.8829307431791014,"\n":0.10556230834238935,"No":0.0030167475443983694," no":0.0021551623970626215," This":0.001822900618738664}
{"id":"chatcmpl-7lxmVxlIwpkQieXqka7kjIjuqFhVE","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":41,"completion_tokens":1,"total_tokens":42}}
{"id":"cmpl-7lxmVWVIBdXgGddFYsMMFNVpV7Dsz","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.2528765],"top_logprobs":[{" No":-0.2528765,"\n":-2.1032832," Yes":-2.4068902,"No":-6.3776555," This":-6.6262965}],"text_offset":[157]},"finish_reason":"length"}],"usage":{"prompt_tokens":37,"completion_tokens":1,"total_tokens":38}}
{"id":"chatcmpl-7lxmVsoBKFTNstiJ39JfYUn9Q7L1I","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"Yes"},"finish_reason":"stop"}],"usage":{"prompt_tokens":122,"completion_tokens":1,"total_tokens":123}}
{"id":"chatcmpl-7lxmVhWB1qBgAWC1xss3j5HaREfBm","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":196,"completion_tokens":1,"total_tokens":197}}
{"id":"chatcmpl-7lxmVAKaqWxHvuLCAo3fLGO6eVtLN","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":44,"completion_tokens":1,"total_tokens":45}}
{"id":"chatcmpl-7lxmVnayiEXMGJLJ0sqi1warR4srj","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":41,"completion_tokens":1,"total_tokens":42}}
{"id":"cmpl-7lxmVeAzluphHguGmC68ob38MDU3V","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.20237237],"top_logprobs":[{" No":-0.20237237,"\n":-1.9465349," Yes":-3.7941492," The":-5.731549,"No":-5.731923}],"text_offset":[487]},"finish_reason":"length"}],"usage":{"prompt_tokens":112,"completion_tokens":1,"total_tokens":113}}
{"id":"cmpl-7lxmVVxppoXj2LJ8323OrblV0bSga","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.1789419],"top_logprobs":[{" No":-0.1789419,"\n":-2.0479364," Yes":-3.9998446,"No":-5.5280786," The":-5.992037}],"text_offset":[1320]},"finish_reason":"length"}],"usage":{"prompt_tokens":299,"completion_tokens":1,"total_tokens":300}}
{"id":"chatcmpl-7lxmVhBeCexMGQMdWsqqlqltC3oX0","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":148,"completion_tokens":1,"total_tokens":149}}
{"id":"chatcmpl-7lxmVmCQsjwqU5n4UsN8yKtZIWoDp","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"Yes"},"finish_reason":"stop"}],"usage":{"prompt_tokens":170,"completion_tokens":1,"total_tokens":171}}
{"id":"chatcmpl-7lxmVNGJkKulXPbOeYESmnKHX5ZuT","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":183,"completion_tokens":1,"total_tokens":184}}
{"id":"cmpl-7lxmV3K372SV2kkZbkNBqgSnBIbo1","object":"text_completion","created":1691665571,"model":"text-davinci-002","choices":[{"text":" No","index":0,"logprobs":{"tokens":[" No"],"token_logprobs":[-0.09633864],"top_logprobs":[{" No":-0.09633864,"\n":-2.627725," Yes":-4.323939,"No":-6.5412545," This":-6.7703304}],"text_offset":[326]},"finish_reason":"length"}],"usage":{"prompt_tokens":70,"completion_tokens":1,"total_tokens":71}}
{"id":"chatcmpl-7lxmVqGdtB85MqX5ISqy5QQCZilNY","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":149,"completion_tokens":1,"total_tokens":150}}
0.009600357439159489
[{" Yes":0.0036260572886404213," No":0.9963739427113596},null]
[{" Yes":0.10551178264668833," No":0.8944882173533116},null]
[{" Yes":0.00819977098290016," No":0.9918002290170999},null]
{" No":0.8210192687100039,"\n":0.1621488075903859," Yes":0.006168869395969876,"No":0.002191403659625675," This":0.002150589044888381}
{" No":0.7197849299478802,"\n":0.18666873567672934," Yes":0.051309769825306474," The":0.01680411301862279," This":0.008013373063404184}
0.0036260572886404213
0.10551178264668833
0.00819977098290016
[{" Yes":0.007457637642259762," No":0.9925423623577403},null]
[{" Yes":0.06654146350688049," No":0.9334585364931195},null]
{" No":0.776563781528828,"\n":0.12205503858957484," Yes":0.09009503692124125,"No":0.0016991018348124587," This":0.0013250613748171521}
{" No":0.8167907229458411,"\n":0.14276792059593565," Yes":0.022502042412557915," The":0.0032420514044852965,"No":0.0032408391039743458}
{" No":0.8361544785610016,"\n":0.12900083522874384," Yes":0.018318485360182594,"No":0.0039736166659933665," The":0.0024985692773196945}
{" No":0.9081564258984117,"\n":0.07224262740743388," Yes":0.013247598344471019,"No":0.0014426775243698766," This":0.001147315515225431}
{"id":"chatcmpl-7lxmV1vLTXSDCNE1atWebuua4g8lU","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":133,"completion_tokens":1,"total_tokens":134}}
{"id":"chatcmpl-7lxmVr1nfJWhb1oCwzX32hf5Nda1z","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":80,"completion_tokens":1,"total_tokens":81}}
{"id":"chatcmpl-7lxmVpFH7vmSf8I1tjzTSJyDWHKo0","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":141,"completion_tokens":1,"total_tokens":142}}
0.007457637642259762
0.06654146350688049
[{" Yes":0.10395675322656617," No":0.8960432467734338},null]
[{" Yes":0.02681071890682744," No":0.9731892810931725},null]
[{" Yes":0.021438344024507106," No":0.9785616559754929},null]
[{" Yes":0.014377621538343687," No":0.9856223784616563},null]
0.10395675322656617
0.02681071890682744
0.021438344024507106
0.014377621538343687
{"id":"chatcmpl-7lxmVfAc2JNOVotyQTq7xjQ0q5iBQ","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":273,"completion_tokens":1,"total_tokens":274}}
{"id":"chatcmpl-7lxmVPM52S08A9YGmopeS2Z7YQH2m","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":150,"completion_tokens":1,"total_tokens":151}}
{"id":"chatcmpl-7lxmVxxNsb2LlW09B1qzW2YYSpQ1h","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":212,"completion_tokens":1,"total_tokens":213}}
{"id":"chatcmpl-7lxmV4bY2V0N1mSj7yIff4nmQjNLB","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":299,"completion_tokens":1,"total_tokens":300}}
{"id":"chatcmpl-7lxmVMiXhmdPtY1d31MG0T9lsksvZ","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":44,"completion_tokens":1,"total_tokens":45}}
{"id":"chatcmpl-7lxmVBKrI2BMe7NVoRuM577aDsfb4","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":97,"completion_tokens":1,"total_tokens":98}}
{"id":"chatcmpl-7lxmVCojIUSFhdJz2cMCTjI7gH9ES","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":91,"completion_tokens":1,"total_tokens":92}}
{"id":"chatcmpl-7lxmV4LVrTchVjkGZjWxDaeCUnQhW","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":254,"completion_tokens":1,"total_tokens":255}}
[{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},{"sentences":["Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities.","To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.","The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"},{"sentences":["(1)"],"sections":[{"title":"Model Architecture","number":"3.2"}],"section_type":"main"},{"sentences":["."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"},{"sentences":["(5)"],"sections":[{"title":"Knowledgeable Encoder","number":"3.3"}],"section_type":"main"},{"sentences":["UFET.","For Open Entity, we add a new hybrid model UFET for comparison.","UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},{"sentences":["."],"sections":[{"title":"Pre-training for Injecting Knowledge","number":"3.4"}],"section_type":"main"}]
{"id":"chatcmpl-7lxmVZNZrAG8aPjKFmP2zLkaiF65c","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":56,"completion_tokens":1,"total_tokens":57}}
{"id":"chatcmpl-7lxmV8EILFi9CidpqPOg5Iw2XGfL6","object":"chat.completion","created":1691665571,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":115,"completion_tokens":1,"total_tokens":116}}
{"id":"chatcmpl-7lxmWN5CUb9MbKm5m9IAZJxu13Oyj","object":"chat.completion","created":1691665572,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":77,"completion_tokens":1,"total_tokens":78}}
{"id":"chatcmpl-7lxmWuwzRyBxj4FMjlVNsACVEGxPQ","object":"chat.completion","created":1691665572,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":127,"completion_tokens":1,"total_tokens":128}}
{"id":"chatcmpl-7lxmWSdSrDHoh9jCatWVNZnhfiNe5","object":"chat.completion","created":1691665572,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":123,"completion_tokens":1,"total_tokens":124}}
{"id":"chatcmpl-7lxnUiLZMYxyfXRLj21W22LQvJDyi","object":"chat.completion","created":1691665632,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"No"},"finish_reason":"stop"}],"usage":{"prompt_tokens":127,"completion_tokens":1,"total_tokens":128}}
{"doc":null,"source":"async def classify_paragraph(paragraph: Paragraph, question: str, method: str = 'text', model: str = None) -> float:\n    if method == 'text':\n        return await classify_paragraph_by_text(paragraph, question, model)\n    elif method == 'vector':\n        return await classify_paragraph_by_vector(paragraph, question)\n    else:\n        raise NotImplementedError(f\"Unknown method: {method}\")\n"}
{"paragraph":{"sentences":["Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks.","However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.","We argue that informative entities in KGs can enhance language representation with external knowledge.","In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously.","The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks.","The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},"question":"What was the training corpus used?","method":"vector","model":null}
{"paragraph":{"sentences":["To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"question":"What was the training corpus used?","method":"vector","model":null}
{"paragraph":{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"question":"What was the training corpus used?","method":"vector","model":null}
{"paragraph":{"sentences":["Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities.","To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.","The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"question":"What was the training corpus used?","method":"vector","model":null}
{"paragraph":{"sentences":["Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity .","The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.","Open Entity is a completely manually-annotated dataset.","The statistics of these two datasets are shown in Table 1 .","We compare our model with the following baseline models for entity typing: NFGEC.","NFGEC is a hybrid model proposed by Shimaoka et al. (2016).","NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.","As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"question":"What was the training corpus used?","method":"vector","model":null}
{"doc":null,"source":"async def classify_paragraph_by_vector(paragraph: Paragraph, question: str) -> float:\n    relevance = await recipe.agent(\"embedding-ada\").relevance(\n        context=str(paragraph),\n        question=question,\n    )\n    return relevance\n"}
{"paragraph":{"sentences":["Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks.","However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.","We argue that informative entities in KGs can enhance language representation with external knowledge.","In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously.","The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks.","The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},"question":"What was the training corpus used?"}
{"paragraph":{"sentences":["To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},"question":"What was the training corpus used?"}
{"paragraph":{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"question":"What was the training corpus used?"}
{"paragraph":{"sentences":["Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities.","To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.","The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},"question":"What was the training corpus used?"}
{"paragraph":{"sentences":["Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity .","The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.","Open Entity is a completely manually-annotated dataset.","The statistics of these two datasets are shown in Table 1 .","We compare our model with the following baseline models for entity typing: NFGEC.","NFGEC is a hybrid model proposed by Shimaoka et al. (2016).","NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.","As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"},"question":"What was the training corpus used?"}
{"doc":"Generate a relevance score (cosine similarity) from a context and a question.","source":"    async def relevance(\n        self,\n        *,\n        context: str,\n        question: str,\n        verbose: bool = False,\n        default: Optional[float] = None,\n    ) -> float:\n        \"\"\"Generate a relevance score (cosine similarity) from a context and a question.\"\"\"\n        if verbose:\n            self._print_markdown(context)\n            self._print_markdown(question)\n            \n        context_embedding_response = await openai_embedding(context, model=self.model)\n        question_embedding_response = await openai_embedding(question, model=self.model)\n\n        context_embedding = self._extract_embedding(context_embedding_response)\n        question_embedding = self._extract_embedding(question_embedding_response)\n\n        # calculate cosine similarity\n        dot_product = np.dot(context_embedding, question_embedding)\n        norm_product = np.linalg.norm(context_embedding) * np.linalg.norm(\n            question_embedding\n        )\n\n        relevance = float(dot_product / norm_product)\n\n        if verbose:\n            self._print_markdown(relevance)\n        return relevance\n"}
{"self":{"class_name":"OpenAIEmbeddingAgent","model":"text-embedding-ada-002"},"context":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.","question":"What was the training corpus used?","verbose":false,"default":null}
{"self":{"class_name":"OpenAIEmbeddingAgent","model":"text-embedding-ada-002"},"context":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:","question":"What was the training corpus used?","verbose":false,"default":null}
{"self":{"class_name":"OpenAIEmbeddingAgent","model":"text-embedding-ada-002"},"context":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","question":"What was the training corpus used?","verbose":false,"default":null}
{"self":{"class_name":"OpenAIEmbeddingAgent","model":"text-embedding-ada-002"},"context":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.","question":"What was the training corpus used?","verbose":false,"default":null}
{"self":{"class_name":"OpenAIEmbeddingAgent","model":"text-embedding-ada-002"},"context":"Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity . The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1 . We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work.","question":"What was the training corpus used?","verbose":false,"default":null}
[{"sentences":["The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models.","For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens.","Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata.","After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},{"sentences":["To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks.","However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding.","We argue that informative entities in KGs can enhance language representation with external knowledge.","In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously.","The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks.","The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},{"sentences":["Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities.","To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.","The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."],"sections":[{"title":"Pre-training Dataset","number":"4.1"}],"section_type":"main"},{"sentences":["Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types.","To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity .","The training set of FIGER is labeled with distant supervision, and its test set is annotated by human.","Open Entity is a completely manually-annotated dataset.","The statistics of these two datasets are shown in Table 1 .","We compare our model with the following baseline models for entity typing: NFGEC.","NFGEC is a hybrid model proposed by Shimaoka et al. (2016).","NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.","As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work."],"sections":[{"title":"Entity Typing","number":"4.3"}],"section_type":"main"}]
{"doc":null,"source":"async def answer(\n    context: str, question: str, answer_model: str = \"chatgpt\", **kwargs\n) -> str:\n    \n    prompt = F(\nf\"\"\"Background text: \"{context}\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"{question}\"\nAnswer: \"\"\"\n    ).strip()\n\n    prompt = prompt\n    answer = await recipe.agent(answer_model).complete(prompt=prompt, stop='\"')\n    return answer\n"}
{"context":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.","formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.","formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."}]},"question":"What was the training corpus used?","answer_model":"chatgpt"}
{"context":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.","formatted":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"","formatted":""}]},"question":"What was the training corpus used?","answer_model":"chatgpt"}
{"context":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:","formatted":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.","formatted":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."}]},"question":"What was the training corpus used?","answer_model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.","formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.","formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\n(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\n\nThe early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.","formatted":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"","formatted":""}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\nBefore pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.\n\n"},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:","formatted":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.","formatted":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\nTo overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n\nNeural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.","formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.","formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\n(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\n\nThe early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.","formatted":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"","formatted":""}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\nBefore pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.\n\n"},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:","formatted":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.","formatted":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\nTo overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n\nNeural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]},"stop":"\"","max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.","formatted":"(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings.","formatted":"The early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\n(3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\n\nThe early work (Collobert and Weston, 2008;Mikolov et al., 2013;Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7lfzrkPdWFjGfLakF9Mqpt1QAl68Y","object":"chat.completion","created":1691597207,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was English Wikipedia."},"finish_reason":"stop"}],"usage":{"prompt_tokens":345,"completion_tokens":8,"total_tokens":353}}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.","formatted":"Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"","formatted":""}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\nBefore pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata 4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly.\n\n"},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["Background text: \"",{"source":"context","value":{"__fstring__":[{"source":"list((str(p) for p in result[:3]))[0]","value":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.","formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities."},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[1]","value":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:","formatted":"To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:"},{"source":"F(\"\\n\\n\")","value":{"__fstring__":["\n\n"]},"formatted":"\n\n"},{"source":"list((str(p) for p in result[:3]))[2]","value":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE.","formatted":"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."}]},"formatted":"The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google 3 to initialize the Transformer blocks for encoding tokens. Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities.\n\nTo overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n\nNeural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE."},"\"\n\nAnswer the following question about the background text above:\n\nQuestion: \"",{"source":"question","value":"What was the training corpus used?","formatted":"What was the training corpus used?"},"\"\nAnswer:"]}}],"stop":"\"","top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
"The training corpus used was English Wikipedia."
{"id":"chatcmpl-7lxnUc2UQcMfjXdjdyYcdPWxjRUsK","object":"chat.completion","created":1691665632,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was English Wikipedia."},"finish_reason":"stop"}],"usage":{"prompt_tokens":244,"completion_tokens":8,"total_tokens":252}}
{"id":"chatcmpl-7lxnUr4bZQZ10fy1xy1gezfy4Upwq","object":"chat.completion","created":1691665632,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The training corpus used was the English Wikipedia."},"finish_reason":"stop"}],"usage":{"prompt_tokens":394,"completion_tokens":9,"total_tokens":403}}
"The training corpus used was the English Wikipedia."
["Overlap between vector and text: 1","Overlap between text_chatgpt and vector: 2","Overlap between text_chatgpt and text: 2","Total overlap: 1","Order vector and text: [0, -1, -1, -1, -1, -1, -1, -1, -1, -1]","Order text_chatgpt and vector: [0, -1, -1, -1, -1, -1, 1, -1, -1, -1]","Order text_chatgpt and text: [0, 3, -1, -1, -1, -1, -1, -1, -1, -1]","Kendall tau vector and text: -0.2","Kendall tau text_chatgpt and vector: -0.1111111111111111","Kendall tau text_chatgpt and text: -0.3333333333333333","Answer vector: The training corpus used was English Wikipedia.","Answer text: The training corpus used was English Wikipedia.","Answer text_chatgpt: The training corpus used was the English Wikipedia."]

{"doc":null,"source":"async def summarize_paper(\n        paper: Paper, \n        summary_length: int = 3,\n        model: str = 'chatgpt',\n        context: str = None\n):\n    answer = await summarize_paragraphs(\n        paper.paragraphs,\n        summary_length,\n        context=context,\n        model=model\n    )\n\n    return answer\n"}
{"paper":{"document_id":"2022_Peng.pdf"},"summary_length":3,"model":"chatgpt","context":"This is from a scientific articles about a new large language model"}
{"doc":null,"source":"async def summarize_paragraphs(\n    paragraphs: list[Paragraph], summary_length: int = 5, context: str = None, **kwargs\n):\n    if len(paragraphs) > summary_length:\n        # split texts into chunks of size summary_length and recursively summarize\n        paragraphs = await map_async(\n            [paragraphs[i:i+summary_length] for i in range(0, len(paragraphs), summary_length)],\n            lambda chunk: summarize_paragraphs(chunk, summary_length, context, **kwargs)\n        )\n        summary = await summarize_paragraphs(paragraphs, summary_length, context, **kwargs)\n    else:\n        # concatenate summaries and summarize again\n        summary = await summarize(\"\\n\\n\".join([str(p) for p in paragraphs]), context, **kwargs)\n    return summary\n"}
{"paragraphs":[{"sentences":["We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog.","In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses.","Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation.","A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation).","We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics.","Code and data processing scripts are publicly available. 1"],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},{"sentences":["This work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL).","As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced.","GODEL should be of technical interest for two reasons.","First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks.","The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL","be conditioned on information external to the current conversation (e.g., a retrieved document.)","Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models.","We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Our approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019).","Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place?","We take it as a given that it should be fluent and socially engaging.","Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions.","But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user.","In other words, the model must offer utility to the user.","It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["In the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1","[cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) .","We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks.","Our results on these diverse tasks speak to the effectiveness of our approach.","We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["As baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation.","The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research.","While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility).","For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics."," Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities.","BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2 ","The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions.","We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly.","A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix."," Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness.","Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv).","By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets.","Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness).","In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation.","Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},{"sentences":["In the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility.","While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks.","This makes it difficult to analyze results across dialog tasks and datasets.","To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context.","For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat.","The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research.","Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["Considering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ).","The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility.","In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["At its core, GODEL is a standard pre-trained LM, trained on web text data.","We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment.","The dialog context S and environment E are concatenated as a long sequence, which is the input to the model.","A sample training example is shown in Figure 2 ."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food.","System : sure, which area do you prefer ?","User : Bellevue downtown.","<|environment|> Multiple expensive Chinese restaurants in Bellevue downtown.","=> There are multiple restaurants meet your requirement.","peony kitchen is a great Chinese Restaurant.","Would you like to book a table there?","For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens.","For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge.","This has 2 million examples for training.","\u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset.","Each question is associated with multiple passages retrieved by the Bing search engine.","Each passage is annotated as to whether it is selected to compose the final answer.","The selected passages serve as external knowledge for grounded training.","\u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["Similarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions.","\u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples.","Each conversation is annotated with belief states that track users' goals.","The belief states in plain text are used as the external knowledge for grounded training."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ).","The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings.","The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings.","These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions.","We use Byte-Level BPE tokenization as implemented in HuggingFace.","GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) .","We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing.","As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J.","Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs.","Each model is trained until there is no significant loss reduction on the validation data set.","The models are trained for at most 10 epochs, and we select the best versions on the validation set.","We use beam search for inference using a beam size of 5."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We evaluate fine-tuned models in two different setups: few-shot and full.","Our evaluation focuses primarily on few-shot finetuning, for the following reasons.","First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few.","Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models.","Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["GODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples.","We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Evaluation datasets:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["To focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog.","\u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences.","At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences.","Similarly, we use the gold retrieved sentences provided at each turn of the dialog.","\u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively.","Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi.","This dataset is challenging owing to its multi-domain setting and diverse language styles.","For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text.","\u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains.","For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["These existing goal-oriented dialog corpora contain thousands of dialog instances.","For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation.","We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1.","The best model is selected based on the perplexity score on the validation set."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Baseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) .","For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ).","For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ).","For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Automatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer.","\u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Besides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","All BLEU scores in this paper are based on corpuslevel BLEU-4.","In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe.","Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot.","BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ).","time HIT acceptance % > 95.","For each example, we presented workers with dialog history, related knowledge, and two responses from different systems.","The order of the system responses displayed in the interface is randomized.","We consider three questions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Extrinsic evaluation: Which response sounds more useful?","(i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human?","(e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer?","(e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)"],"sections":[{"title":"Model","number":null}],"section_type":"main"},{"sentences":["These were judged on a 5-point Likert scale.","A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."],"sections":[{"title":"Model","number":null}],"section_type":"main"},{"sentences":["Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively.","We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting.","This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence.","We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4 ","dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA.","Table 3 shows the aggregated results on WoW and WoI.","Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods.","Success is the utility function for this task.","As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points.","Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function.","Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting.","However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short.","In general, scaling up the model improves both utility and BLEU scores."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["Our recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3.","In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5 ","We observe that GODEL XL performs better than GPT-3 by a large margin.","This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["We also conducted an ablation study of pretraining stages of GODEL.","Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 .","We observe that dialog pre-training alone hurts performance.","This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation.","GODEL, which incorporates grounded pre-training, achieves the best performance."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["We compare GODEL against T5, since this model family performed best in terms of automatic evaluation.","Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe.","Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks.","We evaluated a total of 4,137 randomly sampled examples from all the tasks.","Table 8 shows the winning percentage of the total.","Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting.","We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks.","It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."],"sections":[{"title":"Human Evaluation Results","number":"5.3"}],"section_type":"main"},{"sentences":["Inter-annotator agreement is shown in Table 9 .","This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings.","Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety.","This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research.","We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic.","Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context.","Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["Table 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation.","Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021).","As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic.","This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic.","We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation.","We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility.","We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing.","Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics.","On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW)."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["We hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric.","These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems.","Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable.","The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging.","The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["In this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways.","First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment.","Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia).","In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness.","Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining.","The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition.","Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small.","We make all of GODEL's code, models, and dataset extraction scripts publicly available."],"sections":[{"title":"Conclusion","number":"7"}],"section_type":"main"},{"sentences":["Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI.","We performed extensive filtering of the social-media data (Reddit) used to train GODEL.","This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit.","This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL).","Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs.","Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available.","We have noted that dialogs geared towards utility appear to tend to be safer.","This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety."],"sections":[{"title":"Ethics Statement","number":null}],"section_type":"main"},{"sentences":["We first provide detailed automatic evaluation for individual datasets.","Results for MultiWOZ are shown in Table 11 .","GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings.","Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively.","Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting.","In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses.","Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem.","Table 14 list the results on CoQA."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["These results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L .","We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU.","Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL .","Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["Table 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts.","We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"},{"sentences":["We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."],"sections":[{"title":"Acknowledgements","number":null}],"section_type":"back"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog.","In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses.","Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation.","A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation).","We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics.","Code and data processing scripts are publicly available. 1"],"sections":[{"title":"Abstract","number":null}],"section_type":"abstract"},{"sentences":["This work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL).","As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced.","GODEL should be of technical interest for two reasons.","First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks.","The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL","be conditioned on information external to the current conversation (e.g., a retrieved document.)","Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models.","We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["Our approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019).","Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place?","We take it as a given that it should be fluent and socially engaging.","Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions.","But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user.","In other words, the model must offer utility to the user.","It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["In the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1","[cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) .","We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks.","Our results on these diverse tasks speak to the effectiveness of our approach.","We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"},{"sentences":["As baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation.","The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."],"sections":[{"title":"Introduction","number":"1"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research.","While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility).","For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics."," Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities.","BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2 ","The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions.","We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly.","A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix."," Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness.","Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv).","By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets.","Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness).","In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation.","Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets."],"sections":[{"title":"Related Work","number":"2"}],"section_type":"main"},{"sentences":["In the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility.","While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks.","This makes it difficult to analyze results across dialog tasks and datasets.","To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context.","For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat.","The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research.","Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["Considering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ).","The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility.","In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training."],"sections":[{"title":"Open-Domain Goal-Directed Dialog","number":"3"}],"section_type":"main"},{"sentences":["At its core, GODEL is a standard pre-trained LM, trained on web text data.","We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment.","The dialog context S and environment E are concatenated as a long sequence, which is the input to the model.","A sample training example is shown in Figure 2 ."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["GODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food.","System : sure, which area do you prefer ?","User : Bellevue downtown.","<|environment|> Multiple expensive Chinese restaurants in Bellevue downtown.","=> There are multiple restaurants meet your requirement.","peony kitchen is a great Chinese Restaurant.","Would you like to book a table there?","For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens.","For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"],"sections":[{"title":"Models","number":"4"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge.","This has 2 million examples for training.","\u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset.","Each question is associated with multiple passages retrieved by the Bing search engine.","Each passage is annotated as to whether it is selected to compose the final answer.","The selected passages serve as external knowledge for grounded training.","\u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["Similarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions.","\u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples.","Each conversation is annotated with belief states that track users' goals.","The belief states in plain text are used as the external knowledge for grounded training."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ).","The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings.","The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings.","These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions.","We use Byte-Level BPE tokenization as implemented in HuggingFace.","GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) .","We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing.","As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J.","Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs.","Each model is trained until there is no significant loss reduction on the validation data set.","The models are trained for at most 10 epochs, and we select the best versions on the validation set.","We use beam search for inference using a beam size of 5."],"sections":[{"title":"Models","number":"4"}],"section_type":"main"},{"sentences":["We evaluate fine-tuned models in two different setups: few-shot and full.","Our evaluation focuses primarily on few-shot finetuning, for the following reasons.","First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few.","Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models.","Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["GODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples.","We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["Evaluation datasets:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["To focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog.","\u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences.","At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences.","Similarly, we use the gold retrieved sentences provided at each turn of the dialog.","\u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively.","Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi.","This dataset is challenging owing to its multi-domain setting and diverse language styles.","For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text.","\u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains.","For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["These existing goal-oriented dialog corpora contain thousands of dialog instances.","For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation.","We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1.","The best model is selected based on the perplexity score on the validation set."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Baseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) .","For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ).","For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ).","For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Automatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer.","\u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018).","\u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["Besides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","All BLEU scores in this paper are based on corpuslevel BLEU-4.","In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe.","Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot.","BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ).","time HIT acceptance % > 95.","For each example, we presented workers with dialog history, related knowledge, and two responses from different systems.","The order of the system responses displayed in the interface is randomized.","We consider three questions:"],"sections":[{"title":"Experimental Setup","number":"5.1"}],"section_type":"main"},{"sentences":["\u2022 Extrinsic evaluation: Which response sounds more useful?","(i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human?","(e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer?","(e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)"],"sections":[{"title":"Model","number":null}],"section_type":"main"},{"sentences":["These were judged on a 5-point Likert scale.","A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."],"sections":[{"title":"Model","number":null}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively.","We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting.","This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence.","We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4 ","dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA.","Table 3 shows the aggregated results on WoW and WoI.","Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods.","Success is the utility function for this task.","As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points.","Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function.","Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting.","However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short.","In general, scaling up the model improves both utility and BLEU scores."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["Our recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3.","In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5 ","We observe that GODEL XL performs better than GPT-3 by a large margin.","This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"},{"sentences":["We also conducted an ablation study of pretraining stages of GODEL.","Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 .","We observe that dialog pre-training alone hurts performance.","This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation.","GODEL, which incorporates grounded pre-training, achieves the best performance."],"sections":[{"title":"Automatic Evaluation Results","number":"5.2"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["We compare GODEL against T5, since this model family performed best in terms of automatic evaluation.","Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe.","Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks.","We evaluated a total of 4,137 randomly sampled examples from all the tasks.","Table 8 shows the winning percentage of the total.","Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting.","We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks.","It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks."],"sections":[{"title":"Human Evaluation Results","number":"5.3"}],"section_type":"main"},{"sentences":["Inter-annotator agreement is shown in Table 9 .","This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings.","Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety.","This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research.","We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic.","Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context.","Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["Table 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation.","Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015).","The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021).","As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic.","This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic.","We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation.","We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility.","We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing.","Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics.","On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW)."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["We hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric.","These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems.","Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable.","The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging.","The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful."],"sections":[{"title":"Analysis and Discussion","number":"6"}],"section_type":"main"},{"sentences":["In this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways.","First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment.","Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia).","In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness.","Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining.","The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition.","Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small.","We make all of GODEL's code, models, and dataset extraction scripts publicly available."],"sections":[{"title":"Conclusion","number":"7"}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI.","We performed extensive filtering of the social-media data (Reddit) used to train GODEL.","This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit.","This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL).","Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs.","Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available.","We have noted that dialogs geared towards utility appear to tend to be safer.","This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety."],"sections":[{"title":"Ethics Statement","number":null}],"section_type":"main"},{"sentences":["We first provide detailed automatic evaluation for individual datasets.","Results for MultiWOZ are shown in Table 11 .","GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings.","Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively.","Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting.","In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses.","Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem.","Table 14 list the results on CoQA."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["These results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L .","We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU.","Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL .","Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":["Table 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts.","We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content."],"sections":[{"title":"A Detailed Results","number":null}],"section_type":"main"},{"sentences":[],"sections":[],"section_type":"main"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":[{"sentences":["We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."],"sections":[{"title":"Acknowledgements","number":null}],"section_type":"back"}],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"doc":null,"source":"async def summarize(\n    text: str, context: str = None, model: str = \"chatgpt\"\n) -> str:\n    if context is None:\n        prompt = F(\nf\"\"\"Summarize the following text:\n\n{text}\n\nSummary:\n\"\"\"\n        ).strip()\n    else:\n        prompt = F(\nf\"\"\"You are given the following context:\n\n{context}\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n{text}\n\nSummary:\n\"\"\"\n        ).strip()\n    answer = await recipe.agent(model).complete(prompt=prompt)\n    return answer\n"}
{"text":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"doc":"Generate an answer to a question given some context.","source":"    async def complete(\n        self,\n        *,\n        prompt: str,\n        stop: Stop = None,\n        verbose: bool = False,\n        default: str = \"\",\n        max_tokens: int = 256,\n    ) -> str:\n        \"\"\"Generate an answer to a question given some context.\"\"\"\n        if verbose:\n            self._print_markdown(prompt)\n        response = await self._complete(prompt, stop=stop, max_tokens=max_tokens)\n        completion = self._extract_completion(response)\n        if verbose:\n            self._print_markdown(completion)\n        return completion\n"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","formatted":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","formatted":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","formatted":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","formatted":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","formatted":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","formatted":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","formatted":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","formatted":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","formatted":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","formatted":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","formatted":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n"},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","formatted":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"doc":"Send a completion request to the OpenAI API with the given prompt and parameters.","source":"    async def _complete(self, prompt, **kwargs) -> dict:\n        \"\"\"Send a completion request to the OpenAI API with the given prompt and parameters.\"\"\"\n        kwargs.update(\n            {\n                \"model\": self.model,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"n\": 1,\n            }\n        )\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = await openai_chatcomplete(messages, **kwargs)\n        if \"choices\" not in response:\n            raise ValueError(f\"No choices in response: {response}\")\n        return response\n"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","formatted":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","formatted":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","formatted":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","formatted":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","formatted":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","formatted":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","formatted":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","formatted":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","formatted":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","formatted":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","formatted":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n"},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","formatted":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"doc":"Send a completion request to the OpenAI API and return the JSON response.","source":"@trace\nasync def openai_chatcomplete(\n    messages: list[dict[str, str]],\n    stop: Optional[str] = \"\\n\",\n    top_p: float = 1,\n    temperature: float = 0,\n    model: str = \"gpt-3.5-turbo\",\n    max_tokens: int = 256,\n    logit_bias: Optional[Mapping[str, Union[int, float]]] = None,\n    n: int = 1,\n    cache_id: int = 0,  # for repeated non-deterministic sampling using caching\n) -> dict:\n    \"\"\"Send a completion request to the OpenAI API and return the JSON response.\"\"\"\n    params = {\n        \"messages\": messages,\n        \"stop\": stop,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"n\": n,\n    }\n    if logit_bias:\n        params[\"logit_bias\"] = logit_bias  # type: ignore[assignment]\n    response = await _post(\"chat/completions\", json=params, cache_id=cache_id)\n    if isinstance(response, TooLongRequestError):\n        raise response\n    add_fields(total_tokens=extract_total_tokens(response))\n    return response\n"}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation.","formatted":"We introduce GODEL (Grounded Open Dialogue Language Model), a large pretrained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot finetuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available. 1\n\nThis work describes the development of a very large pre-trained dialog model -Grounded Open Dialogue Language Model (GODEL). As the name indicates, GODEL is designed for general-domain conversation and is fully open-sourced. GODEL should be of technical interest for two reasons. First, it is pre-trained in three phases, successively folding in data from web text, publiclyavailable dialog (e.g., Reddit), and a collection of existing corpora that support grounded dialog tasks. The grounded dialog corpora, which include MS MARCO (Nguyen et al., 2016) and DSTC7 (Yoshino et al., 2019) , allow for more effective fine-tuning on dialog tasks where responses must 1 http://aka.ms/GODEL be conditioned on information external to the current conversation (e.g., a retrieved document.) Second, GODEL is validated on a utility-driven suite of benchmarks specifically designed for few-shot fine-tuning of open-ended goal-directed generaldomain dialog models. We will show that GODEL, as validated using this methodology, is more readily amenable to fine-tuning for goal-directed dialog tasks than other large pre-trained language models.\n\nOur approach seeks to address a long-standing obstacle to general-purpose open-ended conversation models, namely a lack of robust automated evaluation criteria that can drive development (Gao et al., 2019). Recent state-of-the-art models that leverage large PLMs (e.g., Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Bao et al., 2021; Thoppilan et al., 2022; Gao et al., 2022) offer the potential for substantive open-ended conversational interactions, yet they resist meaningful comparison owing to the lack of consensus on evaluation."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library.","formatted":"This poses a fundamental question: what do we want of a good general-purpose dialog model in the first place? We take it as a given that it should be fluent and socially engaging. Indeed, most SOTA PLMs are primarily evaluated on such intrinsic communicative dimensions. But beyond that, we must also acknowledge that machine-human conversation typically serves a purpose and aims to fulfill one or more goals on the part of the user. In other words, the model must offer utility to the user. It is this extrinsic dimension of functional utility, we suggest, that constitutes the proper focus of automated evaluation in general-domain models.\n\nIn the second half of this paper (Sections 5 and 6), we explore this notion of Utility in finetuning GODEL on four established tasks that cover multiple domains and conversational settings, ranging from the task-oriented MultiWOZ (Budzianowski et al., 2018) to more open-ended goal-oriented tasks, i.e., CoQA (Reddy et al., 2019), Wizard of Wikipedia (Dinan et al., 2018), and Wiz-arXiv:2206.11309v1 [cs.CL] 22 Jun 2022 ard of the Internet (Komeili et al., 2021) . We show that GODEL improves on a baseline DIALOGPT (Zhang et al., 2019b), with performance gains in goal-directed tasks. Our results on these diverse tasks speak to the effectiveness of our approach. We also find that the focal shift to goal-directedness and extrinsic evaluation affords better inter-rater agreement and human-metric correlations.\n\nAs baselines for future research, we release three versions of GODEL: small (GODEL B ) and large (GODEL L ) initiated from T5 and T5-Large respectively, and a very large model (GODEL GPT\u2212J XL ) that is trained off GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021). 2 We also furnish code to download datasets for fine-tuning and evaluation. The release is fully open sourced (models, data, and code) and tightly integrated with the latest version of HuggingFace's transformers library."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:","formatted":"Large pre-trained language models (PLMs) for dialog (Zhang et al., 2019b;Freitas et al., 2020; Roller et al., 2021; Peng et al., 2021; Bao et al., 2021; Thoppilan et al., 2022) have had significant impact in Conversational AI, as these models have brought strong performance gains in competitions such as DSTC (Gunasekara et al., 2020) and ConvAI (Aliannejadi et al., 2020), and have been widely adopted in both task-oriented and chitchat research. While the applications of these dialog models are numerous, empirical evaluation of dialog pre-training has mostly been restricted to intrinsic evaluation (i.e., agnostic to any notion of utility). For example, DialoGPT (Zhang et al., 2019b) offered Reddit-based pre-trained models that are evaluated on Relevance, Informativeness, and Humanness and various string overlap based automated evaluation metrics.  Meena (Freitas et al., 2020) 's evaluation is focused on Sensibleness and Specificity, while also offering a new automated evaluation (SSA) that measures these two qualities. BlenderBot (Roller et al., 2021) contributes a variety of models that capitalize on skills including personality, empathy, and knowledge, but evaluation in this work centers mostly on measures of Humanness and Engagingness. 2  The discussion of our largest model in this paper relates to GODELXL which is based on GPT-3, but which we are unable to release owing to licensing restrictions. We therefore release GODEL GPT\u2212J XL a substitute model based on GPT-J that performs similarly. A comparison of GODEL models initialized from GPT-3 and GPT-J is provided in the Appendix.  Plato-XL (Bao et al., 2021) focuses on intrinsic evaluation with measures of Coherence, Inconsistency, Informativeness, Hallucination, and Engagingness. Plato-XP does however provide some extrinsic evaluation on DSTC9-Track1 (Kim et al., 2020), MultiWOZ 2.2 (Zang et al., 2020), and DuConv (Wu et al., 2019), but is based on taskspecific automated metrics (ROUGE-L for DSTC9, Goal-Accuracy for MultiWOZ, and Knowledge-F1 for DuConv). By contrast, our work provides a more unified measure of Utility, which is measured the same fashion (e.g., same annotation instructions) across dialog tasks and datasets. Finally, LaMDA (Thoppilan et al., 2022) presents dialog models motivated by dialog applications, but is evaluated primarily using intrinsic measures (Sensibleness, Specificity, Interestingness, Safety, Groundedness). In its human evaluation, LaMDA does consider an extrinsic evaluation measure of Utility (Helpfulness), and is probably the closest to our work in terms of analyzing PLMs with a unified extrinsic evaluation. Our works seeks to go further by contrasting intrinsic and extrinsic measures, and analyzing their correlates with automatic evaluation on specific tasks and datasets.\n\nIn the goal-directed framework of this paper, we seek to generate human-like conversational responses that attain a high level of Utility. While prior work in task-oriented dialog has used various measures of Utility (e.g., Inform-rate and Successrate for MultiWOZ (Budzianowski et al., 2018), Knowledge-F1 for Wizard of Wikipedia (Dinan et al., 2018)), these ad-hoc measures are often heterogeneous and only applicable to a limited number of subtasks. This makes it difficult to analyze results across dialog tasks and datasets. To address this limitation, our work seeks to unify the notion of Utility in human evaluation by letting judges decide what appears to be useful to a user given the conversational context. For example, a judge asked to rate conversations in the restaurant domain would generally have no difficulty determining that a dialog system capable of making reservations or providing general information about restaurants should be given higher Utility scores than a system that can only engage in chitchat. The generality of Utility is comparable to that of other qualities such as Humanness and Interestingness often used in chitchat work, but unified evaluation of this kind is rarely used in more task-oriented dialog research. Section 6 will analyze and contrast both intrinsic (e.g., Humanness) and extrinsic (e.g., Utility) qualities measured under human evaluation, and correlate them with automatic evaluation metrics.\n\nConsidering the goal of generating responses that attain high Utility, we cast the open-domain goal-directed dialog generation task as follows: Given a source dialog context S = (s 1 , \u2022 \u2022 \u2022 , s N ) and an environment E, the goal is to generate a target sentence T = (t 1 , \u2022 \u2022 \u2022 , t N ). The conditional probability of P (T |S, E) can be written as the product of a series of conditional probabilities:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:","formatted":"The environment E represents the state of the world and external knowledge (e.g., a database or results of a search engine) that are needed to go beyond chitchat and satisfy a need for Utility. In the pretraining stage, E is generally absent as E tends to be task specific, but we find it effective to substitute E with a grounding text during the third phase of pre-training, i.e., grounded dialog pre-training.\n\nAt its core, GODEL is a standard pre-trained LM, trained on web text data. We utilize a sequence-tosequence Transformer model, as shown in Figure 1 , to generate responses given dialog history and environment. The dialog context S and environment E are concatenated as a long sequence, which is the input to the model. A sample training example is shown in Figure 2 .\n\nGODEL is pre-trained in three phases: (1) Linguistic pre-training on public web documents to User : I would like to find an expensive restaurant that serves Chinese food. System : sure, which area do you prefer ? User : Bellevue downtown. <|environment|> Multiple expensive Chinese restaurants in Bellevue downtown. => There are multiple restaurants meet your requirement. peony kitchen is a great Chinese Restaurant. Would you like to book a table there? For general-domain dialog pre-training, we use a dataset extracted from Reddit comment chains for DialoGPT (Zhang et al., 2019b), which consists of 147M dialog sessions for a total of 6B tokens. For the final grounded dialog pre-training phase, we use a corpus constructed by leveraging existing datasets that support knowledge grounded response generation, conversational question-answering, and task-oriented dialog, as follows:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version.","formatted":"\u2022 The DSTC7 Task 2 corpus is designed for end-to-end conversational modeling that goes beyond chitchat by injecting informational responses grounded in external knowledge. This has 2 million examples for training. \u2022 MS MARCO (Nguyen et al., 2016) is a largescale question-answering dataset. Each question is associated with multiple passages retrieved by the Bing search engine. Each passage is annotated as to whether it is selected to compose the final answer. The selected passages serve as external knowledge for grounded training. \u2022 UnifiedQA (Khashabi et al., 2020) is a largescale question-answering corpus that unifies different QA datasets into a plain text-to-text format.\n\nSimilarly, context paragraphs are treated as the external knowledge on which the model is trained to generate answers given questions. \u2022 The Schema-Guided Dialog is a multi-domain, task-oriented conversation dataset that contains approximately 20k annotated examples. Each conversation is annotated with belief states that track users' goals. The belief states in plain text are used as the external knowledge for grounded training.\n\nWe consider three sizes of model: a 220M pa-rameter base version (GODEL B ), a 770M parameter version (GODEL L ), and a 175B parameter version (GODEL XL ). The 220M parameter base model (GODEL B ) has a 12 layer encoder and 12 layer decoder with 768 dimensional embeddings. The 770M parameter model (GODEL L ) doubles the number of layers for encoder and decoder and has 1024 dimensional embeddings. These two models, GODEL B and GODEL L , are initiated from T5 and T5-Large, respectively, and are based on the Hug-gingFace repository versions. We use Byte-Level BPE tokenization as implemented in HuggingFace. GODEL XL is initiated from GPT-3 and is based on the implementation by Brown et al. (2020) . We chose GPT-3 for our initialization of GODEL XL , despite being aware that we would not be able to release a model due to licensing restrictions, because we wanted to test our grounded fine-tuning recipe on one of the best pre-trained language models available at the time of the writing. As a proxy for the GPT-3-initialized GODEL XL , we release a version pre-trained from GPT-J. Table 15 in the Appendix shows that this substitute model is competitive with the GPT-3-based version."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:","formatted":"GODEL B and GODEL L were trained on 16 Nvidia V100 machines, and GODEL XL was trained with 128 Nvidia V100 GPUs. Each model is trained until there is no significant loss reduction on the validation data set. The models are trained for at most 10 epochs, and we select the best versions on the validation set. We use beam search for inference using a beam size of 5.\n\nWe evaluate fine-tuned models in two different setups: few-shot and full. Our evaluation focuses primarily on few-shot finetuning, for the following reasons. First, datasets of goal-directed dialogs (e.g., MultiWOZ) are more expensive to construct and tend to be much smaller than chitchat datasets, hence we want to assess models' ability to perform well even if fine-tuning examples are relatively few. Second, this work focuses on language model pretraining for dialog, and evaluation in few-shot setups more directly evaluates the effectiveness of our released pre-trained models. Third, few-shot fine-tuning is a realistic approach in application scenarios where it can facilitate fast turnaround of updated models and greater developer control over model characteristics.\n\nGODEL can be used as an initial model to finetune for any open-domain goal-directed dialog tasks with a handful of annotated examples. We evaluate GODEL in terms of its success in finetuning three types of goal-directed dialog, i.e., knowledge-grounded response generation, taskoriented dialog, and conversational QA:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage.","formatted":"Evaluation datasets:\n\n\u2022 Wizard of Wikipedia (Dinan et al., 2018) is a dataset of multi-turn knowledge grounded dialogs between an apprentice and a wizard, who has access to Wikipedia sentences and labels the sentences utilized for each utterance they made.\n\nTo focus on grounded response generation, we use the gold Wikipedia sentences provided at each turn of the dialog. \u2022 Wizard of Internet (Komeili et al., 2021) is an open-domain dialog dataset grounded on internet retrieved sentences. At each turn, the wizard can issue a free text web search and replies to the apprentice grounding the response on the retrieved sentences. Similarly, we use the gold retrieved sentences provided at each turn of the dialog. \u2022 MultiWOZ (Budzianowski et al., 2018) is a multi-domain task-oriented dialog dataset that contains 8438/1000/1000 dialogs for training/validation/testing, respectively. Each dialog session spans multiple domains, such as Attraction, Hotel, Hospital, Police, Restaurant, Train, and Taxi. This dataset is challenging owing to its multi-domain setting and diverse language styles. For this dataset, we consider the task of contextto-response, i.e., the model generates responses given the golden belief state and database status in plain text. \u2022 CoQA (Reddy et al., 2019) is a conversational question answering dataset that contains 127,000 questions and answers collected from seven diverse domains. For each conversation, two workers chat in the form of conversational questions and free-form text-based answers grounding on a provided passage."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:","formatted":"These existing goal-oriented dialog corpora contain thousands of dialog instances. For few-shot fine-tuning, we sample randomly from the corpora 50 dialogs for each task for fine-tuning and use their original test sets for evaluation. We fine-tune GODEL on each task with the same setup as in pre-training, using the training objective shown in Equation 1. The best model is selected based on the perplexity score on the validation set.\n\nBaseline pretrained models We compare GODEL against established pretrained models: T5 (Raffel et al., 2020), BART (Lewis et al., 2020), DialoGPT (Zhang et al., 2019b), and BlenderBot (Roller et al., 2021) . For T5, we fine-tune from both T5-base (T5 B ) and T5-large (T5 L ). For BART, we fine-tune from both BART (BART B ) and BART-large (BART L ). For BlenderBot, we fine-tune from BLENDERBOT 400M , which is distilled from a 3B model. 3\n\nAutomatic evaluation metrics As surrogates for the broad notion of Utility introduced in Section 3, we use the following functions:"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor.","formatted":"\u2022 F R 1 is a metric measuring the average overlap between the prediction and ground truth response or answer. \u2022 F K 1 , i.e., knowledge F1, measures the overlap between the model's response and the knowledge sentences on which the responses are grounded during dataset collection.\n\n\u2022 Inform indicates whether the model provides sufficient information to satisfy users' information seeking needs, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 Success measures whether the model provides appropriate information and answers contain all required attributes, as defined for MultiWOZ (Budzianowski et al., 2018). \u2022 The Combined score (Mehri et al., 2019) defined as (Inform + Success) * 0.5 + BLEU is used as an overall quality measure.\n\nBesides these utility functions (extrinsic), we report intrinsic evaluation results using BLEU, BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). All BLEU scores in this paper are based on corpuslevel BLEU-4. In tables 1 through 5, significance is computed using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 .","formatted":"Human evaluation setup We perform a turnlevel human evaluation to investigate whether GODEL generates responses that are 1) useful, 2) human-like and 3) safe. Using Amazon Mechanical Turk, we hired master-level workers with life- 3 We used version 1.0 of BlenderBot. BlenderBot 2.0 is not evaluated in this paper as it is specifically designed for internet augmented generation and uses a paradigm that is different from that of task-oriented dialog (e.g., MultiWOZ). time HIT acceptance % > 95. For each example, we presented workers with dialog history, related knowledge, and two responses from different systems. The order of the system responses displayed in the interface is randomized. We consider three questions:\n\n\u2022 Extrinsic evaluation: Which response sounds more useful? (i.e., contributes to making the conversation productive, especially towards achieving any stated goals) \u2022 Intrinsic evaluation: Which speaker sounds more human? (e.g., coherent, fluent, and natural) \u2022 Safety evaluation: Which response is socially safer? (e.g., friendly, polite, and empathetic, as opposed to harmful, biased, misinformative, or incomprehensible)\n\nThese were judged on a 5-point Likert scale. A snapshot of the human evaluation interface is provided in Appendix Figure 3 ."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance.","formatted":"Tables 1 and 2 show the aggregated results for all the tasks in few-shot and full fine-tuning settings, respectively. We found that GODEL shows improvements over T5 and DIALOGPT (DGPT) on all metrics, although differences are smaller in the full fine-tuning setting. This highlights the need to focus on the few-shot fine-tuning setup, as models of the same structure (e.g., T5 B and GODEL B ) have a tendency to reach similar levels of performance if fine-tuned until convergence. We also observed that open-domain dialog models such as DialoGPT and Blenderbot exhibit much worse results-to be expected given that these models are trained only on chitchat corpora. 4  dditionally, GODEL shows improvement in intrinsic evaluations on knowledge-grounded generation tasks including WoW and WoI and in extrinsic evaluations on MultiWOZ and CoQA. Table 3 shows the aggregated results on WoW and WoI. Our results show that in the few-shot fine-tuning setting, GODEL achieves better performance in terms of BLEU and maintains a similar level of utility (F K 1 ) to that of baseline methods. Success is the utility function for this task. As shown in Table 4, GODEL outperforms T5 by 23.6 utility scores and 8.5 BLEU points. Table 5 lists the results on CoQA, which focuses on response correctness and hence F R 1 is utilized as the utility function. Again, the results show that GODEL achieves a much better utility score in the few-shot learning setting. However, the BLEU score fails to improve, which is probably not surprising as the responses of CoQA are usually short. In general, scaling up the model improves both utility and BLEU scores.\n\nOur recipe for building goal-directed dialog models generalizes to different PLMs e.g., T5 and GPT-3. In Table 6 , we compare GODEL XL to 175B GPT-3 baselines fine-tuned on each task. 5  We observe that GODEL XL performs better than GPT-3 by a large margin. This indicates that grounded pre-training is beneficial in using GPT-3 on goaldirected dialogs.\n\nWe also conducted an ablation study of pretraining stages of GODEL. Automatic evaluation results in the few-shot fine-tuning setting are shown in Table 7 . We observe that dialog pre-training alone hurts performance. This is likely because the model primarily learns to be conversational in this setting while the downstream tasks all require the capacity for knowledge-grounded generation. GODEL, which incorporates grounded pre-training, achieves the best performance."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits.","formatted":"We compare GODEL against T5, since this model family performed best in terms of automatic evaluation. Furthermore, GODEL is initialized from T5, and comparing T5 and GODEL directly measures the impact of our grounded pre-training recipe. Human raters significantly prefer GODEL over T5 both in terms of Extrinsic and Intrinsic evaluations on all the tasks. We evaluated a total of 4,137 randomly sampled examples from all the tasks. Table 8 shows the winning percentage of the total. Evaluation results are converted from a 5-scale Likert-like scale to a win/tie/loss scale for reporting. We observed a strong preference for GODEL over T5 in terms of both Extrinsic and Intrinsic evaluations on all tasks. It is interesting to note that GODEL performs similarly to T5 on CoQA in terms of utility score but has significantly higher Extrinsic and Intrinsic scores in human evaluation on this task, presumably because the goal of CoQA is more explicit than other tasks.\n\nInter-annotator agreement is shown in Table 9 . This is computed using Krippendorff's alpha (Krippendorff, 2011) which is suitable for Likert-scale crowd-sourced evaluation as it is applicable to any number of coders, to missing data, and to ordinal ratings. Inter-rater agreement is generally higher for Extrinsic evaluation compared to Intrinsic and Safety. This suggests that Utility is more reliably evaluated than Humanness-an interesting finding given that the latter is more commonly evaluated in open-ended dialog research. We note that on WoW agreement on Intrinsic is slightly higher than Extrinsic. Conversations in WoW lean more towards chitchat and there may not always be an overarching sense of what is useful in a given conversational context. Finally, we suggest that low agreement on CoQA data is due to the prevalence of short, factual responses (e.g., a name) in this dataset, which may make it harder for judges to tease system responses apart.\n\nTable 10 analyzes the correlation between human evaluation results of Section 5 and several common automated evaluation metrics for generation. Here, we consider BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020), BERTScore (Zhang et al., 2019a), and chrF (Popovi\u0107, 2015). The chrF metric is a lexicalmatch metric similar to BLEU, but is characterbased rather than word-based and but has been found to be more robust than other surface-level metrics (Kocmi et al., 2021). As the distinction between Intrinsic and Extrinsic measures of quality for open-ended response generation is relatively new, we sought to determine whether some metrics are better suited than others for measuring these different traits."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available.","formatted":"First, we observe that these metrics tend to correlate better with Extrinsic than with Intrinsic. This is perhaps not surprising considering the higher level of inter-annotator agreement on Extrinsic. We also observe that levels of correlation involving LMbased metrics (BERTScore and BLEURT) are inferior to those of lexical metrics (e.g., chrF), while LM-based metrics tend to better correlate with intrinsic evaluation. We attribute this to the fact that their focus on language modeling naturally primes them to measure Humanness (e.g., fluency and well-formedness), and that may sometimes hamper their ability to recognize Utility. We are not exactly sure why chrF performs so well on Extrinsic relative to metrics generally regarded as superior (e.g., BLEURT), but we hypothesize that usefulness in tasks such as QA and knowledge-grounding requires getting factual information exactly right (e.g., a year or a person's name) with little need or room for paraphrasing. Finally, we find it interesting that even Safety enjoys decent correlation with automated metrics. On task-oriented dialog (Mul-tiWOZ), Safety correlates best with chrF, but with BERTScore on a more chitchatty dataset (WoW).\n\nWe hypothesize this increased correlation is due to references being generally quite safe, so that unsafe or harmful statements are less likely to be favored by reference-based metric. These inter-annotator and correlation results argue for adopting extrinsic evaluation for openended dialog systems. Whether or not the dialog system is associated with a specific task, conversations tend to have one or multiple goals, and judging the utility of responses as steps towards these goals is both natural and desirable. The results shown here also suggest that a shift towards more extrinsic qualities could make automatic dialog evaluation less challenging. The assessment of Humanness (Intrinsic evaluation) still has a purpose, but mainly to ensure that dialog systems do not sacrifice either Humanness or Safety in order to be become more useful.\n\nIn this paper, we presented a new large-scale pretrained model for dialogue (GODEL) that improves on (Zhang et al., 2019b) in two main ways. First, it is textually grounded and therefore suitable for downstream grounded text generation tasks that rely on external knowledge, e.g., provided by the dialog's environment. Second, it is designed for open-ended goal-directed dialog and evaluated on a variety of tasks ranging from traditional taskoriented dialog (MultiWOZ) and scenarios that mainly involve informing the user (e.g., Wizard of Wikipedia). In our evaluation setup we have been able to disentangle intrinsic and extrinsic evaluation, which respectively capture Utility and Humanness. Considering that most conversations are aimed at being useful, we advocate greater use of extrinsic evaluation in research on dialog pretraining. The evidence that extrinsic evaluation enjoys greater inter-annotator agreement and correlation with automated evaluation metrics further supports this proposition. Our experiments show that GODEL is quite competitive when evaluated against state-of-the-art pre-trained models for dialog and generation (T5, BART, and BlenderBot), and outperforms these models by a large margin in few-shot fine-tuning setups appropriate to goaldirected datasets that are often small. We make all of GODEL's code, models, and dataset extraction scripts publicly available."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU.","formatted":"Ensuring that responses generated by dialog systems are safe and inoffensive is a long-standing problem in Conversational AI. We performed extensive filtering of the social-media data (Reddit) used to train GODEL. This involved block-word filtering, and excluding troll responses (e.g., large number of downvotes) and entire subreddits marked as notsafe-for-work or that have been banned by Reddit. This extensive filtering reduces dataset size to 25% of that prior to filtering, with the result that our data is relatively small relative to that of other Redditbased models (e.g., Plato-XL). Nonetheless, harderto-detect forms of social bias and other toxicity that do not involve overtly offensive terms persist the training data, and we acknowledge that GODEL may generate harmful responses, especially when challenged with adversarial inputs. Further mitigation constitutes a major area for future research, and it is in part to this end that we make our models and dataset extraction code available. We have noted that dialogs geared towards utility appear to tend to be safer. This suggests that future work aimed at optimizing utility might offer the added benefit of increasing model safety.\n\nWe first provide detailed automatic evaluation for individual datasets. Results for MultiWOZ are shown in Table 11 . GODEL B achieves the best results on Combined score compared to the competitors in both few-shot and full fine-tuning settings. Results for Wizard of Wikipedia and Wizard of Internet are listed in Table 12 and 13 , respectively. Both GODEL B and GODEL L obtain better BLEU and F R 1 than competitors of the same model size in the few-shot fine-tuning setting, but this gap is not observable in the full fine-tuning setting. In addition, T5 and GODEL achieve high F K 1 as they tend to copy content from the given knowledge into responses. Whether F K 1 is an appropriate metric for knowledge grounded generation tasks is still an open problem. Table 14 list the results on CoQA.\n\nThese results also show that GODEL XL has significant lower BLEU scores than GODEL and GODEL L . We speculate that this is due to GODEL XL having a decoder-only transformer architecture, which tends it be more abstractive and therefore to be less favored by exact-match metrics such as BLEU. Encoder-decoder transformer models such GODEL and GODEL L , on the other hand, are more likely to copy words or phrases directly from the grounding, which yields more a extractive behavior favored by metrics such as BLEU."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n","formatted":"As we are unable to distribute the GPT-3 version of GODEL XL (GODEL GPT\u22123 XL below), we instead release GODEL XL based on GPT-J (Gao et al., 2020;Wang and Komatsuzaki, 2021) (GODEL GPT\u2212J XL ) as a proxy for GODEL XL . Table 15 compares the results of GODEL GPT\u22123 XL and GODEL GPT\u2212J XL on all tasks in the few-shot setting.\n\nTable 16 provides examples of baselines, GODEL L , and GODEL XL in given contexts. We observe that GODEL L based on T5 is more extractive while GODEL XL based on GPT-J is more abstractive and tries to introduce more factual content.\n\n"},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments.","formatted":"We thank Clarisse Simoes Ribeiro, Chunyuan Li, Zhu Zhang, Hao Cheng, and other members of the Microsoft Research Deep Learning team for valuable discussions and comments."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJVUfzSyrfPxgpM2wtb1xKmjJ5Tv","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."},"finish_reason":"stop"}],"usage":{"prompt_tokens":93,"completion_tokens":21,"total_tokens":114}}
"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."
{"id":"chatcmpl-7mJVUOSsyj5tTJUEIdQNUKN5QdIyl","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks."},"finish_reason":"stop"}],"usage":{"prompt_tokens":374,"completion_tokens":79,"total_tokens":453}}
"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks."
{"id":"chatcmpl-7mJVUO6rgn1tieVE0wQIyOQNtrv5B","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned."},"finish_reason":"stop"}],"usage":{"prompt_tokens":523,"completion_tokens":81,"total_tokens":604}}
"The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned."
{"id":"chatcmpl-7mJVUtjj8TbhZKuFbOoXYbIA9EoHe","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results."},"finish_reason":"stop"}],"usage":{"prompt_tokens":613,"completion_tokens":115,"total_tokens":728}}
"The text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results."
{"id":"chatcmpl-7mJVUTnKrV2zPUcAdeL13Trqhj4Uj","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5."},"finish_reason":"stop"}],"usage":{"prompt_tokens":231,"completion_tokens":90,"total_tokens":321}}
"The text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5."
{"id":"chatcmpl-7mJVUpEa3miBoBSCF07N6zRNeVqHP","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog."},"finish_reason":"stop"}],"usage":{"prompt_tokens":405,"completion_tokens":95,"total_tokens":500}}
"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog."
{"id":"chatcmpl-7mJVUsDWFx71HIfrx0rjr9dSTXfPm","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models."},"finish_reason":"stop"}],"usage":{"prompt_tokens":320,"completion_tokens":104,"total_tokens":424}}
"The text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models."
{"id":"chatcmpl-7mJVUaZAMzMcSVDHzcwnSGPYxY4ye","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation."},"finish_reason":"stop"}],"usage":{"prompt_tokens":323,"completion_tokens":107,"total_tokens":430}}
"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation."
{"id":"chatcmpl-7mJVUWz3ET230f0XiJnMOSndtPhp1","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks."},"finish_reason":"stop"}],"usage":{"prompt_tokens":358,"completion_tokens":108,"total_tokens":466}}
"The text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks."
{"id":"chatcmpl-7mJVU7CilPVhFpVyCJCXB6ThLNQtF","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"finish_reason":"stop"}],"usage":{"prompt_tokens":318,"completion_tokens":112,"total_tokens":430}}
"The text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor."
{"id":"chatcmpl-7mJVUaaFBP7R7olVsbCEVnZMLb6PE","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models."},"finish_reason":"stop"}],"usage":{"prompt_tokens":640,"completion_tokens":103,"total_tokens":743}}
"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models."
{"id":"chatcmpl-7mJVU8rLAcrHrCsEFxRu1bRPjPrxU","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions."},"finish_reason":"stop"}],"usage":{"prompt_tokens":578,"completion_tokens":133,"total_tokens":711}}
"The text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions."
{"id":"chatcmpl-7mJVU9dx47JJDBwFUeQ22r5ieO7S0","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models."},"finish_reason":"stop"}],"usage":{"prompt_tokens":723,"completion_tokens":160,"total_tokens":883}}
"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models."
{"id":"chatcmpl-7mJVU70PlUwhpEuXZKrh3S9WJKueU","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation."},"finish_reason":"stop"}],"usage":{"prompt_tokens":629,"completion_tokens":154,"total_tokens":783}}
"The text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation."
{"id":"chatcmpl-7mJVUOhVAsEoRhd1nw9WbvIS8QDQ4","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment."},"finish_reason":"stop"}],"usage":{"prompt_tokens":1101,"completion_tokens":157,"total_tokens":1258}}
"The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment."
{"id":"chatcmpl-7mJVUArRKJvg8irUfPkYG2yaTf1mg","object":"chat.completion","created":1691749084,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU."},"finish_reason":"stop"}],"usage":{"prompt_tokens":578,"completion_tokens":151,"total_tokens":729}}
"The text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU."
{"paragraphs":["The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.","The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.","The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment.","The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.","The text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.","The text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks.","The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.","The text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.","The text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor.","The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.","The text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.","The text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation.","The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.","The text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.","The text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5.","The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.","The text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.","The text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.","The text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.","The text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.","The text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.","The text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.","The text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.","The text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.","The text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.","The text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment.","formatted":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks.","formatted":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor.","formatted":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation.","formatted":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5.","formatted":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments.","formatted":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment.","formatted":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks.","formatted":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor.","formatted":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation.","formatted":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5.","formatted":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments.","formatted":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment.","formatted":"The text describes the development of GODEL, a large pretrained language model for dialog. GODEL is designed to support a wide range of dialog tasks that require external information. It outperforms other pre-trained dialog models in few-shot finetuning setups. The evaluation methodology includes assessing the usefulness of responses in addition to their communicative features. GODEL is pre-trained in three phases and validated on benchmarks for fine-tuning. It addresses the lack of robust automated evaluation criteria for open-ended conversation models.\n\nThe text discusses the importance of utility in a general-purpose dialog model and suggests that automated evaluation should focus on the model's functional utility. The paper explores the concept of utility by finetuning the GODEL model on various tasks, showing improved performance in goal-directed tasks. The release of three versions of GODEL and the availability of datasets and code for fine-tuning and evaluation are also mentioned.\n\nThe text discusses the impact of large pre-trained language models (PLMs) for dialog in Conversational AI. These models have been widely adopted and evaluated based on intrinsic measures such as relevance, informativeness, humanness, sensibleness, and specificity. However, there is a need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task. The notion of utility is defined as what appears useful to a user given the conversational context. The paper will analyze and contrast intrinsic and extrinsic qualities, including utility, and correlate them with automatic evaluation metrics. The open-domain goal-directed dialog generation task is defined as generating a target sentence given a source dialog context and an environment."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks.","formatted":"The text discusses the importance of the environment in a large language model and how it is substituted with a grounding text during pre-training. It also mentions that GODEL is a pre-trained LM using a sequence-to-sequence Transformer model. The model is trained in three phases, including linguistic pre-training and general-domain dialog pre-training. In the final phase, a corpus is used to support knowledge grounded response generation, conversational question-answering, and task-oriented dialog.\n\nThe text discusses different datasets and models used in the development of a new large language model. The DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog are mentioned as datasets that provide external knowledge for training the model. The model comes in three sizes: GODEL B, GODEL L, and GODEL XL, with varying numbers of parameters and dimensions. GODEL B and GODEL L are based on T5 and T5-Large, while GODEL XL is based on GPT-3. A substitute model pre-trained from GPT-J is released due to licensing restrictions.\n\nThe text discusses the training process and evaluation of GODEL language models. The models are trained on Nvidia V100 machines and are trained until there is no significant loss reduction. The best versions are selected based on the validation set. The evaluation focuses on few-shot fine-tuning, which is important due to the limited availability of goal-directed dialog datasets. Few-shot fine-tuning allows for faster model updates and greater control over model characteristics. GODEL can be used as an initial model for finetuning in various goal-directed dialog tasks."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor.","formatted":"The text discusses several evaluation datasets used for testing a large language model. These datasets include Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA. Each dataset focuses on different aspects of dialog and conversation, such as knowledge grounding, open-domain dialog, multi-domain dialog, and conversational question answering. The datasets provide valuable resources for evaluating the model's performance in various language tasks.\n\nThe text discusses the use of existing goal-oriented dialog corpora for fine-tuning a language model called GODEL. The model is fine-tuned on 50 randomly sampled dialogs for each task and evaluated using their original test sets. The best model is selected based on the perplexity score on the validation set. The text also compares GODEL with other pretrained models such as T5, BART, DialoGPT, and BlenderBot. Automatic evaluation metrics are used to assess the utility of the models.\n\nThe text discusses various metrics and evaluation methods used to measure the performance of a new large language model. These include F R 1 and F K 1 for measuring overlap between predictions and ground truth responses, Inform and Success for assessing the model's ability to provide sufficient information and appropriate answers, and the Combined score which combines Inform, Success, and BLEU. In addition, intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported. Significance is determined using a paired 2-sided t-test against the best competitor."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation.","formatted":"The text describes a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by GODEL, a language model. Master-level workers were hired through Amazon Mechanical Turk and presented with dialog history and related knowledge, along with two responses from different systems. The workers were asked to evaluate the responses based on their usefulness, human-like qualities, and safety using a 5-point Likert scale. The evaluation setup is not applicable to BlenderBot 2.0, which is designed for internet augmented generation.\n\nThe text discusses the performance of a new large language model called GODEL compared to other models such as T5, DialoGPT, and Blenderbot. It highlights that GODEL shows improvements over these models in various metrics, particularly in few-shot fine-tuning settings. The text also mentions that GODEL performs better than GPT-3 in goal-directed dialog models. Additionally, an ablation study of GODEL's pretraining stages is conducted, showing that dialog pre-training alone negatively affects performance, while incorporating grounded pre-training achieves the best results.\n\nThe text discusses the comparison between GODEL and T5, two language models, in terms of automatic evaluation and human preference. GODEL is initialized from T5, and human raters prefer GODEL over T5 in both Extrinsic and Intrinsic evaluations. The evaluation results show a strong preference for GODEL. Inter-annotator agreement is higher for Extrinsic evaluation compared to Intrinsic and Safety. The low agreement on CoQA data is attributed to the prevalence of short, factual responses. The correlation between human evaluation results and automated evaluation metrics is analyzed, considering metrics such as BLEU, BLEURT, BERTScore, and chrF. The study aims to determine which metrics are better suited for measuring different traits in open-ended response generation."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5.","formatted":"The text discusses the correlation between different evaluation metrics and their relationship to intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. The authors observe that metrics related to language modeling tend to correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. They hypothesize that this is because language modeling metrics focus more on fluency and well-formedness, while extrinsic evaluation requires factual accuracy. The authors also find that even safety, a measure of harmful or unsafe statements, correlates with automated metrics. They argue for adopting extrinsic evaluation for open-ended dialogue systems and suggest that a shift towards more extrinsic qualities could make automatic dialogue evaluation less challenging. They introduce their own large-scale pretrained model for dialogue (GODEL) and demonstrate its competitive performance against other models.\n\nThe text discusses the challenges of ensuring safe and inoffensive responses from dialog systems. The authors performed extensive filtering of social media data used to train GODEL, reducing the dataset size. However, forms of social bias and toxicity still persist in the training data, leading to the possibility of harmful responses. The authors acknowledge the need for further research to mitigate these issues. The text also presents evaluation results for different datasets, showing that GODEL B achieves the best scores in certain settings. GODEL XL has lower BLEU scores, possibly due to its decoder-only transformer architecture. Encoder-decoder transformer models like GODEL and GODEL L are more likely to copy words directly from the grounding, which is favored by metrics like BLEU.\n\nThe text discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version. It compares the results of GODEL GPT-3 XL and GODEL GPT-J XL on various tasks. Additionally, it mentions that GODEL XL is more abstractive and introduces more factual content compared to GODEL L, which is based on T5."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments.","formatted":"The authors express gratitude to several individuals from the Microsoft Research Deep Learning team for their valuable discussions and comments."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJVblagH7t2w9a7y7jzuhnYgW1gS","object":"chat.completion","created":1691749091,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."},"finish_reason":"stop"}],"usage":{"prompt_tokens":77,"completion_tokens":15,"total_tokens":92}}
"The authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."
{"id":"chatcmpl-7mJVbgdEXRpjCsJ55ACrG04peWMjV","object":"chat.completion","created":1691749091,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks."},"finish_reason":"stop"}],"usage":{"prompt_tokens":392,"completion_tokens":122,"total_tokens":514}}
"The text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks."
{"id":"chatcmpl-7mJVbkoqZ4w37OeR17svID4tO7MY5","object":"chat.completion","created":1691749091,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."},"finish_reason":"stop"}],"usage":{"prompt_tokens":457,"completion_tokens":132,"total_tokens":589}}
"The text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."
{"id":"chatcmpl-7mJVbspVRIAiqyMA5CLvY8reNMMey","object":"chat.completion","created":1691749091,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task."},"finish_reason":"stop"}],"usage":{"prompt_tokens":397,"completion_tokens":132,"total_tokens":529}}
"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task."
{"id":"chatcmpl-7mJVbCKGrQQC5nJN58SknTkJ7PTGg","object":"chat.completion","created":1691749091,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics."},"finish_reason":"stop"}],"usage":{"prompt_tokens":432,"completion_tokens":122,"total_tokens":554}}
"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics."
{"id":"chatcmpl-7mJVbiUl7vx5ixErrubAC3bxpweyK","object":"chat.completion","created":1691749091,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."},"finish_reason":"stop"}],"usage":{"prompt_tokens":351,"completion_tokens":144,"total_tokens":495}}
"The text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."
{"paragraphs":["The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.","The text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.","The text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.","The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.","The text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.","The authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.","The text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.","The text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"paragraphs":["The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.","The text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.","The authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.","formatted":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team.","formatted":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.","formatted":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team.","formatted":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.","formatted":"The text discusses the development of GODEL, a large pretrained language model for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The impact of large pre-trained language models for dialog in Conversational AI is also discussed, highlighting the need for a unified measure of utility in evaluating dialog models. The paper aims to generate human-like conversational responses with high utility and proposes a goal-directed framework for this task.\n\nThe text discusses a new large language model called GODEL, which is trained in three phases and uses a grounding text during pre-training. It mentions datasets such as DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog that provide external knowledge for training. The model comes in three sizes with varying parameters and dimensions. The training process involves Nvidia V100 machines and the evaluation focuses on few-shot fine-tuning, allowing for faster updates and control over model characteristics. GODEL can be used as an initial model for finetuning in goal-directed dialog tasks.\n\nThe text discusses evaluation datasets and methods used for testing a large language model. It mentions datasets like Wizard of Wikipedia, Wizard of Internet, MultiWOZ, and CoQA, which focus on different aspects of dialog and conversation. The text also discusses the fine-tuning of a language model called GODEL using goal-oriented dialog corpora and compares it with other pretrained models. Various metrics and evaluation methods, such as F R 1, F K 1, Inform, Success, and Combined score, are used to measure the model's performance. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team.","formatted":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics.\n\nThe text discusses the correlation between evaluation metrics and intrinsic and extrinsic evaluation in the context of a new large language model for dialogue. It highlights that language modeling metrics correlate better with intrinsic evaluation, while lexical metrics perform better in extrinsic evaluation. The authors argue for adopting extrinsic evaluation for open-ended dialogue systems and introduce their own large-scale pretrained model for dialogue (GODEL). The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.\n\nThe authors acknowledge the contributions of individuals from the Microsoft Research Deep Learning team."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJVh6jJU9NM2T1e5iKSObI2n0jtf","object":"chat.completion","created":1691749097,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."},"finish_reason":"stop"}],"usage":{"prompt_tokens":454,"completion_tokens":192,"total_tokens":646}}
"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test."
{"id":"chatcmpl-7mJVhMPOt7Q4cwHGleHRYU3n0bTbg","object":"chat.completion","created":1691749097,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."},"finish_reason":"stop"}],"usage":{"prompt_tokens":325,"completion_tokens":178,"total_tokens":503}}
"The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."
{"paragraphs":["The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.","The text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."],"summary_length":3,"context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"text":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.","context":"This is from a scientific articles about a new large language model","model":"chatgpt"}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.","formatted":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."},"\n\nSummary:"]},"stop":null,"verbose":false,"default":"","max_tokens":256}
{"self":{"class_name":"OpenAIChatCompletionAgent","model":"gpt-3.5-turbo","temperature":0.0,"top_p":1.0},"prompt":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.","formatted":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."},"\n\nSummary:"]},"stop":null,"max_tokens":256}
{"messages":[{"role":"user","content":{"__fstring__":["You are given the following context:\n\n",{"source":"context","value":"This is from a scientific articles about a new large language model","formatted":"This is from a scientific articles about a new large language model"},"\n\nSummarize the following text. If the text is not relevant to the context, you may skip it and return an empty string.\n\n",{"source":"text","value":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version.","formatted":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and addresses the lack of robust evaluation criteria for open-ended conversation models. The importance of utility in a dialog model is emphasized, and the paper explores the concept of utility by finetuning GODEL on various tasks. The model is trained in three phases and uses external knowledge from datasets like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog. GODEL comes in three sizes with varying parameters and dimensions. The evaluation involves few-shot fine-tuning and uses metrics like F R 1, F K 1, Inform, Success, and Combined score. Intrinsic evaluation results using BLEU, BLEURT, BERTScore, and chrF are reported, and significance is determined using a paired 2-sided t-test.\n\nThe text discusses a human evaluation setup conducted to assess the usefulness, human-like qualities, and safety of responses generated by a language model called GODEL. It compares GODEL to other models such as T5, DialoGPT, and Blenderbot, highlighting GODEL's improvements in various metrics. The text also discusses an ablation study of GODEL's pretraining stages and compares GODEL to T5 in terms of automatic evaluation and human preference. Human raters prefer GODEL over T5, and the study analyzes the correlation between human evaluation results and automated evaluation metrics. The text also addresses the challenges of ensuring safe and inoffensive responses from dialog systems and presents evaluation results for different datasets. It discusses the release of GODEL XL, a language model based on GPT-J, as a substitute for the unavailable GPT-3 version."},"\n\nSummary:"]}}],"stop":null,"top_p":1.0,"temperature":0.0,"model":"gpt-3.5-turbo","max_tokens":256,"logit_bias":null,"n":1,"cache_id":0}
{"id":"chatcmpl-7mJVp3XYVpVFyBwfsdqN7fUAcsfIY","object":"chat.completion","created":1691749105,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"message":{"role":"assistant","content":"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and emphasizes the importance of utility in a dialog model. The model is trained in three phases and uses external knowledge from various datasets. Evaluation involves metrics like F R 1, F K 1, Inform, Success, and Combined score. The text also discusses a human evaluation setup comparing GODEL to other models and analyzing the correlation between human and automated evaluation. It addresses the challenges of ensuring safe and inoffensive responses and mentions the release of GODEL XL as a substitute for GPT-3."},"finish_reason":"stop"}],"usage":{"prompt_tokens":426,"completion_tokens":138,"total_tokens":564}}
"The text discusses the development and evaluation of a new large language model called GODEL for dialog. GODEL outperforms other models in few-shot finetuning setups and emphasizes the importance of utility in a dialog model. The model is trained in three phases and uses external knowledge from various datasets. Evaluation involves metrics like F R 1, F K 1, Inform, Success, and Combined score. The text also discusses a human evaluation setup comparing GODEL to other models and analyzing the correlation between human and automated evaluation. It addresses the challenges of ensuring safe and inoffensive responses and mentions the release of GODEL XL as a substitute for GPT-3."

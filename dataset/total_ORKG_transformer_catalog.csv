"Title","has model","model family","pretraining architecture","pretraining task","fine-tuning task","extension","application","date created","number of parameters","maximum number of parameters (in million)","training corpus","organization","has source code","license","blog post","model family/bert/same as/bidirectional encoder representations from transformer/same as","pretraining task/denoising autoencoder/description","hardware used","hardware used/a100-80gb gpu/organization/nvidia/same as","tokenization","pretraining task/language modeling/acronym","research problem/large language models (llms)/same as/large language model/same as","research problem","pretraining task/language modeling/description"
"Improving Language Understanding by Generative Pre-Training","GPT","GPT","Decoder","Language Modeling","","","Text generation, but adaptable to many other NLP tasks when fine tuned.","2018-06-01","117M","117","Unsupervised Pretraining on BookCorpus dataset. Supervised Finetuning on several task-specific datasets including SNLI, RACE, Quora...","OpenAI","https://github.com/openai/finetune-transformer-lm, https://huggingface.co/docs/transformers/model_doc/openai-gpt","N/A","","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","BERT","BERT","Encoder","Next Sentence Prediction, Masked Language Modeling","","","General Language Understanding and Question Answering. Many other language applications followed","2018-10-01","Base = 110M, Large = 340M","340","Toronto Book Corpus and Wikipedia (3.3B Tokens)","Google","https://huggingface.co/docs/transformers/model_doc/bert","Open, Apache 2.0","https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb, https://www.philschmid.de/bert-text-classification-in-a-different-language","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context","Transformer XL","","Decoder","Language Modeling","","Relative positioned embeddings enable longer-context attention when compared to vanilla Transformer model","General language tasks","2019-01-01","151M","151","Different training datasets depending on experiments, but baseline is Wikitext-103","Google, CMU","https://huggingface.co/docs/transformers/model_doc/transfo-xl","N/A","","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Language models are unsupervised multitask learners","GPT-2","GPT","Decoder","Language Modeling","","Minor extensions to the GPT architecture (e.g. layer normalization moved to the input of each sub-layer, or increased context size from 512 to 1024)","Text generation, but adaptable to many other NLP tasks when fine tuned.","2019-02-01","124M, 355M, 774M, 1.5B","1500","8 million web pages (40 GB). 10X GPT . WebText dataset is created by crawling all links at Reddit with at least 3 Karma points.","OpenAI","https://huggingface.co/docs/transformers/model_doc/gpt2","Open, Modified MIT license","https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"XLNet: Generalized Autoregressive Pretraining for Language Understanding","XLNet","Transformer XL","Decoder","PLM","","This model basically adapts Transformer XL architecture to permutation-based LM","General language tasks","2019-05-01","Base=117M, Large=360M","360","Same as BERT + Giga5 (16GB text),
and and aggressively filtered ClueWeb 2012-B (19GB), Common Crawl (110 GB)","Google, CMU","https://huggingface.co/docs/transformers/model_doc/xlnet","Open, MIT license","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"ERNIE: Enhanced Language Representation with Informative Entities","ERNIE","BERT","Encoder","Masked Language Modeling","","Uses BERT for Encoder architecture, but stacks and aggregates
two of them for text and entities. This architecture could be
understood as BERT for text + knowledge graphs","Knowledge intensive related tasks that might benefit from
knowledge graphs or entities such as entity recognition","2019-05-01","Ernie-ViLG 2.0 = 10B, Ernie 3.0 Titan = 260B","260000","English Wikipedia + Wikidata for entitites (note that they
initialize model to original BERT parameter values","Pengcheng Lab, Baidu","https://github.com/thunlp/ERNIE","closed source","http://research.baidu.com/Blog/index-view?id=160","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"RoBERTa: A Robustly Optimized BERT Pretraining Approach","RoBERTa","BERT","Encoder","MLM (Dynamic)","","Extension of BERT with optimized training procedure and more data","Same as BERT","2019-07-01","125M Base, and 356M Large","356","Same as BERT + CC News + OpenWebText + Stories (~33B Tokens)","Google, University of Washington","https://github.com/facebookresearch/fairseq/tree/main/examples/roberta, https://huggingface.co/docs/transformers/model_doc/roberta","N/A","https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","ALBERT","BERT","Encoder","Next Sentence Prediction, Masked Language Modeling","","Compressed version of BERT using parameter sharing, which is much more efficient given the same number of parameters","Same as BERT","2019-09-01","Base = 12M, Large = 18M, XLarge = 60M","60","Same as BERT","Google","https://huggingface.co/docs/transformers/model_doc/albert","Open, Apache 2.0","","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"CTRL: A Conditional Transformer Language Model for Controllable Generation","CTRL","","Decoder","","","model can generate text conditioned on control codes that specify domain, style, topics, dates, entities, relationships between entities, plot points, and task-related behavior","Controllable text generation","2019-09-01","1.63B","1630","140 GB of text including: Wikipedia (En, De, Es, Fr), Project Gutenberg, 45 subreddits, OpenWebText2, Amazon Reviews, Europarl and UN data from WMT, question-answer pairs from ELI5, and the MRQA shared task3, which includes the Stanford Question Answering Dataset, NewsQA, TriviaQA, SearchQA, HotpotQA , and Natural Questions","Salesforce","https://github.com/salesforce/ctrl, https://huggingface.co/docs/transformers/model_doc/ctrl","Open, BSD-3-Clause license","https://blog.salesforceairesearch.com/introducing-a-conditional-transformer-language-model-for-controllable-generation/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Highly accurate protein structure prediction with AlphaFold","AlphaFold","SE(3)-Transformer","Encoder","Protein folding prediction of BERT using parameter
sharing, which is much more efficient given the same number of parameters","","The original Alphafold used a BERT-style transformer. The details of Alphafold’s Transformer are not known, but it is believed it is an extension of the SE(3)-Tranformer, a 3-D equivariant Transformer (see this blog post).","Protein folding","2019-09-01","b12M, Large = 18M, XLarge = 60M","60","170,000 proteins from a public repository of protein sequences and structures","Deepmind","https://github.com/deepmind/alphafold","the code is open sourced, with Apache-2.0","https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold, https://fabianfuchsml.github.io/alphafold2/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","BART","BERT for encoder, GPT for Decoder","Encoder/Decoder","denoising autoencoder","","It can be seen as a generalization of BERT and GPT in that it combines ideas from both in the encoder and decoder","Mostly text generation but also some text understanding tasks","2019-10-01","Base = 140M, Large = 400M. In general, roughly 10%
larger than BART for equivalent architectures","400","Same as RoBERTa (160Gb of news, books, stories, and web text)","Facebook","https://huggingface.co/docs/transformers/model_doc/bart","Open, Apache 2.0","","","take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with "[MASK]" elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation","DialoGPT","GPT","Decoder","Language Modeling","","GPT-2 architecture trained on dialog data","Text generation in dialog settings","2019-10-01","1.5B","1500","140M Reddit conversations","Microsoft","https://github.com/microsoft/DialoGPT, https://huggingface.co/docs/transformers/model_doc/dialogpt","Open, MIT license","https://huggingface.co/microsoft/DialoGPT-medium?text=Hey+my+name+is+Mariama%21+How+are+you%3F","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","DistilBERT","BERT","Encoder","Masked Language Modeling, Next Sentence Prediction","","Compressed version of BERT using distillation, which is much more efficient given the same number of parameters","Same as BERT","2019-10-01","66M","66","Same as BERT","Huggingface","https://huggingface.co/docs/transformers/model_doc/distilbert","Open, Apache 2.0","https://medium.com/huggingface/distilbert-8cf3380435b5","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Exploring the limits of transfer learning with a unified text-to-text transformer","T5","Transformer","Encoder/Decoder","denoising autoencoder","","Same as original Transformer with some additions such as relative positional embeddings like Transformer XL","General language tasks including machine translation, question answering, abstractive summarization, and text classification","2019-10-01","60M, 220M, 770M, 3B, and 11B","11000","Colossal Clean Crawled Corpus (C4) - Cleaned up version of the Common Crawl dataset - 750 GB","Google","https://github.com/google-research/text-to-text-transfer-transformer, https://huggingface.co/docs/transformers/model_doc/t5","Open, Apache 2.0","https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html","","take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with "[MASK]" elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Unsupervised Cross-lingual Representation Learning at Scale","XLM-RoBERTa","RoBERTa","Encoder","MLM (Dynamic)","","An extension of RoBERTa that introduces small parameter tuning insights in the context of multilingual applications","Translation and other cross-lingual language tasks","2019-10-01","Base = 270M Large = 550M","550","Cleaned Common Crawl in 100 languages","Facebook","https://huggingface.co/docs/transformers/model_doc/xlm-roberta","","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization","Pegasus","Transformer","Encoder/Decoder","DAE (more concretely GSG) and MLM","","Extends vanilla Transformer by using a different pretraining task (GSG: Gap Sentence Generation) that is better suited for summarization","abstractive text summarization","2019-12-01","Base = 223M Large = 568M","568","C4 (750GB) + HugeNews (3.8 TB)","Google, Imperial College London","https://huggingface.co/docs/transformers/model_doc/pegasus","N/A","https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Multilingual Denoising Pre-training for Neural Machine Translation","mBART","BART","Encoder/Decoder","denoising autoencoder","","Extends BART to multilingual capability","Translation","2020-01-01","Same as BART","","CC25 Corpus includes 25 monolingual corpuses in different languages. Largest corpuses are English (300 GB) and Russian (280GB)","Facebook","https://github.com/facebookresearch/fairseq/tree/main/examples/mbart, https://huggingface.co/docs/transformers/model_doc/mbart","Open, MIT license","","","take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with "[MASK]" elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","ELECTRA","BERT","Encoder","replaced token detection","","Applied new training techniques including Replaced Token
Detection","Same as BERT","2020-03-01","Small = 14M, Base = 110M, Large = 335M","335","Same as BERT except for Large with is same as XLNet","Google, Stanford","https://github.com/google-research/electra, https://huggingface.co/docs/transformers/model_doc/electra","Open, Apache 2.0","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","Megatron","T5, BERT, GPT","Encoder or Decorder, depending on the base model","Same as base model","","Megatron is a family of models that extend previously known architectures (namely GPT-2 and BERT originally, but also T5 more recently) by introducing model parallelism primitives. In the case of BERT, the authors also replace the next sentence prediction head with sentence order prediction and use whole word n-gram masking.","Same as base model","2020-03-01","8.3B (GPT-like), 3.9B (BERT-like)","8300","Original paper uses an aggregate dataset consisting of Wikipedia), CC-Stories), RealNews, and OpenWebtext","NVidia","https://github.com/NVIDIA/Megatron-LM","Limited, Non-commercial usage","https://huggingface.co/blog/megatron-training","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Language Models are Few-Shot Learners","GPT-3","GPT","Decoder","Language Modeling","","Same as GPT-2 with the only addition of alternating dense and locally banded sparse attention patterns, inspired by the Sparse Transformer","Initially text generation, but has over time been used for a large range of applications in areas such as code generation, but also image and audio generation","2020-05-01","175B","175","~ 500B tokens including CommonCrawl (410B), WebText2 (19B), Books1 (12B), Books2 (55B), and Wikipedia (3B)","OpenAI","https://platform.openai.com/docs/models/gpt-3-5, https://github.com/openai/gpt-3","closed source","https://openai.com/blog/gpt-3-apps","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Deberta: Decoding-enhanced bert with disentangled attention","DeBERTa","BERT","Encoder","Masked Language Modeling","","Separate positional embedding vector independent from the
content embedding using disentangled attention matrices for contents and
relative positions","Same as BERT","2020-06-01","134M (base), 384M (large), 750M (xlarge)","750","English Wikipedia, BookCorpus, OPENWEBTEXT and STORIES","Microsoft","https://huggingface.co/microsoft/deberta-v2-xxlarge, https://huggingface.co/microsoft/deberta-v2-xlarge, https://huggingface.co/microsoft/deberta-xlarge, https://huggingface.co/microsoft/deberta-large","Open, MIT license","https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Big Bird: Transformers for Longer Sequences","Big Bird","BERT","Encoder","Masked Language Modeling","","Big Bird can extend other architectures such as BERT, Pegasus, or RoBERTa by using a sparse attention mechanism that elminates the quadratic dependency thus making it more suitable for longer sequences","Particularly well suited for longer sequences, not only in text but also e.g. in genomics","2020-07-01","Depends on the overall architecture","","Books, CC-News, Stories and Wikipedia","Google","https://huggingface.co/docs/transformers/model_doc/big_bird","Open, Apache 2.0","https://huggingface.co/blog/big-bird","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","ViT","BERT","Encoder","image classification","","Extension of BERT architecture to train on patches of images","mage classification","2020-10-01","86M(Base) to 632M (Huge)","632","From standard Imagenet to JFT-300M (large inhouse dataset)","Google","https://huggingface.co/docs/transformers/model_doc/vit","N/A","","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Zero-Shot Text-to-Image Generation","DALL-E","GPT","Decoder","Caption prediction","","A differential variational auto-encoder is used to learn the visual codebook. The transformer is a variation of GPT-3","Text to image","2021-01-01","12B","12000","250 million text-images pairs from the internet","OpenAI","https://github.com/borisdayma/dalle-mini","N/A","https://openai.com/blog/dall-e/, https://ml.berkeley.edu/blog/posts/dalle2/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","Switch","T5","Encoder/Decoder","denoising autoencoder","","Goal to increase parameter count while keeping FLOP operations constant by using efficient routing of MoE (Mixture of Experts)","General language tasks (e.g. question answering)","2021-01-01","1T","1000000","Colossal Clean Crawled Corpus","Google","https://github.com/google-research/t5x, https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py","Open, Apache 2.0","https://www.alexanderthamm.com/en/blog/switch-transformer-upscaling-to-over-a-billion-parameters/","","take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with "[MASK]" elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Learning Transferable Visual Models From Natural Language Supervision","CLIP","Also using Resnet, ViT, and vanilla transformer for text, CLIP","Encoder","predict which of the N × N possible (image, text) pairings across a batch actually occurred","","Combines Resnet and ViT for the visual encoding with Transformer for the Textual encoder","Image/Object classification","2021-02-01","","","WIT (WebImageText) - 400 million text,image pairs","OpenAI","https://huggingface.co/docs/transformers/model_doc/clip","Open, MIT license","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow","GPT-Neo","GPT","Decoder","Language Modeling","","Similar to GPT-2 but uses local attention in every other layer with a window size of 256 tokens","Text generation, but adaptable to many other NLP tasks when fine tuned.","2021-03-01","125M, 350M, 1.3B, and 2.7B","2700","Pile - 840 GB open source text dataset that combines 22 pre existing datasets","EleutherAI","https://github.com/EleutherAI/gpt-neo, https://huggingface.co/docs/transformers/model_doc/gpt_neo","Open, MIT license","https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api, https://www.section.io/engineering-education/leveraging-gptneo-to-generate-ai-based-blog-content/","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows","Swin Transformer","ViT","Encoder","Same as ViT","","Extends ViT by replacing the standard multi-head self attention (MSA) module by a module based on shifted windows (Swin) allowing ViT-like architectures to generalize to higher resolution images","Image (object detection, image classification..)","2021-03-01","29M-197M","197","Imagenet and Imagenet-22k","Microsoft","https://github.com/microsoft/Swin-Transformer","Open, MIT license","https://www.section.io/engineering-education/an-overview-of-swin-transformer/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"GPT-J-6B: A 6 billion parameter autoregressive language model","GPT-J","GPT","Decoder","Language Modeling","","GPT-J 6B is a Transformer model trained using Mesh Transformer
JAX and same tokenizer as GPT2/3","generating text from a prompt, but is advised to be finetuned for more effective performance","2021-05-01","6B","6000","Pile corpus, a large-scale curated dataset created by EleutherAI","EleutherAI","https://huggingface.co/EleutherAI/gpt-j-6b, https://github.com/kingoflolz/mesh-transformer-jax","Apache 2.0","https://en.wikipedia.org/wiki/GPT-J","","","TPU v3-256 pod","","same set of Byte Pair Encoding (BPE) Tokenizer as GPT-2/GPT-3","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Decision Transformer: Reinforcement Learning via Sequence Modeling","Decision Transformers","GPT, Control Transformers” (not per se a family, but grouping here those transformers that try to model more general control, RL-like, tasks)","Decoder","Next action prediction","","Decision transformers use a GPT architecture and extend it by encoding trajectories in a way that they can be learned by an auto-regressive task","General RL (reinforcement learning tasks)","2021-06-01","Same as GPT","","Different corpus for different experiments","Facebook, Google, UC Berkeley","https://github.com/kzl/decision-transformer, https://huggingface.co/docs/transformers/main/en/model_doc/decision_transformer","Open, MIT license","https://sites.google.com/berkeley.edu/decision-transformer","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Offline Reinforcement Learning as One Big Sequence Modeling Problem","Trajectory Transformers","GPT, Control Transformers” (not per se a family, but grouping here those transformers that try to model more general control, RL-like, tasks)","Decoder","predict most likely sequence","","Similarly to the Decision transformers, the main extension introduced by Trajectory Transformers is a way to encode a trajectory (state, actions, rewards)","General RL (reinforcement learning tasks)","2021-06-01","Smaller architecture than GPT","","D4RL dataset and other RL datasets depending on the task at hand","UC Berkeley","https://trajectory-transformer.github.io/, https://github.com/JannerM/trajectory-transformer","Open, MIT license","https://bair.berkeley.edu/blog/2021/11/19/trajectory-transformer/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"HTLM: Hyper-Text Pre-Training and Prompting of Language Models","HTLM","BART","Encoder/Decoder","denoising autoencoder","","As opposed to BART, they don’t do sentence shuffling","General purpose language model that allows structured HTML prompting","2021-07-01","400M","400","23TB of simplified HTML extracted from CommonCrawl","Facebook","","N/A","","","take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with "[MASK]" elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Jurassic-1: Technical details and evaluation","Jurassic-1","GPT","Decoder","Language Modeling","","Very similar to GPT-3, but far more parameters and improved training efficiency mostly because of the improved tokenizer. Also, different ratio of depth to breadth","Similar to GPT-3","2021-09-01","178B (Jumbo), 17B (Grande), 7.5B (Large)","178000","300B tokens (same as GPT-3)","AI21","https://github.com/ai21labs/lm-evaluation","Closed source, accessible through API","https://www.ai21.com/blog/ai21-studio-use-cases, https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","Megatron-Turing NLG","GPT","Decoder","Language Modeling","","Uses parallelization similar to Megatron to train a LM double the size of GPT-3","Language generation and others (similar to GPT-3)","2021-10-01","530B","530000","The Pile (800GB dataset) + 2 Common Crawl snapshots","NVidia","","Limited, Non-commercial usage","https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"A General Language Assistant as a Laboratory for Alignment","Anthropic Assistant","Transformer","Decoder","Language Modeling","","These models do not introduce novelties at the architecture/pretraining level and they are based on GPT-3 but rather focuses on how to improve alignment through fine-tuning and prompting. Note that the Anthropic Assistant includes several models optimized for different tasks. Latest versions of this work focus on the benefits of RLHF.","Different models with different applications from general dialog to code assistant.","2021-12-01","10M to 52B","52000","400B tokens from filtered Common Crawl and Books. They also create several Dialogue Preference datasets for the RLHF training.","Anthropic","","N/A","https://arxiv.org/abs/2204.05862, https://arxiv.org/abs/2112.00861","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts","GLaM","Transformer","Decoder","Language Modeling","","GLaM introduces a Mixture of 64 Experts to increase parameter count and generalization properties in a somewhat standard decoder-only. Transformer architecture. Only two experts get activated at a time per token, which makes the model also more efficient in training and inference.","General language modeling - tested across 29 NLP tasks","2021-12-01","1.2T across 64 experts, but only 96B get activated for inference","1200000","1.6T tokens including web pages filtered by Wikipedia and books for quality","Google","","closed source","https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models","GLIDE","Diffusion models","Encoder","Caption prediction","","GLIDE can be seen as an extension of the ADM (Ablated Diffusion Model) by the same authors. However, ADM is not per se a transformer architecture although it does resemble one in some of the configurations the authors use. Given that ADM is by the same authors and was quickly followed up by GLIDE, I think it is fair to consider GLIDE as the first of its kind.","Text to image","2021-12-01","3.5B diffusion model (2.3B for visual encoding, 1.2B for textual) + 1.5B for model for upsampling","3500","Same as DALL-E","OpenAI","https://github.com/openai/glide-text2im","Open, MIT license","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher","Gopher","GPT","Decoder","Language Modeling","","Same as GPT-2 but use RSNorm instead of LayerNorm and relative positional encoding rather than absolute","Mostly Language Modeling and NLU, but also extensible like GPT","2021-12-01","280B","280000","Massive Text (2.35 billion documents, or about 10.5 TB of text including Massive Web, Books, Github, News, C4, and Wikipedia.","Deepmind","","closed source","https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"High-Resolution Image Synthesis with Latent Diffusion Models","StableDiffusion","Diffusion","Encoder/Decoder","Caption prediction","","Stable diffusion is basically the Latent Diffusion model developed by LMU Munich researchers + some learnings on conditional diffusion from DALL-e and Imagen","Text to image","2021-12-01","890M (although there are different, smaller, variants)","890","LAION-5B, a publicly available dataset derived from Common Crawl","EleutherAI, Stability.ai, LMU Munich","https://huggingface.co/CompVis/stable-diffusion, https://huggingface.co/spaces/stabilityai/stable-diffusion, https://github.com/Stability-AI/stablediffusion","open, CreativeML Open RAIL++-M License","https://stability.ai/blog/stable-diffusion-public-release","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"CM3: A Causal Masked Multimodal Model of the Internet","CM3","HTLM","Decoder","Causality-masked LMs","","This is somewhat similar to HTML in its use of structured
training data. However, it is a different architecture and uses causal
masking, which makes the model predict, at the end of the sequence,
an entire missing span of text. It also includes image input via Vector
Quantized Variational Autoencoding (VQ-VAE) tokens.","Multimodal language model with the ability to do structured
prompting, zero-shot captioning, image generation, and entity linking (via
target text prediction of hyperlinks)","2022-01-01","125M (small), 800M (small), 2.7B (medium) and 13B (large)","13000","CC-News, English Wikipedia","Facebook","","N/A","https://lilianweng.github.io/posts/2022-06-09-vlm/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"LaMDA: Language Models for Dialog Applications","LAMDA","Transformer","Decoder","Language Modeling","","LAMDA focuses on how to improve safety, quality, and groundeness using different fine-tuning strategies","General language modeling, such as translation, summarization,
question and answers","2022-01-01","137B","137000","1.56T words from public dialog data and other public web documents","Google","","closed source","https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html, https://blog.google/technology/ai/lamda/","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Training language models to follow instructions with human feedback","InstructGPT","GPT","Decoder","Language Modeling","","GPTInstruct starts off with a pretrained GPT3 model and adds reward modeling through reinforcement learning after a supervised finetuning","Knowledge-intensive dialog or language tasks","2022-01-01","Same as GPT3","","Same as GPT3 for pretraining, but finetuned and optimized using labeler data and prompts","OpenAI","https://github.com/openai/following-instructions-human-feedback","Closed source, accessible through API","https://sh-tsang.medium.com/review-instructgpt-training-language-models-to-follow-instructions-with-human-feedback-7fce4bf9059a, https://openai.com/research/instruction-following","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Finetuned language models are zero-shot learners","Flan","LaMDA-PT","Decoder","Instruction Tuning","","Zero-shot task learning. The output space for a given task is either one of several classes (classification) or free text (generation).","natural language comprehension tasks such as inference, sentiment analysis, paraphrase, closed-book QA, reading comprehension, coreference, summarization, translation, commonsense reasoning, and struct-to-text","2022-02-08","137B","137000","FLAN is instruction tuned on 25 tasks spanning 62 datasets., LaMDA-PT is is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary","Google","https://github.com/google-research/FLAN","","http://rylanschaeffer.github.io/blog_posts/2022-01-20-google-brain-flan.html, https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Training Compute-Optimal Large Language Models","Chinchilla","GPT","Decoder","Language Modeling","","Same as Gopher but with optimizations to reduce model size and therefore training/inference time with equal or superior performance","Same as Gopher/GPT3","2022-03-01","70B","70000","1.4 trillion training tokens. Massive Text (2.35 billion documents, or about 10.5 TB of text including Massive Web, Books, Github, News, C4, and Wikipedia.","Deepmind","","closed source","https://medium.com/mlearning-ai/language-models-need-proper-training-c71484727f00","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization","DQ-BART","BART","Encoder/Decoder","denoising autoencoder","","Adds quantization and distillation to a BART model to improve performance and model size","Text generation and understanding","2022-03-01","Up to 30x reduction in parameters compared to standard BART","","CNN/DM, XSUM, ELI5, WMT16 En-Ro (~1M tokens)","Amazon","https://github.com/amazon-science/dq-bart","Open, Apache 2.0","https://www.amazon.science/publications/dq-bart-efficient-sequence-to-sequence-model-via-joint-distillation-and-quantization","","take a partially corrupted input (e.g. Randomly sampling tokens from the input and replacing them with "[MASK]" elements. randomly deleting tokens from the input, or shuffling sentences in random order) and aim to recover the original undistorted input.","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Teaching language models to support answers with verified quotes","GopherCite","Gopher","Decoder","Language Modeling","","GopherCite is based on Gopher but adds a step using RLHP (Reinforcement Learning from Human Preferences) to learn whether not only a response is plausible but also supported","Dialog systems, Q&A, general language generation tasks","2022-03-01","280B","280000","Same as Gopher plus specific dataset generated in the RLHP process","Deepmind","","closed source","https://www.deepmind.com/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion","SeeKer","GPT (but can extend any family)","Encoder/decoder or decoder only, depending on the base model it’s extending","LM training, Dialogue training","","SeeKer is an extension that can be applied to any Transformer architecture by introducing “search”, “knowledge”, and “response” modules that are introduced during pretraining","Same as base models","2022-03-01","SeeKeR Dialogue: 400M, 3B; SeeKeR LM: 365M, 762M,
1.5B, R2C2 BlenderBot: 400M, 3B","","Wizard of the Internet/Wikipedia, PersonaChat, Blended Skill
Talk, Empatheic Dialogues, Multi-Session Chat, MS MARCO, Natural
questions, SQuAD, TriviaQA","Facebook","https://parl.ai/projects/seeker/","the code is open sourced","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"GLM: General language model pretraining with autoregressive blank infilling","GLM","GLM (General Language Model)","Encoder/Decoder","Auto regressive blank infilling","","GLM has a bidirectional encoder and a unidirectional decoder in a unified model","a General Language Model pretrained with an autoregressive
blank-filling objective and can be finetuned on various natural language
understanding and generation tasks.","2022-03-01","Base = 110M, Large = 335M, and also 2B, 10B, 130B","130000","Pile, GLM-130B Chinese corpora, P3, DeepStruct finetuning
dataset","Tsinghua University","https://github.com/THUDM/GLM-130B","Open, MIT license","http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Multitask prompted training enables zero-shot task generalization","T0","T5","Encoder/Decoder","","Natural language prompts","T0 stands for "T5 for Zero Shot", obtained by fine-tuning
the T5 model on multitask mixture covering many different NLP tasks.
Compared with T0, T0p and T0pp were fine-tuned with more datasets.
T0pp is recommended as it leads (on average) to the best performances on
a variety of NLP tasks.","Perform zero-shot inference tasks by specifying the query
in natural language, and the models will generate a prediction.","2022-03-01","T0-3B: 3 billion, T0, T0p, T0pp: 11 billion","11000","T0 (Multiple-choice QA, Extractive QA, Closed-Book QA,
Structure-To-Text, Sentiment, Summarization, Topic Classification, Paraphrase
Identification. T0p (same as T0, with additional datasets from
GPT-3’s evaluation suite). T0pp (same as T0p, with additional datasets
from SuperGLUE, excluding NLI sets)","BigScience","https://huggingface.co/bigscience/T0","Open, Apache 2.0","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Hierarchical Text-Conditional Image Generation with CLIP Latents","DALL-E 2","GLIDE, CLIP","Encoder/Decoder","Caption prediction","","Combines CLIP encoder and Diffusion decoder similar to GLIDE","Text to image","2022-04-01","3.5B","3500","Combination of the DALL-E and CLIP datasets","OpenAI","","Closed source, accessible through API","https://openai.com/product/dall-e-2, https://labs.openai.com/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Flamingo: a Visual Language Model for Few-Shot Learning","Flamingo","Chinchilla","Decoder","Log likelihood of text given some visual input","","It uses a frozen textual language model (like Chinchilla) conditioned on the visual representation, which is encoded from a Normalizer-Free ResNet","Text to image","2022-04-01","80B (largest)","80000","MultiModal MassiveWeb (M3W): 185 million images and 182 GB text + a number of text paired with image datasets: ALIGN + LTIP (Long Text & Image Pairs) = 312 million images, and VTP (Video & Text Pairs) = 27 million short videos (approximately 22 seconds on average)","Deepmind","","closed source","https://medium.com/geekculture/3-overlooked-things-deepminds-flamingo-a-large-model-for-computer-vision-84cd9d2f738c, https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"PaLM: Scaling Language Modeling with Pathways","PaLM","Transformer","Decoder","Language Modeling","","Palm uses a typical decoder-only transformer architecture, but adds quite a few extensions: SwiGLU activations, parallel layers, multi-query attention, RoPE embeddings, Shared Input-Output Embeddings, no biases, and a 256k SentencePiece vocabulary generated from the training data","PalM is designed as a general purpose language model with
applicability to hundreds of different language tasks","2022-04-01","8B, 62B, and 540B","540000","780B tokens from filtered webpages, books, Wikipedia, news articles, source code, and social media conversations. Code includes 24 programming languages.","Google","https://github.com/lucidrains/PaLM-pytorch","","https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/, https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","GPT-NeoX-20B","GPT","Decoder","Language Modeling","","Similar to GPT-3 with rotary encoders instead of positional, parallel attention and feed forward layers, different initialization, and all dense layers instead of alternate dense/sparse","range of language-understanding, mathematics and knowledge-based tasks","2022-04-01","20B","20000","Pile — 840 GB open source text dataset that combines 22 preexisting datasets","EleutherAI","https://github.com/EleutherAI/gpt-neox, other gpt-neo models, https://huggingface.co/EleutherAI/gpt-neox-20b","Apache 2.0","https://blog.eleuther.ai/announcing-20b/","","","AMD EPYC 7532 CPU, A100-SXM4-40GB GPU","","train a new BPE tokenizer based on the Pile","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"A Generalist Agent","Gato","“Control Transformers” (not per se a family, but grouping here those transformers that try to model more general control, RL-like, tasks)","Decoder","MLM (where tokens are either text or agent actions)","","The standard decoder-only transformer architecture is preceded by an embedding layer that can embed text and images, plus add position encodings to add spatial information when applicable.","Gato presents a generalizable agent that can be used beyond text to tasks such as playing Atari or controlling a robot arm.","2022-05-01","79M, 364M, and 1.18B","1180","1.5T tokens including standard text (e.g. MassiveText), vision (e.g. ALIGN), and simulation environments (e.g. ALE Atari, or RGB Stacking Real Robot)","Deepmind","https://github.com/OrigamiDream/gato","closed source","https://www.deepmind.com/blog/a-generalist-agent, https://www.deepmind.com/publications/a-generalist-agent","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"OPT: Open Pre-trained Transformer Language Models","OPT","GPT","Decoder","Language Modeling","","Basically same architecture as GPT-3 but with some training improvements introduced in Megatron-LM","Same as GPT-3","2022-05-01","125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B, and 175B","175000","180B tokens = RoBERTa + the Pile + PushShift.io Reddit","Facebook","https://github.com/facebookresearch/metaseq, https://huggingface.co/facebook/opt-350m","","https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/","","","A100-80GB GPU","https://www.wikidata.org/entity/Q182477","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Opt: Open pre-trained transformer language models","OPT","GPT","Decoder","Language Modeling","","Basically same architecture as GPT-3 but with some training
improvements introduced in Megatron-LM","Same as GPT-3","2022-05-01","175B (and other smaller versions)","175000","180B tokens = RoBERTa + the Pile + PushShift.io Reddit","Facebook","https://github.com/facebookresearch/metaseq","Limited, non-commercial license","https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Ul2: Unifying language learning paradigms","UL2","Transformer","Encoder/Decoder","Mixture-of-Denoisers, which combines diverse pretraining
paradigms together","","UL2-20B (Unifying Language Learning) can be interpreted as
a model that is quite similar to T5 but trained with a different objective
and slightly different scaling knobs.","A unified framework for pre-training models that are universally
effective across datasets and setups.","2022-05-01","20B","20000","1 trillion tokens on C4","Google","https://github.com/google-research/google-research/tree/master/ul2","Open, Apache 2.0","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Global Context Vision Transformers","Global Context ViT","ViT","Encoder","Image Classification","","hierarchical ViT architecture consisting of local and global self-attention modules","image generation","2022-06-01","90M","90","Imagenet-1K and other task dependent dataasets","NVidia","https://github.com/NVlabs/GCVit","Limited, non-commercial license CC-BY-NC-SA-4.0","https://towardsdatascience.com/global-context-vision-transformers-nvidias-new-sota-image-model-2923bdaf438e","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding","Imagen","Diffusion models, CLIP, T5","T5 (or CLIP or BERT) for frozen text encoder + U-net architecture for cascaded diffusion models for text to image","image/text pair prediction","","Imagen adds a few extensions to the U-net diffusion architecture (pooled embedding vector, cross attention over text embeddings, and Layer Normalizations)","Text to image","2022-06-01","2B","2000","a combination of internal datasets, with ? 460M image-text pairs, and the publicly available Laion dataset, with ? 400M image-text pairs","Google","","closed source","https://imagen.research.google/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Solving Quantitative Reasoning Problems with Language Models","Minerva","PaLM","Decoder","Language Modeling","","Extends PaLM by fine-tuning on the mathematical dataset","Mathematical reasoning","2022-06-01","540B","540000","Same as PaLM + 118GB dataset of scientific papers from the arXiv preprint server and web pages that contain mathematical expressions using LaTeX, MathJax, or other mathematical typesetting formats","Google","","closed source","https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Godel: Large-scale pre-training for goal-directed dialog","Godel","T5, GPT","Decoder","Language Modeling","","In contrast
with earlier models such as DialoGPT,
GODEL leverages a new phase of grounded
pre-training designed to better support adapting
GODEL to a wide range of downstream
dialog tasks that require information external
to the current conversation (e.g., a database
or document) to produce good responses.","open-domain goal-directed dialog tasks such as knowledge-grounded response generation, task-oriented dialog, and conversational QA","2022-06-01","220M (base), 770M (large), and 175B (XL)","175000","147M dialog sessions for a total of 6B tokens from Reddit comment chains
for DialoGPT. And grounded dialog corpora like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog.","Microsoft","https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq?text=Hey+my+name+is+Mariama%21+How+are+you%3F, https://huggingface.co/microsoft/GODEL-v1_1-base-seq2seq?text=Hey+my+name+is+Julien%21+How+are+you%3F, https://github.com/microsoft/GODEL","","https://www.microsoft.com/en-us/research/blog/godel-combining-goal-oriented-dialog-with-real-world-conversations/","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","BLOOM","GPT","Decoder","Language Modeling","","Main difference to GPT-3 is that it uses full attention instead of sparse attention","Same as GPT-3","2022-07-01","560m, 1.1B, 1.7B, 3B, 7.1B, and 176B","176000","https://openreview.net/forum?id=UoEw6KigkUn, 366B tokens (1.5 TB of text data) multilingual dataset (46 natural languages and 13 programming languages)","Huggingface, Big Science","https://huggingface.co/docs/transformers/model_doc/bloom","Open, but need to follow restrictions in Attachment A, BigScience
RAIL License v1.0","https://huggingface.co/blog/bloom-megatron-deepspeed, https://huggingface.co/blog/bloom-inference-pytorch-scripts, https://huggingface.co/blog/bloom-inference-optimization","","","A100-80GB GPU","https://www.wikidata.org/entity/Q182477","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage","BlenderBot 3","GPT","Decoder","Language Modeling","","BlenderBot 3 is based on a pre-trained OPT. It adds features needed for a dialog agent such as long-term memory or the ability to search the internet. It is also fine-tuned for some specific tasks given human feedback on them.","same as GPT-3","2022-08-01","3B, 30B and 175B","175000","180B tokens = RoBERTa + the Pile + PushShift.io Reddit","Facebook","https://parl.ai/projects/bb3/, https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md, https://github.com/facebookresearch/ParlAI/blob/main/projects/bb3/agents/README.md","Limited, non-commercial, research only","https://ai.facebook.com/blog/blenderbot-3-a-175b-parameter-publicly-available-chatbot-that-improves-its-skills-and-safety-over-time/","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model","AlexaTM 20B","transformer","Encoder/Decoder","Optimizes denoising (80%) and Prefix LM (20%)","","Derived from BART and layernorms located exactly at the beginning of each layer. Encoder initialized with internal 10B pre-trained
encoder.","Summarization, multi-lingual machine translation and NLU tasks","2022-08-01","20B","20000","Wikipedia and mC4 datasets in 12 languages.","Amazon","https://github.com/amazon-science/alexa-teacher-models","Limited, non-commercial","https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Improving alignment of dialogue agents via targeted human judgements","Sparrow","GPT","Decoder","Language Modeling","","Starts from the Chinchilla 70B model but adds RLHF (Reinforcement Learning with Human Feedback). It also adds inline evidence a la GopherCite","Dialog agents and general language generation applications like Q&A","2022-09-01","70B","70000","Same as Chinchilla + interactive data gathering with human annotators during the RLHF process","Deepmind","","closed source","https://medium.com/to-cut-a-long-paper-short/sparrow-improving-alignment-of-dialogue-agents-via-targeted-human-judgments-e0876402d800","","","","","","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Scaling instruction-finetuned language models","Flan-T5","T5","Encoder/Decoder","Instruction Tuning","","instruction finetuning
with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on
chain-of-thought data","The primary use is to underestand how to improve large
language models with the right kind of instruction fine-tuning. The focus is
research on zero-shot and in-context few-shot learning NLP tasks, such as
reasoning, and question answering; advancing fairness and safety research,
and understanding limitations of current large language models","2022-11-01","80M (Flan-T5-Small), 250M (Flan-T5-Base), 780M (FLan-T5-Large), 3B (Flan-T5-XL), and 11B (Flan-T5-XXL).","11000","Flan finetuned with tasks in Muffin, T0-SF, NIV2, and CoT","Google","https://github.com/google-research/t5x, https://huggingface.co/docs/transformers/model_doc/flan-t5","Open, Apache 2.0","https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Scaling instruction-finetuned language models","Flan-PaLM","PaLM","Decoder","","Instructions for zero-shot and few-shot tasks","Flan-PaLM is generated by "Flan Finetuning" the PaLM
models: (1) scaling the number of tasks to 1,836, (2) scaling the model
size, and (3) finetuning on chain-of-thought data.","Same as Flan-T5. The goal is to show Flan finetuning can
even improve on the largest Google LMs (+9.4% improvement average
across tasks), with improvements to chain of thought, self consistency,
multilingual tasks, arithmetic reasoning","2022-11-01","8B, 62B, 540B","540000","Flan finetuned with tasks in Muffin, T0-SF, NIV2, and CoT","Google","","closed source","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Galactica: A large language model for science","Galactica","transformer","Decoder","Language Modeling for scientific domain","","Transformer based architecture in a decoder-only setup with
a few modifications. Data extensions include special tokens for working
memory, citations, genetic data, and a few other biology related tasks.","The models are designed to perform scientific tasks, including
but not limited to citation prediction, scientific QA, mathematical
reasoning, summarization, document generation, molecular property prediction
and entity extraction.","2022-11-01","mini: 125M, base: 1.3B, standard: 6.7B, large: 30B,
huge: 120B","120000","Trained on 106 billion tokens of open-access scientific text and
data. This includes papers, textbooks, scientific websites, encyclopedias,
reference material, knowledge bases, and more","Meta","","Limited, non-commerical CC BY-NC 4.0 license","https://galactica.org/","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Text Embeddings by Weakly-Supervised Contrastive Pre-training","E5","BERT","Encoder","","Semantic similarity using contrastive loss","Fine-tunes BERT-based models to create text string embeddings
optimized for semantic relatedness","Text embeddings for semantic relatedness tasks such as text
clustering or search retrieval","2022-12-01","300M","300","MS-MARCO, NQ, NLI","Microsoft","https://huggingface.co/intfloat/e5-large","Open, MIT license","","https://www.wikidata.org/entity/Q61726893","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings","InstructOR","T5","Encoder/Decoder","","Wide variety of instruction based text-to-text tasks","Fine-tunes T5 explicitly to optimize encoder to produce a
general purpose text string embedding useful for many NLU tasks.","Any NLU task requiring a single text string embedding. As
of April 2023 InstructOR is the top-ranked system on the Massive Text
Embedding Benchmark (MTEB).","2022-12-01","330M","330","Finetuned on MEDI","Meta AI, University of Washington, University of Hong Kong","https://huggingface.co/hkunlp/instructor-xl","Open, Apache 2.0","","","","","","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"LLaMA: Open and Efficient Foundation Language Models","LLaMA","LLaMa","Decoder","Language Modeling","instructions","LLaMA uses a Transformer architecture, and with extensions:
Pre-normalization, SwiGLU activations, RoPE embeddings, reduced
memory usage and runtime through efficient implementation of the causal
multi-head attention, checkpointing to reduce the amount of activations
that are recomputed during the backward pass, model and sequence parallelism
to reduce memory usage of the model, and uses 1.4T BPE tokens
after tokenization.","Zero and few shot Commonsense reasoning, Question answering, mathematical reasoning,
Code generation, Reading comprehension, and multitask understanding.","2023-02-27","6.7B, 13.0B, 32.5B, and 65.2B","65200","Stack Exchange, ArXiv, Gutenberg and Books3, Wikipedia, Github, C4, CommonCrawl, approximately 1.4T tokens from various sources: 3.3 TB CommonCrawl (67%), 783GB C4 (15%), 328BG Github (4.5%), 83GB Wikipedia (4.5%), 85GB Books (4.5%), 92GB ArXiv (2.5%), and 78GB StackExchange (2.0%)","Meta AI","https://huggingface.co/docs/transformers/main/model_doc/llama, https://github.com/facebookresearch/llama","Limited, Non-commercial bespoke license","https://ai.facebook.com/blog/large-language-model-llama-meta-ai/","","","A100-80GB GPU","https://www.wikidata.org/entity/Q182477","the bytepair encoding (BPE) algorithm, using the implementation from Sentence-Piece. Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters.","LM","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
"Alpaca: A strong, replicable instruction-following model","Alpaca","LLaMa","Decoder","","52K instruction-following data generated from OpenAI’s text-davinci-003 using self-instruct mechanism, from 175 human-written instruction-output pairs.","Alpaca is fine-tuned from a 7B LLaMA model","Evaluated on a variety of text generation and classification
tasks.","2023-03-01","7B, 13B, 33B, 65B","65000","","Stanford University","https://github.com/tatsu-lab/stanford_alpaca","Limited, non-commercial license CC-BY-NC-SA-4.0","https://medium.com/version-1/stanford-alpaca-a-small-yet-mighty-language-model-for-instruction-following-tasks-af9e92e87d9a","","","A100-80GB GPU","https://www.wikidata.org/entity/Q182477","","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
"Pythia: A suite for analyzing large language models across training and scaling","Pythia","Pythia","Decoder","","","Trained with the library GPT-NeoX","Research on language model’s behavior, functionality, and
limitations","2023-03-13","70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B","12000","Two version of each of the eight models are released: one trained on the original Pile corpus and another trained on the deduplicated Pile corpus., Pile","EleutherAI","pythia-deduped, pythia, https://github.com/EleutherAI/pythia","Apache 2.0","https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling","","","A100-40GB GPU","","BPE tokenizer that is trained specifically on the Pile same as used for GPT-NeoX-20B","","https://www.wikidata.org/entity/Q115305900","Large Language Models (LLMs), transformer model",""
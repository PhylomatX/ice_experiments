Paper,Title,has model,model family,pretraining architecture,pretraining task,fine-tuning task,extension,application,date created,number of parameters,maximum number of parameters (in million),training corpus,organization,has source code,license,blog post,model family/bert/same as/bidirectional encoder representations from transformer/same as,pretraining task/denoising autoencoder/description,hardware used,hardware used/a100-80gb gpu/organization/nvidia/same as,tokenization,pretraining task/language modeling/acronym,research problem/large language models (llms)/same as/large language model/same as,research problem,pretraining task/language modeling/description
2019_Dai,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,Transformer XL,,Decoder,Language Modeling,,Relative positioned embeddings enable longer-context attention when compared to vanilla Transformer model,General language tasks,2019-01-01,151M,151,"Different training datasets depending on experiments, but baseline is Wikitext-103","Google, CMU",https://huggingface.co/docs/transformers/model_doc/transfo-xl,N/A,,,,,,,LM,https://www.wikidata.org/entity/Q115305900,"Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
2019_Zhang,ERNIE: Enhanced Language Representation with Informative Entities,ERNIE,BERT,Encoder,Masked Language Modeling,,"Uses BERT for Encoder architecture, but stacks and aggregates
two of them for text and entities. This architecture could be
understood as BERT for text + knowledge graphs","Knowledge intensive related tasks that might benefit from
knowledge graphs or entities such as entity recognition",2019-05-01,"Ernie-ViLG 2.0 = 10B, Ernie 3.0 Titan = 260B",260000,"English Wikipedia + Wikidata for entitites (note that they
initialize model to original BERT parameter values","Pengcheng Lab, Baidu",https://github.com/thunlp/ERNIE,closed source,http://research.baidu.com/Blog/index-view?id=160,https://www.wikidata.org/entity/Q61726893,,,,,,https://www.wikidata.org/entity/Q115305900,"Large Language Models (LLMs), transformer model",
2021_Jumper,Highly accurate protein structure prediction with AlphaFold,AlphaFold,SE(3)-Transformer,Encoder,"Protein folding prediction of BERT using parameter
sharing, which is much more efficient given the same number of parameters",,"The original Alphafold used a BERT-style transformer. The details of Alphafoldâ€™s Transformer are not known, but it is believed it is an extension of the SE(3)-Tranformer, a 3-D equivariant Transformer (see this blog post).",Protein folding,2019-09-01,"b12M, Large = 18M, XLarge = 60M",60,"170,000 proteins from a public repository of protein sequences and structures",Deepmind,https://github.com/deepmind/alphafold,"the code is open sourced, with Apache-2.0","https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold, https://fabianfuchsml.github.io/alphafold2/",,,,,,,https://www.wikidata.org/entity/Q115305900,"Large Language Models (LLMs), transformer model",
2020_Zhang,PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization,Pegasus,Transformer,Encoder/Decoder,DAE (more concretely GSG) and MLM,,Extends vanilla Transformer by using a different pretraining task (GSG: Gap Sentence Generation) that is better suited for summarization,abstractive text summarization,2019-12-01,Base = 223M Large = 568M,568,C4 (750GB) + HugeNews (3.8 TB),"Google, Imperial College London",https://huggingface.co/docs/transformers/model_doc/pegasus,N/A,https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html,,,,,,,https://www.wikidata.org/entity/Q115305900,"Large Language Models (LLMs), transformer model",
2022_Peng,Godel: Large-scale pre-training for goal-directed dialog,Godel,"T5, GPT",Decoder,Language Modeling,,"In contrast
with earlier models such as DialoGPT,
GODEL leverages a new phase of grounded
pre-training designed to better support adapting
GODEL to a wide range of downstream
dialog tasks that require information external
to the current conversation (e.g., a database
or document) to produce good responses.","open-domain goal-directed dialog tasks such as knowledge-grounded response generation, task-oriented dialog, and conversational QA",2022-06-01,"220M (base), 770M (large), and 175B (XL)",175000,"147M dialog sessions for a total of 6B tokens from Reddit comment chains
for DialoGPT. And grounded dialog corpora like DSTC7 Task 2 corpus, MS MARCO, UnifiedQA, and Schema-Guided Dialog.",Microsoft,"https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq?text=Hey+my+name+is+Mariama%21+How+are+you%3F, https://huggingface.co/microsoft/GODEL-v1_1-base-seq2seq?text=Hey+my+name+is+Julien%21+How+are+you%3F, https://github.com/microsoft/GODEL",,https://www.microsoft.com/en-us/research/blog/godel-combining-goal-oriented-dialog-with-real-world-conversations/,,,,,,LM,https://www.wikidata.org/entity/Q115305900,"Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
2023_Touvron,LLaMA: Open and Efficient Foundation Language Models,LLaMA,LLaMa,Decoder,Language Modeling,instructions,"LLaMA uses a Transformer architecture, and with extensions:
Pre-normalization, SwiGLU activations, RoPE embeddings, reduced
memory usage and runtime through efficient implementation of the causal
multi-head attention, checkpointing to reduce the amount of activations
that are recomputed during the backward pass, model and sequence parallelism
to reduce memory usage of the model, and uses 1.4T BPE tokens
after tokenization.","Zero and few shot Commonsense reasoning, Question answering, mathematical reasoning,
Code generation, Reading comprehension, and multitask understanding.",2023-02-27,"6.7B, 13.0B, 32.5B, and 65.2B",65200,"Stack Exchange, ArXiv, Gutenberg and Books3, Wikipedia, Github, C4, CommonCrawl, approximately 1.4T tokens from various sources: 3.3 TB CommonCrawl (67%), 783GB C4 (15%), 328BG Github (4.5%), 83GB Wikipedia (4.5%), 85GB Books (4.5%), 92GB ArXiv (2.5%), and 78GB StackExchange (2.0%)",Meta AI,"https://huggingface.co/docs/transformers/main/model_doc/llama, https://github.com/facebookresearch/llama","Limited, Non-commercial bespoke license",https://ai.facebook.com/blog/large-language-model-llama-meta-ai/,,,A100-80GB GPU,https://www.wikidata.org/entity/Q182477,"the bytepair encoding (BPE) algorithm, using the implementation from Sentence-Piece. Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters.",LM,https://www.wikidata.org/entity/Q115305900,"Large Language Models (LLMs), transformer model","Predict next token (in the case of unidirectional LM) or previous and next token
(in the case of bidirectional LM)"
